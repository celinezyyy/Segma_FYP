{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d01967",
   "metadata": {},
   "source": [
    "## Customer Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30abd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fuzzywuzzy\n",
    "# !python -m pip install --upgrade pip\n",
    "# !pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ec00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from fuzzywuzzy import process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4769ad2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Date of Birth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Income Level</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUST0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sungai Besi</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUST0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUST0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mutiara Damansara</td>\n",
       "      <td>Selangor</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUST0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUST0003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shah Alam</td>\n",
       "      <td>Selangor</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CustomerID  Date of Birth  Gender  Income Level               City  \\\n",
       "0   CUST0001            NaN  female           NaN        Sungai Besi   \n",
       "1   CUST0001            NaN  female           NaN                NaN   \n",
       "2   CUST0002            NaN  female           NaN  Mutiara Damansara   \n",
       "3   CUST0002            NaN  female           NaN                NaN   \n",
       "4   CUST0003            NaN  female           NaN          Shah Alam   \n",
       "\n",
       "          State   Country  \n",
       "0  Kuala Lumpur  Malaysia  \n",
       "1           NaN       NaN  \n",
       "2      Selangor  Malaysia  \n",
       "3           NaN       NaN  \n",
       "4      Selangor  Malaysia  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load dataset\n",
    "# Replace 'customer_dataset.csv' with your actual file name or path\n",
    "file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Soapan Santun/2021 - 2025 Customer.csv\"\n",
    "\n",
    "original_dataset_name = \"2021 - 2025 Customer.csv\"\n",
    "\n",
    "# Read dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Show first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706371d9",
   "metadata": {},
   "source": [
    "### Initial checking: (Before perform data cleaning) Check optional and mandatory columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508463d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional columns\n",
    "def check_optional_columns(df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Check optional columns for fill percentage and drop columns that are mostly empty.\n",
    "    Returns the modified DataFrame and a friendly message.\n",
    "    \"\"\"\n",
    "    \n",
    "    optional_columns = [\"Date of Birth\", \"Gender\", \"Income Level\"]\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.columns = df.columns.str.lower()\n",
    "    optional_columns = [col.lower() for col in optional_columns]\n",
    "\n",
    "    dropped_columns = []\n",
    "\n",
    "    for col in optional_columns:\n",
    "        if col in df.columns:\n",
    "            fill_ratio = df[col].count() / len(df) # \n",
    "            if fill_ratio < threshold:\n",
    "                dropped_columns.append(col)\n",
    "                df.drop(columns=[col], inplace=True)  # Drop the column immediately\n",
    "                # df[col].count(): This counts the number of non-missing (non-null/non-NaN) values in the current column (col).\n",
    "                # len(df): This gives the total number of rows in the DataFrame.\n",
    "                # fill_ratio: The division calculates the proportion of filled (non-missing) values in that column. A ratio of 1.0 means the column is entirely filled; a ratio of 0.1 means 90% of the values are missing.\n",
    "\n",
    "\n",
    "    # Generate user-friendly message\n",
    "    if dropped_columns:\n",
    "        dropped_str = \", \".join(dropped_columns)\n",
    "        message = (\n",
    "            f\"We noticed that very few entries were provided for {dropped_str}. \"\n",
    "            \"These columns have been removed. \"\n",
    "            \"Segmentation will still be performed using geographic (City, State, Country) \"\n",
    "            \"and behavioral data (e.g., orders, purchase items, total spend).\"\n",
    "        )\n",
    "    else:\n",
    "        message = \"All optional columns have enough data and are kept for analysis.\"\n",
    "    \n",
    "    return df, message\n",
    "\n",
    "# Mandatory columns \n",
    "def check_mandatory_columns(df, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Check mandatory columns for missing values and return a friendly message.\n",
    "    We will continue processing even if too many rows are missing.\n",
    "    \"\"\"\n",
    "    \n",
    "    mandatory_columns = [\"CustomerID\", \"City\", \"State\", \"Country\"] \n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.columns = df.columns.str.lower()\n",
    "    mandatory_columns = [col.lower() for col in mandatory_columns]\n",
    "\n",
    "    # Calculate total number of rows with missing mandatory values\n",
    "    invalid_rows = df[mandatory_columns].isnull().any(axis=1)\n",
    "    percent_invalid = invalid_rows.sum() / len(df)\n",
    "\n",
    "    if percent_invalid > threshold:\n",
    "        message = (\n",
    "            f\"Your dataset has too many missing values in required fields \"\n",
    "            f\"({percent_invalid*100:.0f}% of rows). \"\n",
    "            \"The system can still clean and process the dataset with the information you have, \"\n",
    "            \"but please note that the results may be slightly biased and not fully represent all your customers. \"\n",
    "            \"You can continue with the current file, but we advise to re-check your data source. Are you sure you want to continue?\"\n",
    "        )\n",
    "    else:\n",
    "        message = \"Dataset passed initial validation for mandatory columns. Ready for cleaning!\"\n",
    "\n",
    "    return df, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea956f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optional Columns Check:\n",
      "We noticed that very few entries were provided for date of birth, income level. These columns have been removed. Segmentation will still be performed using geographic (City, State, Country) and behavioral data (e.g., orders, purchase items, total spend).\n",
      "\n",
      "Mandatory Columns Check:\n",
      "Your dataset has too many missing values in required fields (47% of rows). The system can still clean and process the dataset with the information you have, but please note that the results may be slightly biased and not fully represent all your customers. You can continue with the current file, but we advise to re-check your data source. Are you sure you want to continue?\n",
      "\n",
      "Cleaned dataset saved as '2021 - 2025 Customer_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- Apply checks and save cleaned dataset ---\n",
    "# Step 1: Optional columns\n",
    "df, optional_check_message = check_optional_columns(df)\n",
    "\n",
    "# Step 2: Mandatory columns (just check, but keep all columns)\n",
    "df, mandatory_check_message = check_mandatory_columns(df)\n",
    "\n",
    "# Step 3: Save the updated dataset for cleaning\n",
    "\n",
    "# Split the name and extension\n",
    "base_name, ext = os.path.splitext(original_dataset_name)\n",
    "\n",
    "# Create new cleaned file name\n",
    "cleaned_file = f\"{base_name}_cleaned{ext}\"\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv(cleaned_file, index=False)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"Optional Columns Check:\")\n",
    "print(optional_check_message)\n",
    "\n",
    "print(\"\\nMandatory Columns Check:\")\n",
    "print(mandatory_check_message)\n",
    "\n",
    "print(f\"\\nCleaned dataset saved as '{cleaned_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "354c5c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerid</th>\n",
       "      <th>gender</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUST0001</td>\n",
       "      <td>female</td>\n",
       "      <td>Sungai Besi</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUST0001</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUST0002</td>\n",
       "      <td>female</td>\n",
       "      <td>Mutiara Damansara</td>\n",
       "      <td>Selangor</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUST0002</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUST0003</td>\n",
       "      <td>female</td>\n",
       "      <td>Shah Alam</td>\n",
       "      <td>Selangor</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customerid  gender               city         state   country\n",
       "0   CUST0001  female        Sungai Besi  Kuala Lumpur  Malaysia\n",
       "1   CUST0001  female                NaN           NaN       NaN\n",
       "2   CUST0002  female  Mutiara Damansara      Selangor  Malaysia\n",
       "3   CUST0002  female                NaN           NaN       NaN\n",
       "4   CUST0003  female          Shah Alam      Selangor  Malaysia"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After perform initial checking on optional and mandatory columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f407f8",
   "metadata": {},
   "source": [
    "### Perform Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb56a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_customer_dataset(df, order_df=None):\n",
    "    \"\"\"\n",
    "    Automatically clean the customer dataset for segmentation.\n",
    "    Handles both mandatory and optional columns flexibly.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Normalize column names\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # Step 2: Remove duplicate rows\n",
    "    if \"customerid\" in df.columns:\n",
    "        df.drop_duplicates(subset=[\"customerid\"], inplace=True)\n",
    "    else:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Step 3: Clean CustomerID\n",
    "    if \"customerid\" in df.columns:\n",
    "        df[\"customerid\"] = df[\"customerid\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Step 4: Cross-check with orders (if provided)\n",
    "    if order_df is not None and \"customerid\" in order_df.columns:\n",
    "        valid_ids = set(order_df[\"customerid\"].astype(str).str.strip().str.upper())\n",
    "        df = df[df[\"customerid\"].isin(valid_ids)]\n",
    "\n",
    "    # Step 5: Handle optional columns\n",
    "    optional_cols = [\"date of birth\", \"gender\", \"income level\"]\n",
    "    for col in optional_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # --- Date of Birth ---\n",
    "        if col == \"date of birth\":\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\", infer_datetime_format=True)\n",
    "            if df[col].isnull().mean() > 0.5:\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "\n",
    "        # --- Gender ---\n",
    "        elif col == \"gender\":\n",
    "            df[col] = df[col].str.strip().str.lower()\n",
    "            gender_map = {\n",
    "                \"m\": \"Male\", \"male\": \"Male\",\n",
    "                \"f\": \"Female\", \"female\": \"Female\"\n",
    "            }\n",
    "            df[col] = df[col].map(gender_map).fillna(\"Other\")\n",
    "\n",
    "        # --- Income Level ---\n",
    "        elif col == \"income level\":\n",
    "            df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "            income_map = {\n",
    "                \"low\": \"Low\", \"lower\": \"Low\",\n",
    "                \"medium\": \"Medium\", \"mid\": \"Medium\", \"med\": \"Medium\", \"middle\": \"Medium\",\n",
    "                \"high\": \"High\", \"upper\": \"High\"\n",
    "            }\n",
    "            df[col] = df[col].replace(income_map)\n",
    "            # if mostly missing, drop\n",
    "            if df[col].isnull().mean() > 0.5:\n",
    "                df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    # Step 6: Clean City, State, Country\n",
    "    for col in [\"city\", \"state\", \"country\"]:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        df[col] = df[col].astype(str).str.strip().str.title()\n",
    "\n",
    "    # Step 7: Save cleaned dataset\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
