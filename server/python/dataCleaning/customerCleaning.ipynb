{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d01967",
   "metadata": {},
   "source": [
    "## Customer Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30abd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install fuzzywuzzy\n",
    "# !python -m pip install --upgrade pip\n",
    "# !pip install pycountry\n",
    "# %pip install python-Levenshtein\n",
    "# %pip install pandas numpy matplotlib seaborn scikit-learn jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ec00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate kernal:\n",
    "# .\\venv_fyp\\Scripts\\activate\n",
    "\n",
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pycountry\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4769ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load dataset\n",
    "# Replace 'customer_dataset.csv' with your actual file name or path\n",
    "file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Soapan Santun/2021 - 2025 Customer - Copy.csv\"\n",
    "\n",
    "original_customer_dataset_name = \"2021 - 2025 Customer - Copy.csv\"\n",
    "\n",
    "# Read dataset\n",
    "customer_df = pd.read_csv(file_path)\n",
    "\n",
    "# Show first few rows (original raw data)\n",
    "customer_df.head()\n",
    "\n",
    "COUNTRY = 'Malaysia'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe554a9",
   "metadata": {},
   "source": [
    "### Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd905e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706371d9",
   "metadata": {},
   "source": [
    "### Perform Data Cleaning Pipeline - CustomerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508463d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================= STAGE 1: SCHEMA & COLUMN VALIDATION =============================================\n",
    "# # Optional columns\n",
    "def check_optional_columns(df, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Check optional columns for fill percentage and drop columns that are mostly empty.\n",
    "    Returns the modified DataFrame and a friendly message.\n",
    "    \"\"\"\n",
    "    print(\"[LOG] Running check_optional_columns...\")\n",
    "    optional_columns = [\"date of birth\", \"gender\"]\n",
    "    dropped_columns = []\n",
    "\n",
    "    for col in optional_columns:\n",
    "        if col in df.columns:\n",
    "            fill_ratio = df[col].count() / len(df) \n",
    "            print(f\"[LOG] Optional column '{col}' fill ratio: {fill_ratio:.2f}\")\n",
    "            if fill_ratio < threshold:\n",
    "                dropped_columns.append(col)\n",
    "                df.drop(columns=[col], inplace=True)  # Drop the column immediately\n",
    "                # df[col].count(): This counts the number of non-missing (non-null/non-NaN) values in the current column (col).\n",
    "                # len(df): This gives the total number of rows in the DataFrame.\n",
    "                # fill_ratio: The division calculates the proportion of filled (non-missing) values in that column. A ratio of 1.0 means the column is entirely filled; a ratio of 0.1 means 90% of the values are missing.\n",
    "                print(f\"[LOG] Dropped optional column '{col}' due to too many missing values\")\n",
    "        else:\n",
    "            print(f\"[LOG] Optional column '{col}' not found\")\n",
    "\n",
    "    # Generate user-friendly message\n",
    "    if dropped_columns:\n",
    "        dropped_str = \", \".join(dropped_columns)\n",
    "        message = (\n",
    "            f\"We noticed that very few entries were provided for {dropped_str}. \"\n",
    "            \"These columns have been removed. \"\n",
    "            \"Segmentation will still be performed using geographic (City, State) \"\n",
    "            \"and behavioral data (e.g., orders, purchase items, total spend).\"\n",
    "        )\n",
    "    else:\n",
    "        message = \"All optional columns have enough data and are kept for analysis.\"\n",
    "    \n",
    "    return df, message\n",
    "\n",
    "# Mandatory columns \n",
    "def check_mandatory_columns(df, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Check mandatory columns for missing values (column-wise).\n",
    "    Does not drop columns â€” only warns user if any column is too incomplete.\n",
    "    Returns the DataFrame and a message summarizing issues.\n",
    "    \"\"\"\n",
    "    print(\"[LOG] Running check_mandatory_columns...\")\n",
    "    mandatory_columns = [\"customerid\", \"city\", \"state\"]\n",
    "    missing_report = []\n",
    "    warning_columns = []\n",
    "\n",
    "    for col in mandatory_columns:\n",
    "        if col in df.columns:\n",
    "            fill_ratio = df[col].count() / len(df)\n",
    "            print(f\"[LOG] Mandatory column '{col}' fill ratio: {fill_ratio:.2f}\")\n",
    "            missing_percent = (1 - fill_ratio) * 100\n",
    "            missing_report.append(f\"{col}: {missing_percent:.1f}% missing\")\n",
    "\n",
    "            # Warn if missing exceeds threshold\n",
    "            if fill_ratio < threshold:\n",
    "                warning_columns.append(col)\n",
    "        else:\n",
    "            print(f\"[LOG] Mandatory column '{col}' not found\")\n",
    "            # Handle case where column completely missing\n",
    "            missing_report.append(f\"{col}: column not found (100% missing)\")\n",
    "            warning_columns.append(col)\n",
    "\n",
    "    # Generate friendly message\n",
    "    if warning_columns:\n",
    "        warning_str = \", \".join(warning_columns)\n",
    "        message = (\n",
    "            f\"Some key fields have a high number of missing values: {warning_str}. \"\n",
    "            \"The system will still continue cleaning and processing, \"\n",
    "            \"but missing values will be handled automatically by our system. \"\n",
    "            \"Please ensure your source data is as complete as possible for more accurate segmentation results.\\n\\n\"\n",
    "            \"Missing Data Summary:\\n\" + \"\\n\".join(missing_report)\n",
    "        )\n",
    "    else:\n",
    "        message = (\n",
    "            \"All mandatory columns have sufficient data and are ready for cleaning.\\n\\n\"\n",
    "            \"Missing Data Summary:\\n\" + \"\\n\".join(missing_report)\n",
    "        )\n",
    "\n",
    "    return df, message\n",
    "\n",
    "# ============================================= STAGE 2: REMOVE DUPLICATE ENTRY ROW =================================================\n",
    "def remove_duplicate_entries(df):\n",
    "    \"\"\"Remove duplicate rows, keeping the first occurrence\"\"\"\n",
    "    print(\"[LOG] Running remove_duplicate_entries...\")\n",
    "    initial_len = len(df)\n",
    "    df = df.drop_duplicates(keep='first').copy()\n",
    "    print(f\"[LOG] Removed {initial_len - len(df)} duplicate rows.\")\n",
    "    return df\n",
    "\n",
    "# ============================================= STAGE 3: STANDARDIZATION & NORMALIZATION =============================================\n",
    "\n",
    "def normalize_columns_name(df):\n",
    "    \"\"\"Normalize column names: lowercase, strip spaces\"\"\"\n",
    "    print(\"[LOG] Running normalize_columns_name...\")\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    print(f\"[LOG] Columns after normalization: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "# ===============================================================================\n",
    "\n",
    "def standardize_customer_id(df):\n",
    "    \"\"\"Standardize CustomerID format while preserving NaN\"\"\"\n",
    "    print(\"[LOG] Running standardize_customer_id...\")\n",
    "    if 'customerid' in df.columns:\n",
    "        # Fill NaN with empty string before converting to string\n",
    "        df['customerid'] = df['customerid'].fillna('').astype(str).str.strip().str.upper()\n",
    "        print(\"[LOG] CustomerID column standardized\")\n",
    "    else:\n",
    "        print(\"[LOG] CustomerID column not found, skipping\")\n",
    "    return df\n",
    "# Might have special case of dirty data exist such as \"****\", \"1234....\", \"annbwbciwbciowb\"\n",
    "# not sure how to handle it (Currently will say bcs we focus on small business enterprise that have use digital system, so normally customerID will not have inconsistent format issue, even the inconsistant format exist, at the end this row will not be use as when we merge we cant found that customerID)\n",
    "\n",
    "# ===============================================================================\n",
    "\n",
    "def standardize_dob(df):\n",
    "    \"\"\"Standardize Date of Birth column and convert to YYYY-MM-DD\"\"\"\n",
    "    print(\"[LOG] Running standardize_dob...\")\n",
    "    # Rename only 'date of birth' to 'dob'\n",
    "    df = df.rename(columns={'date of birth': 'dob'})  \n",
    "    if 'dob' in df.columns:\n",
    "        print(\"[LOG] DOB column found, parsing dates...\")\n",
    "        def parse_date(x):\n",
    "            if pd.isnull(x):\n",
    "                return \"Unknown\"\n",
    "            for format in (\"%d/%m/%Y\", \"%m-%d-%y\", \"%Y-%m-%d\", \"%d-%b-%Y\", \"%d-%m-%Y\"):    \n",
    "                try:\n",
    "                    return datetime.strptime(str(x), format).date() # Final format: YYYY-MM-DD | 2025-10-15\n",
    "                except Exception:\n",
    "                    continue\n",
    "            return \"Unknown\"  # If no valid format found\n",
    "        df['dob'] = df['dob'].apply(parse_date)\n",
    "        print(\"[LOG] DOB parsing complete\")\n",
    "    else:\n",
    "        print(\"[LOG] DOB column not found, skipping\")\n",
    "    return df\n",
    "\n",
    "# %d/%m/%Y â†’ 12/05/2000\n",
    "# %m-%d-%y â†’ 05-12-00\n",
    "# %Y-%m-%d â†’ 2000-05-12\n",
    "# %d-%b-%Y â†’ 12-May-2000\n",
    "# %d-%m-%Y â†’ 12-5-2000\n",
    "\n",
    "# ===============================================================================\n",
    "\n",
    "def derive_age_features(df):\n",
    "    \"\"\"Derive Age from DOB\"\"\"\n",
    "    print(\"[LOG] Running derive_age_features...\")\n",
    "    if 'dob' in df.columns:\n",
    "        today = date.today()\n",
    "        df['age'] = df['dob'].apply(\n",
    "            lambda x: today.year - x.year - ((today.month, today.day) < (x.month, x.day))\n",
    "            if pd.notnull(x) else \"Unknown\"\n",
    "        )\n",
    "        print(\"[LOG] Age derived from DOB\")\n",
    "    else:\n",
    "        print(\"[LOG] DOB column not found, skipping\")\n",
    "    return df\n",
    "# Example: ((today.month, today.day) < (x.month, x.day))\n",
    "# (10,15) < (12,1) â†’ True (birthday in Dec is after Oct 15)\n",
    "# (10,15) < (10,16) â†’ True (birthday tomorrow)\n",
    "# (10,15) < (5,20) â†’ False (birthday already passed)\n",
    "\n",
    "# This function calculates each personâ€™s age from their date of birth (dob) by subtracting their birth year from the current year and adjusting if their birthday hasnâ€™t occurred yet this year.\n",
    "\n",
    "# ===============================================================================\n",
    "\n",
    "def derive_age_group(df):\n",
    "    \"\"\"Derive Age Group based on defined buckets\"\"\"\n",
    "    print(\"[LOG] Running derive_age_group...\")\n",
    "    if 'age' in df.columns:\n",
    "        def categorize_age(age):\n",
    "            if pd.isnull(age):\n",
    "                return 'Unknown'\n",
    "            if age < 18: return 'Below 18'\n",
    "            elif 18 <= age <= 24: return '18-24'\n",
    "            elif 25 <= age <= 34: return '25-34'\n",
    "            elif 35 <= age <= 44: return '35-44'\n",
    "            elif 45 <= age <= 54: return '45-54'\n",
    "            elif 55 <= age <= 64: return '55-64'\n",
    "            else: return 'Above 65'\n",
    "        df['age_group'] = df['age'].apply(categorize_age)\n",
    "        print(\"[LOG] Age groups derived\")\n",
    "    else:\n",
    "        print(\"[LOG] Age column not found, skipping\")\n",
    "    return df\n",
    "# ===============================================================================\n",
    "\n",
    "def drop_dob_after_age_derived(df):\n",
    "    \"\"\"Drop DOB column after deriving age and age_group\"\"\"\n",
    "    print(\"[LOG] Running drop_dob_after_age_derived...\")\n",
    "    if 'dob' in df.columns:\n",
    "        df = df.drop(columns=['dob'])\n",
    "        print(\"[LOG] Dropped DOB column\")\n",
    "    else:\n",
    "        print(\"[LOG] DOB column not found, skipping\")\n",
    "    return df\n",
    "\n",
    "# =================================================================================\n",
    "\n",
    "def standardize_gender(df):\n",
    "    \"\"\"Clean and standardize gender values\"\"\"\n",
    "    print(\"[LOG] Running standardize_gender...\")\n",
    "    if 'gender' in df.columns:\n",
    "        # Clean text (remove spaces, make lowercase)\n",
    "        df['gender'] = df['gender'].astype(str).str.strip().str.lower()\n",
    "\n",
    "        # Standardize using keyword detection\n",
    "        def detect_gender(value):\n",
    "            if any(word in value for word in ['m', 'male', 'man', 'boy']):\n",
    "                return 'Male'\n",
    "            elif any(word in value for word in ['f', 'female', 'woman', 'girl']):\n",
    "                return 'Female'\n",
    "            else:\n",
    "                return 'Unknown'\n",
    "\n",
    "        df['gender'] = df['gender'].apply(detect_gender)\n",
    "        print(\"[LOG] Gender standardized\")\n",
    "    else:\n",
    "        print(\"[LOG] Gender column not found, skipping\")\n",
    "    return df\n",
    "\n",
    "# ==================================================================================\n",
    "\n",
    "def standardize_location(df):\n",
    "    \"\"\"Standardize City, and State fields\"\"\"\n",
    "    print(\"[LOG] Running standardize_location...\")\n",
    "        \n",
    "    # Helper function: detect suspicious city names\n",
    "    def is_suspicious_city(name):\n",
    "        if not name or name.strip() == '':\n",
    "            return True\n",
    "        name = str(name).strip()\n",
    "        \n",
    "        if len(name) < 2 or len(name) > 50: # Too short or too long\n",
    "            return True\n",
    "        \n",
    "        if re.search(r'[^A-Za-z\\s\\'-]', name):  # Contains non-alphabetic or weird symbols   # letters, space, apostrophe, dash allowed\n",
    "            return True\n",
    "        \n",
    "        if re.search(r'(.)\\1{3,}', name):   # Repeated characters (e.g., \"Ccciiiittty\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # --- City ---\n",
    "    if 'city' in df.columns:\n",
    "        df['city'] = df['city'].fillna('').astype(str).str.title().str.strip()\n",
    "        \n",
    "        # Count entries that are either suspicious or \"Others\"/\"other\"\n",
    "        suspicious_count = df['city'].apply(\n",
    "            lambda x: is_suspicious_city(x) or x.lower() in ['others', 'other']\n",
    "        ).sum()\n",
    "        \n",
    "        # Replace suspicious or \"Others\"/\"other\" entries with \"Unknown\"\n",
    "        df['city'] = df['city'].apply(\n",
    "            lambda x: 'Unknown' if (is_suspicious_city(x) or x.lower() in ['others', 'other']) else x\n",
    "        )\n",
    "        \n",
    "        print(f\"[LOG] Standardized 'city'. Suspicious/unknown entries set to 'Unknown': {suspicious_count}\")\n",
    "    else:\n",
    "        print(\"[LOG] 'city' column not found, skipping city standardization\")\n",
    "    \n",
    "    # --- State ---\n",
    "    if 'state' in df.columns:\n",
    "        malaysia_states = [sub.name for sub in pycountry.subdivisions if sub.country_code == 'MY']\n",
    "        df['state'] = df['state'].fillna('').astype(str).str.title().str.strip()\n",
    "        df['state'] = df['state'].apply(\n",
    "            lambda x: process.extractOne(x, malaysia_states, scorer=fuzz.token_sort_ratio)[0] if x else 'Unknown'\n",
    "        )\n",
    "    else:\n",
    "        print(\"[LOG] 'state' column not found, skipping state standardization\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================= STAGE 4: MISSING VALUE HANDLING =============================================\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Handle missing values using a column-based approach.\n",
    "    Each column is treated independently based on its type and business logic.\n",
    "    \"\"\"\n",
    "    print(\"[LOG] Running handle_missing_values...\")\n",
    "    \n",
    "    # ---- CONFIGURATION ----\n",
    "    API_KEY = \"68f8ce9a38c3f632237334dyiedb96e\"  # Replace with your Maps.co API key\n",
    "    GEOCODE_URL = \"https://geocode.maps.co/search\"\n",
    "    SLEEP_TIME = 1.2  # delay between API calls to avoid rate limit\n",
    "    \n",
    "    # ----- 1. Customer ID -----\n",
    "    if 'customerid' in df.columns:\n",
    "        before_drop = len(df)\n",
    "        df = df[df['customerid'].notna()].copy()  # Drop rows without ID\n",
    "        print(f\"[LOG] Dropped {before_drop - len(df)} rows without CustomerID\")\n",
    "    else:\n",
    "        print(\"[LOG] 'customerid' column missing, skipping drop\")\n",
    "        \n",
    "    # ----- 2. Age -----\n",
    "    if 'age' in df.columns:\n",
    "        missing_ratio = df['age'].isna().mean()\n",
    "        print(f\"[LOG] Age missing ratio: {missing_ratio:.2%}\")\n",
    "        if missing_ratio > 0:\n",
    "            if 'gender' in df.columns and df['gender'].nunique() > 1:\n",
    "                # Group by gender if available\n",
    "                df['age'] = df.groupby('gender')['age'].transform(\n",
    "                    lambda x: x.fillna(x.median())\n",
    "                )\n",
    "                print(\"[LOG] Applied gender-based median imputation for missing age\")\n",
    "                \n",
    "                # Always fill any remaining missing values with overall median\n",
    "                df['age'] = df['age'].fillna(df['age'].median())\n",
    "                \n",
    "                # Update derived column if needed\n",
    "                df = derive_age_group(df)\n",
    "        else:\n",
    "            print(\"[LOG] No missing values for age\")\n",
    "    else:\n",
    "        print(\"[LOG] 'age' column missing, skipping age imputation\")\n",
    "\n",
    "    # ----- 4. Gender -----\n",
    "    if 'gender' in df.columns:\n",
    "        gender_mode = df['gender'].mode()[0]    # 1. Calculate the mode (most frequent value)\n",
    "        unknown_count = (df['gender'] == 'Unknown').sum()\n",
    "        df['gender'] = df['gender'].replace('Unknown', gender_mode)  # 2. Replace 'Unknown' values with the calculated mode\n",
    "        print(f\"[LOG] Replaced {unknown_count} 'Unknown' gender values with mode: {gender_mode}\")\n",
    "    else:\n",
    "        print(\"[LOG] 'gender' column missing, skipping gender imputation\")\n",
    "        \n",
    "    # ----- 5. City & State-----\n",
    "    if {'city', 'state'}.issubset(df.columns):\n",
    "        # --- Case 1: city present, state missing ---\n",
    "        print(\"\\nğŸ” [Case 1] Filling missing state using Geocoding API (city known)...\")\n",
    "        # Filter only rows needing API call\n",
    "        rows_to_fill = df[(df['city'] != \"Unknown\") & (df['state'] == \"Unknown\")]\n",
    "        print(f\"[LOG] Filling missing states where city known: {len(rows_to_fill)} rows\")\n",
    "        # Optional: cache repeated city lookups\n",
    "        cache = {}\n",
    "        for idx, row in rows_to_fill.iterrows():\n",
    "            city = row['city']\n",
    "\n",
    "            # Check if we've already queried this city\n",
    "            if city in cache:\n",
    "                state_guess = cache[city]\n",
    "            else:\n",
    "                try:\n",
    "                    params = {\"q\": f\"{row['city']}, Malaysia\", \"api_key\": API_KEY}\n",
    "                    response = requests.get(GEOCODE_URL, params=params, timeout=10)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        if isinstance(data, list) and len(data) > 0:\n",
    "                            address = data[0].get(\"display_name\", \"\")\n",
    "                            parts = [p.strip() for p in address.split(\",\")]\n",
    "                            if len(parts) >= 3:\n",
    "                                state_guess = parts[-3]\n",
    "                                cache[city] = state_guess  # store in cache\n",
    "                                df.at[idx, 'state'] = state_guess\n",
    "                                print(f\"âœ… {row['city']} â†’ Filled state: {state_guess}\")\n",
    "                    time.sleep(SLEEP_TIME)\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Error for {row['city']}: {e}\")\n",
    "\n",
    "        # --- Case 2: no city but have state ---\n",
    "        print(\"\\nğŸ” [Case 2] Filling missing city using mode per state...\")\n",
    "        mode_city_per_state = df[df['city'] != \"Unknown\"].groupby('state')['city'].agg(\n",
    "            lambda x: x.mode().iloc[0] if not x.mode().empty else \"Unknown\"\n",
    "        )\n",
    "        for idx, row in df.iterrows():\n",
    "            if row['city'] == \"Unknown\" and row['state'] != \"Unknown\":\n",
    "                most_common_city = mode_city_per_state.get(row['state'], \"Unknown\")\n",
    "                df.at[idx, 'city'] = most_common_city\n",
    "                print(f\"âœ… {row['state']} â†’ Filled city: {most_common_city}\")\n",
    "\n",
    "        # --- Case 3: both city and state unknown ---\n",
    "        print(\"\\nğŸ” [Case 3] Filling rows where both city & state are missing using mode pair...\")\n",
    "        valid_pairs = df[(df['city'] != \"Unknown\") & (df['state'] != \"Unknown\")]\n",
    "        if not valid_pairs.empty:\n",
    "            mode_pair = valid_pairs.groupby(['city', 'state']).size().idxmax()\n",
    "            city_mode, state_mode = mode_pair\n",
    "            mask = (df['city'] == \"Unknown\") & (df['state'] == \"Unknown\")\n",
    "            df.loc[mask, ['city', 'state']] = [city_mode, state_mode]\n",
    "            print(f\"[LOG] Filled {mask.sum()} rows with mode pair â†’ City: {city_mode}, State: {state_mode}\")\n",
    "    else:\n",
    "        print(\"[LOG] 'city' or 'state' column missing, skipping location missing value handling\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================= STAGE 5: OUTLIER DETECTION =============================================\n",
    "\n",
    "def detect_outliers(df):\n",
    "    \"\"\"Adaptive outlier handling based on dataset size.\"\"\"\n",
    "    print(\"[LOG] Running detect_outliers...\")\n",
    "    if 'age' in df.columns:\n",
    "        df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "        # df['age_original'] = df['age']  # âœ… Keep a copy of original age values (for comparison or re-deriving age_group)\n",
    "\n",
    "        n = len(df)\n",
    "        print(f\"[LOG] Dataset has {n} rows\")\n",
    "        if n < 500:\n",
    "            # IQR method\n",
    "            Q1 = df['age'].quantile(0.25)\n",
    "            Q3 = df['age'].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = df[(df['age'] <  Q1) | (df['age'] > Q3)].shape[0]\n",
    "            df.loc[(df['age'] < lower_bound) | (df['age'] > upper_bound), 'age'] = np.nan\n",
    "            print(f\"[LOG] IQR Applied for {n} rows. Range: [{lower_bound:.1f}, {upper_bound:.1f}] Outliers set to NaN: {outliers}\")\n",
    "        else:\n",
    "            # Percentile capping\n",
    "            lower_bound = df['age'].quantile(0.01)\n",
    "            upper_bound = df['age'].quantile(0.99)\n",
    "            df['age'] = df['age'].clip(lower=lower_bound, upper=upper_bound)\n",
    "            print(f\"[LOG] Percentile capping  applied for {n} rows. Capped to [{lower_bound:.1f}, {upper_bound:.1f}]\")\n",
    "    else:\n",
    "        print(\"[LOG] 'age' column missing, skipping outlier detection\")\n",
    "    return df\n",
    "\n",
    "# ============================================= STAGE 6: DEDUPLICATION =============================================\n",
    "# This function removes duplicate CustomerIDs and keeps the row with the most non-missing data to preserve the most complete customer record.\n",
    "def deduplicate_customers(df):\n",
    "    \"\"\"Handle duplicate customer records intelligently\"\"\"\n",
    "    print(\"[LOG] Running deduplicate_customers...\")\n",
    "    if 'customerid' not in df.columns:\n",
    "        print(\"[LOG] 'customerid' column missing, skipping deduplication\")\n",
    "        return df  # Skip if no customerid column\n",
    "    \n",
    "    merged_records = []\n",
    "    total_duplicates = 0\n",
    "    \n",
    "    for cust_id, group in df.groupby('customerid'):\n",
    "        if len(group) > 1:\n",
    "            total_duplicates += len(group) - 1\n",
    "        merged_row = {'customerid': cust_id}\n",
    "        for col in df.columns:\n",
    "            if col == 'customerid':\n",
    "                continue\n",
    "            \n",
    "            # Get all unique non-null values for this customer\n",
    "            values = group[col].dropna().unique()\n",
    "            \n",
    "            if len(values) == 0:\n",
    "                merged_row[col] = pd.NA\n",
    "            elif len(values) == 1:\n",
    "                merged_row[col] = values[0]\n",
    "            else:\n",
    "                # Multiple conflicting values â†’ choose most frequent (mode)\n",
    "                merged_row[col] = group[col].mode()[0]\n",
    "        \n",
    "        merged_records.append(merged_row)\n",
    "    print(f\"[LOG] Deduplicated {total_duplicates} duplicate customer entries\")\n",
    "    return pd.DataFrame(merged_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_customer_dataset(df, original_customer_dataset_name):\n",
    "    \"\"\"\n",
    "    Main cleaning pipeline for customer dataset.\n",
    "    Executes all stages in proper order:\n",
    "    0. Column Normalization\n",
    "    1. Schema & Column Validation\n",
    "    2. Duplicate Entry Removal\n",
    "    3. Standardization & Normalization\n",
    "    4. Missing Value Handling\n",
    "    5. Outlier Detection\n",
    "    6. Deduplication\n",
    "    Finally, saves the cleaned dataset and returns it.\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Starting data cleaning pipeline...\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 0: NORMALIZE COLUMN NAMES\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 0 START] Normalize Column Names ==========\")\n",
    "    df = normalize_columns_name(df)\n",
    "    print(\"âœ… [STAGE 0 COMPLETE] Column names normalized.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 1: SCHEMA & COLUMN VALIDATION\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 1 START] Schema & Column Validation ==========\")\n",
    "    df, optional_msg = check_optional_columns(df)\n",
    "    df, mandatory_msg = check_mandatory_columns(df)\n",
    "    print(optional_msg)\n",
    "    print(mandatory_msg)\n",
    "    print(\"âœ… [STAGE 1 COMPLETE] Schema validation done.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 2: REMOVE DUPLICATE ENTRY ROWS\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 2 START] Remove Duplicate Entry Rows ==========\")\n",
    "    df = remove_duplicate_entries(df)\n",
    "    print(\"âœ… [STAGE 2 COMPLETE] Duplicate entries removed.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 3: DEDUPLICATION\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 3 START] Deduplication ==========\")\n",
    "    df = deduplicate_customers(df)\n",
    "    print(\"âœ… [STAGE 3 COMPLETE] Duplicate CustomerIDs deduplicated.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 4: STANDARDIZATION & NORMALIZATION\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 3 START] Standardization & Normalization ==========\")\n",
    "    df = standardize_customer_id(df)\n",
    "    df = standardize_dob(df)\n",
    "    df = derive_age_features(df)\n",
    "    df = derive_age_group(df)\n",
    "    df = drop_dob_after_age_derived(df)\n",
    "    df = standardize_gender(df)\n",
    "    df = standardize_location(df)\n",
    "    print(\"âœ… [STAGE 3 COMPLETE] Standardization and normalization finished.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 5: MISSING VALUE HANDLING\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 4 START] Missing Value Handling ==========\")\n",
    "    df = handle_missing_values(df)\n",
    "    print(\"âœ… [STAGE 4 COMPLETE] Missing values handled.\\n\")\n",
    "\n",
    "    # # =============================================\n",
    "    # # STAGE 6: OUTLIER DETECTION\n",
    "    # # =============================================\n",
    "    print(\"========== [STAGE 5 START] Outlier Detection ==========\")\n",
    "    df = detect_outliers(df)   # make sure detect_outliers returns df\n",
    "    print(\"âœ… [STAGE 5 COMPLETE] Outliers handled.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # SAVE CLEANED DATASET\n",
    "    # =============================================\n",
    "    print(\"========== [FINAL STAGE START] Save Cleaned Dataset ==========\")\n",
    "    base_name, ext = os.path.splitext(original_customer_dataset_name)\n",
    "    cleaned_file = f\"{base_name}_cleaned{ext}\"\n",
    "    df.to_csv(cleaned_file, index=False)\n",
    "    print(f\"âœ… [FINAL STAGE COMPLETE] Cleaned dataset saved as: {cleaned_file}\\n\")\n",
    "\n",
    "    print(\"==========================================================\")\n",
    "    print(\"ğŸ‰ Data cleaning pipeline completed successfully!\\n\")\n",
    "    return df, cleaned_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dbb17ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting data cleaning pipeline...\n",
      "\n",
      "========== [STAGE 0 START] Normalize Column Names ==========\n",
      "[LOG] Running normalize_columns_name...\n",
      "[LOG] Columns after normalization: ['customerid', 'date of birth', 'gender', 'city', 'state']\n",
      "âœ… [STAGE 0 COMPLETE] Column names normalized.\n",
      "\n",
      "========== [STAGE 1 START] Schema & Column Validation ==========\n",
      "[LOG] Running check_optional_columns...\n",
      "[LOG] Optional column 'date of birth' fill ratio: 0.00\n",
      "[LOG] Dropped optional column 'date of birth' due to too many missing values\n",
      "[LOG] Optional column 'gender' fill ratio: 0.00\n",
      "[LOG] Dropped optional column 'gender' due to too many missing values\n",
      "[LOG] Running check_mandatory_columns...\n",
      "[LOG] Mandatory column 'customerid' fill ratio: 1.00\n",
      "[LOG] Mandatory column 'city' fill ratio: 0.55\n",
      "[LOG] Mandatory column 'state' fill ratio: 0.53\n",
      "We noticed that very few entries were provided for date of birth, gender. These columns have been removed. Segmentation will still be performed using geographic (City, State) and behavioral data (e.g., orders, purchase items, total spend).\n",
      "Some key fields have a high number of missing values: city, state. The system will still continue cleaning and processing, but missing values will be handled automatically by our system. Please ensure your source data is as complete as possible for more accurate segmentation results.\n",
      "\n",
      "Missing Data Summary:\n",
      "customerid: 0.0% missing\n",
      "city: 44.8% missing\n",
      "state: 46.9% missing\n",
      "âœ… [STAGE 1 COMPLETE] Schema validation done.\n",
      "\n",
      "========== [STAGE 2 START] Remove Duplicate Entry Rows ==========\n",
      "[LOG] Running remove_duplicate_entries...\n",
      "[LOG] Removed 879 duplicate rows.\n",
      "âœ… [STAGE 2 COMPLETE] Duplicate entries removed.\n",
      "\n",
      "========== [STAGE 6 START] Deduplication ==========\n",
      "[LOG] Running deduplicate_customers...\n",
      "[LOG] Deduplicated 1697 duplicate customer entries\n",
      "âœ… [STAGE 6 COMPLETE] Duplicate CustomerIDs deduplicated.\n",
      "\n",
      "========== [STAGE 3 START] Standardization & Normalization ==========\n",
      "[LOG] Running standardize_customer_id...\n",
      "[LOG] CustomerID column standardized\n",
      "[LOG] Running standardize_dob...\n",
      "[LOG] DOB column not found, skipping\n",
      "[LOG] Running derive_age_features...\n",
      "[LOG] DOB column not found, skipping\n",
      "[LOG] Running derive_age_group...\n",
      "[LOG] Age column not found, skipping\n",
      "[LOG] Running drop_dob_after_age_derived...\n",
      "[LOG] DOB column not found, skipping\n",
      "[LOG] Running standardize_gender...\n",
      "[LOG] Gender column not found, skipping\n",
      "[LOG] Running standardize_location...\n",
      "[LOG] Standardized 'city'. Suspicious/unknown entries set to 'Unknown': 283\n",
      "âœ… [STAGE 3 COMPLETE] Standardization and normalization finished.\n",
      "\n",
      "========== [STAGE 4 START] Missing Value Handling ==========\n",
      "[LOG] Running handle_missing_values...\n",
      "[LOG] Dropped 0 rows without CustomerID\n",
      "[LOG] 'age' column missing, skipping age imputation\n",
      "[LOG] 'gender' column missing, skipping gender imputation\n",
      "\n",
      "ğŸ” [Case 1] Filling missing state using Geocoding API (city known)...\n",
      "[LOG] Filling missing states where city known: 46 rows\n",
      "âœ… Setapak â†’ Filled state: Kuala Lumpur\n",
      "âœ… Seberang Perai Tengah â†’ Filled state: Seberang Perai\n",
      "âœ… Alor Gajah â†’ Filled state: Alor Gajah\n",
      "âœ… Timur Laut â†’ Filled state: North-East\n",
      "âœ… Durian Tunggal â†’ Filled state: Malacca\n",
      "âœ… Merlimau â†’ Filled state: Jasin District\n",
      "âœ… Seberang Perai Utara â†’ Filled state: Seberang Perai\n",
      "âœ… Barat Daya â†’ Filled state: George Town\n",
      "âœ… Seberang Perai Selatan â†’ Filled state: Seberang Perai\n",
      "\n",
      "ğŸ” [Case 2] Filling missing city using mode per state...\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Melaka â†’ Filled city: Melaka Tengah\n",
      "âœ… Sabah â†’ Filled city: Kota Kinabalu\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Sarawak â†’ Filled city: Kuching\n",
      "âœ… Negeri Sembilan â†’ Filled city: Seremban\n",
      "âœ… Pahang â†’ Filled city: Kuantan\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Sarawak â†’ Filled city: Kuching\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Kedah â†’ Filled city: Kuala Muda\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Sabah â†’ Filled city: Kota Kinabalu\n",
      "âœ… Melaka â†’ Filled city: Melaka Tengah\n",
      "âœ… Pahang â†’ Filled city: Kuantan\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Sabah â†’ Filled city: Kota Kinabalu\n",
      "âœ… Sabah â†’ Filled city: Kota Kinabalu\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Sarawak â†’ Filled city: Kuching\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Johor â†’ Filled city: Johor Bahru\n",
      "âœ… Johor â†’ Filled city: Johor Bahru\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Sabah â†’ Filled city: Kota Kinabalu\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Melaka â†’ Filled city: Melaka Tengah\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Sabah â†’ Filled city: Kota Kinabalu\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Melaka â†’ Filled city: Melaka Tengah\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Melaka â†’ Filled city: Melaka Tengah\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Negeri Sembilan â†’ Filled city: Seremban\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Johor â†’ Filled city: Johor Bahru\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Sabah â†’ Filled city: Kota Kinabalu\n",
      "âœ… Sabah â†’ Filled city: Kota Kinabalu\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Negeri Sembilan â†’ Filled city: Seremban\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Negeri Sembilan â†’ Filled city: Seremban\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Melaka â†’ Filled city: Melaka Tengah\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Perak â†’ Filled city: Kinta\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Johor â†’ Filled city: Johor Bahru\n",
      "âœ… Johor â†’ Filled city: Johor Bahru\n",
      "âœ… Johor â†’ Filled city: Johor Bahru\n",
      "âœ… Pahang â†’ Filled city: Kuantan\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Kelantan â†’ Filled city: Kota Bharu\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Johor â†’ Filled city: Johor Bahru\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Perak â†’ Filled city: Kinta\n",
      "âœ… Pahang â†’ Filled city: Kuantan\n",
      "âœ… Johor â†’ Filled city: Johor Bahru\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Kelantan â†’ Filled city: Kota Bharu\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Terengganu â†’ Filled city: Kuala Terengganu\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Sabah â†’ Filled city: Kota Kinabalu\n",
      "âœ… Johor â†’ Filled city: Johor Bahru\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Johor â†’ Filled city: Johor Bahru\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Johor â†’ Filled city: Johor Bahru\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Negeri Sembilan â†’ Filled city: Seremban\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Sabah â†’ Filled city: Kota Kinabalu\n",
      "âœ… Sarawak â†’ Filled city: Kuching\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Perlis â†’ Filled city: Kuala Perlis\n",
      "âœ… Pahang â†’ Filled city: Kuantan\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Johor â†’ Filled city: Johor Bahru\n",
      "âœ… Perak â†’ Filled city: Kinta\n",
      "âœ… Pahang â†’ Filled city: Kuantan\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Perak â†’ Filled city: Kinta\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Negeri Sembilan â†’ Filled city: Seremban\n",
      "âœ… Melaka â†’ Filled city: Melaka Tengah\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Pahang â†’ Filled city: Kuantan\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Pulau Pinang â†’ Filled city: Unknown\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Selangor â†’ Filled city: Petaling Jaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Putrajaya â†’ Filled city: Putrajaya\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "âœ… Wilayah Persekutuan Kuala Lumpur â†’ Filled city: Kuala Lumpur\n",
      "\n",
      "ğŸ” [Case 3] Filling rows where both city & state are missing using mode pair...\n",
      "[LOG] Filled 84 rows with mode pair â†’ City: Kuala Lumpur, State: Wilayah Persekutuan Kuala Lumpur\n",
      "âœ… [STAGE 4 COMPLETE] Missing values handled.\n",
      "\n",
      "========== [FINAL STAGE START] Save Cleaned Dataset ==========\n",
      "âœ… [FINAL STAGE COMPLETE] Cleaned dataset saved as: 2021 - 2025 Customer - Copy_cleaned.csv\n",
      "\n",
      "==========================================================\n",
      "ğŸ‰ Data cleaning pipeline completed successfully!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerid</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUST0001</td>\n",
       "      <td>Sungai Besi</td>\n",
       "      <td>Wilayah Persekutuan Kuala Lumpur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUST0002</td>\n",
       "      <td>Mutiara Damansara</td>\n",
       "      <td>Selangor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUST0003</td>\n",
       "      <td>Shah Alam</td>\n",
       "      <td>Selangor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUST0004</td>\n",
       "      <td>Muar</td>\n",
       "      <td>Johor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUST0005</td>\n",
       "      <td>Others</td>\n",
       "      <td>Selangor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customerid               city                             state\n",
       "0   CUST0001        Sungai Besi  Wilayah Persekutuan Kuala Lumpur\n",
       "1   CUST0002  Mutiara Damansara                          Selangor\n",
       "2   CUST0003          Shah Alam                          Selangor\n",
       "3   CUST0004               Muar                             Johor\n",
       "4   CUST0005             Others                          Selangor"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep a copy for comparison\n",
    "original_customer_df = customer_df.copy()\n",
    "cleaned_customer_df, cleaned_file_name = clean_customer_dataset(customer_df, original_customer_dataset_name)\n",
    "cleaned_customer_df.head()\n",
    "# cleaned_customer_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcb11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3199 entries, 0 to 3198\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   customerid  3199 non-null   object\n",
      " 1   city        3199 non-null   object\n",
      " 2   state       3199 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 75.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "customerid    0\n",
       "city          0\n",
       "state         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_customer_df.info()\n",
    "cleaned_customer_df.isna().sum()\n",
    "#BEFORE and AFTER Report - 24/10/2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP Kernel",
   "language": "python",
   "name": "fyp_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
