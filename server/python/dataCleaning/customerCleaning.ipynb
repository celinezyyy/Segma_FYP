{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d01967",
   "metadata": {},
   "source": [
    "## Customer Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30abd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install fuzzywuzzy\n",
    "# !python -m pip install --upgrade pip\n",
    "# !pip install pycountry\n",
    "# %pip install python-Levenshtein\n",
    "# %pip install pandas numpy matplotlib seaborn scikit-learn jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate kernal:\n",
    "# .\\venv_fyp\\Scripts\\activate\n",
    "\n",
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, date\n",
    "from fuzzywuzzy import process, fuzz\n",
    "import pycountry\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4769ad2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Date of Birth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUST0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sungai Besi</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUST0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUST0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mutiara Damansara</td>\n",
       "      <td>Selangor</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUST0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUST0003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shah Alam</td>\n",
       "      <td>Selangor</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CustomerID  Date of Birth  Gender               City         State   Country\n",
       "0   CUST0001            NaN     NaN        Sungai Besi  Kuala Lumpur  Malaysia\n",
       "1   CUST0001            NaN     NaN                NaN           NaN       NaN\n",
       "2   CUST0002            NaN     NaN  Mutiara Damansara      Selangor  Malaysia\n",
       "3   CUST0002            NaN     NaN                NaN           NaN       NaN\n",
       "4   CUST0003            NaN     NaN          Shah Alam      Selangor  Malaysia"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load dataset\n",
    "# Replace 'customer_dataset.csv' with your actual file name or path\n",
    "file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Soapan Santun/2021 - 2025 Customer - Copy.csv\"\n",
    "\n",
    "original_customer_dataset_name = \"2021 - 2025 Customer - Copy.csv\"\n",
    "\n",
    "# Read dataset\n",
    "customer_df = pd.read_csv(file_path)\n",
    "\n",
    "# Show first few rows (original raw data)\n",
    "customer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe554a9",
   "metadata": {},
   "source": [
    "### Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd905e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706371d9",
   "metadata": {},
   "source": [
    "### Perform Data Cleaning Pipeline - CustomerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508463d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================= STAGE 1: SCHEMA & COLUMN VALIDATION =============================================\n",
    "# # Optional columns\n",
    "def check_optional_columns(df, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Check optional columns for fill percentage and drop columns that are mostly empty.\n",
    "    Returns the modified DataFrame and a friendly message.\n",
    "    \"\"\"\n",
    "    \n",
    "    optional_columns = [\"date of birth\", \"gender\"]\n",
    "    dropped_columns = []\n",
    "\n",
    "    for col in optional_columns:\n",
    "        if col in df.columns:\n",
    "            fill_ratio = df[col].count() / len(df) \n",
    "            if fill_ratio < threshold:\n",
    "                dropped_columns.append(col)\n",
    "                df.drop(columns=[col], inplace=True)  # Drop the column immediately\n",
    "                # df[col].count(): This counts the number of non-missing (non-null/non-NaN) values in the current column (col).\n",
    "                # len(df): This gives the total number of rows in the DataFrame.\n",
    "                # fill_ratio: The division calculates the proportion of filled (non-missing) values in that column. A ratio of 1.0 means the column is entirely filled; a ratio of 0.1 means 90% of the values are missing.\n",
    "\n",
    "    # Generate user-friendly message\n",
    "    if dropped_columns:\n",
    "        dropped_str = \", \".join(dropped_columns)\n",
    "        message = (\n",
    "            f\"We noticed that very few entries were provided for {dropped_str}. \"\n",
    "            \"These columns have been removed. \"\n",
    "            \"Segmentation will still be performed using geographic (City, State, Country) \"\n",
    "            \"and behavioral data (e.g., orders, purchase items, total spend).\"\n",
    "        )\n",
    "    else:\n",
    "        message = \"All optional columns have enough data and are kept for analysis.\"\n",
    "    \n",
    "    return df, message\n",
    "\n",
    "# Mandatory columns \n",
    "def check_mandatory_columns(df, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Check mandatory columns for missing values (column-wise).\n",
    "    Does not drop columns ‚Äî only warns user if any column is too incomplete.\n",
    "    Returns the DataFrame and a message summarizing issues.\n",
    "    \"\"\"\n",
    "\n",
    "    mandatory_columns = [\"customerid\", \"city\", \"state\"]\n",
    "\n",
    "    missing_report = []\n",
    "    warning_columns = []\n",
    "\n",
    "    for col in mandatory_columns:\n",
    "        if col in df.columns:\n",
    "            fill_ratio = df[col].count() / len(df)\n",
    "            missing_percent = (1 - fill_ratio) * 100\n",
    "\n",
    "            missing_report.append(f\"{col}: {missing_percent:.1f}% missing\")\n",
    "\n",
    "            # Warn if missing exceeds threshold\n",
    "            if fill_ratio < threshold:\n",
    "                warning_columns.append(col)\n",
    "        else:\n",
    "            # Handle case where column completely missing\n",
    "            missing_report.append(f\"{col}: column not found (100% missing)\")\n",
    "            warning_columns.append(col)\n",
    "\n",
    "    # Generate friendly message\n",
    "    if warning_columns:\n",
    "        warning_str = \", \".join(warning_columns)\n",
    "        message = (\n",
    "            f\"Some key fields have a high number of missing values: {warning_str}. \"\n",
    "            \"The system will still continue cleaning and processing, \"\n",
    "            \"but missing values will be handled automatically by our system. \"\n",
    "            \"Please ensure your source data is as complete as possible for more accurate segmentation results.\\n\\n\"\n",
    "            \"Missing Data Summary:\\n\" + \"\\n\".join(missing_report)\n",
    "        )\n",
    "    else:\n",
    "        message = (\n",
    "            \"All mandatory columns have sufficient data and are ready for cleaning.\\n\\n\"\n",
    "            \"Missing Data Summary:\\n\" + \"\\n\".join(missing_report)\n",
    "        )\n",
    "\n",
    "    return df, message\n",
    "\n",
    "# ============================================= STAGE 2: REMOVE DUPLICATE ENTRY ROW =================================================\n",
    "def remove_duplicate_entries(df):\n",
    "    \"\"\"Remove duplicate rows, keeping the first occurrence\"\"\"\n",
    "    df = df.drop_duplicates(keep='first')\n",
    "    return df\n",
    "\n",
    "# ============================================= STAGE 3: STANDARDIZATION & NORMALIZATION =============================================\n",
    "\n",
    "def normalize_columns_name(df):\n",
    "    \"\"\"Normalize column names: lowercase, strip spaces\"\"\"\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    return df\n",
    "\n",
    "# ===============================================================================\n",
    "\n",
    "def standardize_customer_id(df):\n",
    "    \"\"\"Standardize CustomerID format\"\"\"\n",
    "    if 'customerid' in df.columns:\n",
    "        df['customerid'] = df['customerid'].astype(str).str.strip().str.upper()\n",
    "    return df\n",
    "# Might have special case of dirty data exist such as \"****\", \"1234....\", \"annbwbciwbciowb\"\n",
    "# not sure how to handle it (Currently will say bcs we focus on small business enterprise that have use digital system, so normally customerID will not have inconsistent format issue, even the inconsistant format exist, at the end this row will not be use as when we merge we cant found that customerID)\n",
    "\n",
    "# ===============================================================================\n",
    "\n",
    "def standardize_dob(df):\n",
    "    \"\"\"Standardize Date of Birth column and convert to YYYY-MM-DD\"\"\"\n",
    "    # Rename only 'date of birth' to 'dob'\n",
    "    df = df.rename(columns={'date of birth': 'dob'})  \n",
    "    if 'dob' in df.columns:\n",
    "        def parse_date(x):\n",
    "            if pd.isnull(x):\n",
    "                return np.nan\n",
    "            for format in (\"%d/%m/%Y\", \"%m-%d-%y\", \"%Y-%m-%d\", \"%d-%b-%Y\", \"%d-%m-%Y\"):    \n",
    "                try:\n",
    "                    return datetime.strptime(str(x), format).date() # Final format: YYYY-MM-DD | 2025-10-15\n",
    "                except Exception:\n",
    "                    continue\n",
    "            return np.nan  # If no valid format found\n",
    "        df['dob'] = df['dob'].apply(parse_date)\n",
    "    return df\n",
    "\n",
    "# %d/%m/%Y ‚Üí 12/05/2000\n",
    "# %m-%d-%y ‚Üí 05-12-00\n",
    "# %Y-%m-%d ‚Üí 2000-05-12\n",
    "# %d-%b-%Y ‚Üí 12-May-2000\n",
    "# %d-%m-%Y ‚Üí 12-5-2000\n",
    "\n",
    "# ===============================================================================\n",
    "\n",
    "def derive_age_features(df):\n",
    "    \"\"\"Derive Age from DOB\"\"\"\n",
    "    if 'dob' in df.columns:\n",
    "        today = date.today()\n",
    "        df['age'] = df['dob'].apply(\n",
    "            lambda x: today.year - x.year - ((today.month, today.day) < (x.month, x.day))\n",
    "            if pd.notnull(x) else np.nan\n",
    "        )\n",
    "    return df\n",
    "# Example: ((today.month, today.day) < (x.month, x.day))\n",
    "# (10,15) < (12,1) ‚Üí True (birthday in Dec is after Oct 15)\n",
    "# (10,15) < (10,16) ‚Üí True (birthday tomorrow)\n",
    "# (10,15) < (5,20) ‚Üí False (birthday already passed)\n",
    "\n",
    "# This function calculates each person‚Äôs age from their date of birth (dob) by subtracting their birth year from the current year and adjusting if their birthday hasn‚Äôt occurred yet this year.\n",
    "\n",
    "# ===============================================================================\n",
    "\n",
    "def derive_age_group(df):\n",
    "    \"\"\"Derive Age Group based on defined buckets\"\"\"\n",
    "    if 'age' in df.columns:\n",
    "        def categorize_age(age):\n",
    "            if pd.isnull(age):\n",
    "                return 'Unknown'\n",
    "            if age < 18: return 'Below 18'\n",
    "            elif 18 <= age <= 24: return '18-24'\n",
    "            elif 25 <= age <= 34: return '25-34'\n",
    "            elif 35 <= age <= 44: return '35-44'\n",
    "            elif 45 <= age <= 54: return '45-54'\n",
    "            elif 55 <= age <= 64: return '55-64'\n",
    "            else: return 'Above 65'\n",
    "        df['age_group'] = df['age'].apply(categorize_age)\n",
    "    return df\n",
    "# ===============================================================================\n",
    "\n",
    "def drop_dob_after_age_derived(df):\n",
    "    \"\"\"Drop DOB column after deriving age and age_group\"\"\"\n",
    "    if 'dob' in df.columns:\n",
    "        df = df.drop(columns=['dob'])\n",
    "    return df\n",
    "\n",
    "# =================================================================================\n",
    "\n",
    "def standardize_gender(df):\n",
    "    \"\"\"Clean and standardize gender values\"\"\"\n",
    "    if 'gender' in df.columns:\n",
    "        # Clean text (remove spaces, make lowercase)\n",
    "        df['gender'] = df['gender'].astype(str).str.strip().str.lower()\n",
    "\n",
    "        # Standardize using keyword detection\n",
    "        def detect_gender(value):\n",
    "            if any(word in value for word in ['m', 'male', 'man', 'boy']):\n",
    "                return 'Male'\n",
    "            elif any(word in value for word in ['f', 'female', 'woman', 'girl']):\n",
    "                return 'Female'\n",
    "            else:\n",
    "                return 'Unknown'\n",
    "\n",
    "        df['gender'] = df['gender'].apply(detect_gender)\n",
    "    return df\n",
    "\n",
    "# ==================================================================================\n",
    "\n",
    "def standardize_location(df):\n",
    "    \"\"\"Standardize City, and State fields\"\"\"\n",
    "    \n",
    "    # Helper function: detect suspicious city names\n",
    "    def is_suspicious_city(name):\n",
    "        if not name or name.strip() == '':\n",
    "            return True\n",
    "        name = str(name).strip()\n",
    "        # Too short or too long\n",
    "        if len(name) < 2 or len(name) > 50:\n",
    "            return True\n",
    "        # Contains non-alphabetic or weird symbols\n",
    "        if re.search(r'[^A-Za-z\\s\\'-]', name):  # letters, space, apostrophe, dash allowed\n",
    "            return True\n",
    "        # Repeated characters (e.g., \"Ccciiiittty\")\n",
    "        if re.search(r'(.)\\1{3,}', name):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # --- City ---\n",
    "    if 'city' in df.columns:\n",
    "        df['city'] = df['city'].astype(str).str.title().str.strip()\n",
    "        df['city'] = df['city'].apply(lambda x: 'Unknown' if is_suspicious_city(x) else x)\n",
    "\n",
    "    # --- State ---\n",
    "    if 'state' in df.columns:\n",
    "        malaysia_states = [sub.name for sub in pycountry.subdivisions if sub.country_code == 'MY']\n",
    "        df['state'] = df['state'].astype(str).str.title().str.strip()\n",
    "        df['state'] = df['state'].apply(\n",
    "            lambda x: process.extractOne(x, malaysia_states, scorer=fuzz.token_sort_ratio)[0] if x else 'Unknown'\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================= STAGE 4: MISSING VALUE HANDLING =============================================\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Handle missing values using a column-based approach.\n",
    "    Each column is treated independently based on its type and business logic.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- 1. Customer ID -----\n",
    "    if 'customerid' in df.columns:\n",
    "        df = df[df['customerid'].notna()]  # Drop rows without ID\n",
    "\n",
    "    # ----- 2. Age -----\n",
    "    if 'age' in df.columns:\n",
    "        missing_ratio = df['age'].isna().mean()\n",
    "\n",
    "        if missing_ratio > 0:\n",
    "            print(f\"Missing ratio for age: {missing_ratio:.2%}\")\n",
    "            \n",
    "            if 'gender' in df.columns and df['gender'].nunique() > 1:\n",
    "                # Group by gender if available\n",
    "                df['age'] = df.groupby('gender')['age'].transform(\n",
    "                    lambda x: x.fillna(x.median())\n",
    "                )\n",
    "                print(\"Applied gender-based median imputation for age.\")\n",
    "            else:\n",
    "                # No gender column found or only one unique gender\n",
    "                print(\"Gender column not available or not diverse. Using overall median for imputation.\")\n",
    "            \n",
    "            # Always fill any remaining missing values with overall median\n",
    "            df['age'] = df['age'].fillna(df['age'].median())\n",
    "            \n",
    "            # Update derived column if needed\n",
    "            df = derive_age_group(df)\n",
    "\n",
    "    # ----- 4. Gender -----\n",
    "    if 'gender' in df.columns:\n",
    "        df['gender'] = df['gender'].fillna(df['gender'].mode()[0])\n",
    "# CHECK THE REST (SUSU, HOPE CAN SOLVE BY TOMORROW)\n",
    "    # ----- 5. City -----\n",
    "    if 'city' in df.columns and 'state' in df.columns:\n",
    "        # Precompute the most common city per state once\n",
    "        city_mode_by_state = (\n",
    "            df[df['city'] != 'Unknown']\n",
    "            .groupby('state')['city']\n",
    "            .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown')\n",
    "            .to_dict()\n",
    "        )\n",
    "\n",
    "        # Replace 'Unknown' with that state's most common city if available\n",
    "        df['city'] = df.apply(\n",
    "            lambda r: city_mode_by_state.get(r['state'], 'Unknown')\n",
    "            if r['city'] == 'Unknown' and pd.notna(r['state'])\n",
    "            else r['city'],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # --- State ---\n",
    "    if 'state' in df.columns:\n",
    "        df['state'] = df['state'].fillna(df['state'].mode()[0])\n",
    "    return df\n",
    "\n",
    "# ============================================= STAGE 5: OUTLIER DETECTION =============================================\n",
    "\n",
    "def detect_outliers(df):\n",
    "    \"\"\"Detect outliers in Age\"\"\"\n",
    "    if 'age' in df.columns:\n",
    "        # Convert to numeric just in case\n",
    "        df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "\n",
    "        # --- 1Ô∏è‚É£ Detect outliers using IQR method ---\n",
    "        Q1 = df['age'].quantile(0.25)\n",
    "        Q3 = df['age'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Mark extreme outliers as NaN\n",
    "        df.loc[(df['age'] < lower_bound) | (df['age'] > upper_bound), 'age'] = np.nan\n",
    "\n",
    "        print(f\"Outlier detection complete. Replaced ages outside [{lower_bound:.1f}, {upper_bound:.1f}] with NaN.\")\n",
    "\n",
    "        # --- 2Ô∏è‚É£ Handle missing/outlier ages ---\n",
    "        missing_ratio = df['age'].isna().mean()\n",
    "        if missing_ratio > 0:\n",
    "            print(f\"Missing ratio for age: {missing_ratio:.2%}\")\n",
    "            \n",
    "            if 'gender' in df.columns and df['gender'].nunique() > 1:\n",
    "                # Group by gender if available\n",
    "                df['age'] = df.groupby('gender')['age'].transform(\n",
    "                    lambda x: x.fillna(x.median())\n",
    "                )\n",
    "                print(\"Applied gender-based median imputation for age.\")\n",
    "            else:\n",
    "                # No gender column found or only one unique gender\n",
    "                df['age'] = df['age'].fillna(df['age'].median())\n",
    "                print(\"Applied overall median imputation for age.\")\n",
    "            \n",
    "        # --- 3Ô∏è‚É£ Update derived column ---\n",
    "        df = derive_age_group(df)\n",
    "        return df\n",
    "\n",
    "# ============================================= STAGE 6: DEDUPLICATION =============================================\n",
    "# This function removes duplicate CustomerIDs and keeps the row with the most non-missing data to preserve the most complete customer record.\n",
    "def deduplicate_customers(df):\n",
    "    \"\"\"Handle duplicate customer records intelligently\"\"\"\n",
    "    if 'customerid' not in df.columns:\n",
    "        return df  # Skip if no customerid column\n",
    "    \n",
    "    merged_records = []\n",
    "\n",
    "    for cust_id, group in df.groupby('customerid'):\n",
    "        merged_row = {'customerid': cust_id}\n",
    "        for col in df.columns:\n",
    "            if col == 'customerid':\n",
    "                continue\n",
    "            \n",
    "            # Get all unique non-null values for this customer\n",
    "            values = group[col].dropna().unique()\n",
    "            \n",
    "            if len(values) == 0:\n",
    "                merged_row[col] = pd.NA\n",
    "            elif len(values) == 1:\n",
    "                merged_row[col] = values[0]\n",
    "            else:\n",
    "                # Multiple conflicting values ‚Üí choose most frequent (mode)\n",
    "                merged_row[col] = group[col].mode()[0]\n",
    "        \n",
    "        merged_records.append(merged_row)\n",
    "    \n",
    "    return pd.DataFrame(merged_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_customer_dataset(df, original_customer_dataset_name):\n",
    "    \"\"\"\n",
    "    Main cleaning pipeline for customer dataset.\n",
    "    Executes all stages in proper order:\n",
    "    0. Column Normalization\n",
    "    1. Schema & Column Validation\n",
    "    2. Duplicate Entry Removal\n",
    "    3. Standardization & Normalization\n",
    "    4. Missing Value Handling\n",
    "    5. Outlier Detection\n",
    "    6. Deduplication\n",
    "    Finally, saves the cleaned dataset and returns it.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting data cleaning pipeline...\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 0: NORMALIZE COLUMN NAMES\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 0 START] Normalize Column Names ==========\")\n",
    "    df = normalize_columns_name(df)\n",
    "    print(\"‚úÖ [STAGE 0 COMPLETE] Column names normalized.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 1: SCHEMA & COLUMN VALIDATION\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 1 START] Schema & Column Validation ==========\")\n",
    "    df, optional_msg = check_optional_columns(df)\n",
    "    df, mandatory_msg = check_mandatory_columns(df)\n",
    "    print(optional_msg)\n",
    "    print(mandatory_msg)\n",
    "    print(\"‚úÖ [STAGE 1 COMPLETE] Schema validation done.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 2: REMOVE DUPLICATE ENTRY ROWS\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 2 START] Remove Duplicate Entry Rows ==========\")\n",
    "    df = remove_duplicate_entries(df)\n",
    "    print(\"‚úÖ [STAGE 2 COMPLETE] Duplicate entries removed.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 3: STANDARDIZATION & NORMALIZATION\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 3 START] Standardization & Normalization ==========\")\n",
    "    df = standardize_customer_id(df)\n",
    "    df = standardize_dob(df)\n",
    "    df = derive_age_features(df)\n",
    "    df = derive_age_group(df)\n",
    "    df = drop_dob_after_age_derived(df)\n",
    "    df = standardize_gender(df)\n",
    "    df = standardize_location(df)\n",
    "    print(\"‚úÖ [STAGE 3 COMPLETE] Standardization and normalization finished.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 4: MISSING VALUE HANDLING\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 4 START] Missing Value Handling ==========\")\n",
    "    df = handle_missing_values(df)\n",
    "    print(\"‚úÖ [STAGE 4 COMPLETE] Missing values handled.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 5: OUTLIER DETECTION\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 5 START] Outlier Detection ==========\")\n",
    "    df = detect_outliers(df)   # make sure detect_outliers returns df\n",
    "    print(\"‚úÖ [STAGE 5 COMPLETE] Outliers handled.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 6: DEDUPLICATION\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 6 START] Deduplication ==========\")\n",
    "    df = deduplicate_customers(df)\n",
    "    print(\"‚úÖ [STAGE 6 COMPLETE] Duplicate CustomerIDs deduplicated.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # SAVE CLEANED DATASET\n",
    "    # =============================================\n",
    "    print(\"========== [FINAL STAGE START] Save Cleaned Dataset ==========\")\n",
    "    base_name, ext = os.path.splitext(original_customer_dataset_name)\n",
    "    cleaned_file = f\"{base_name}_cleaned{ext}\"\n",
    "    df.to_csv(cleaned_file, index=False)\n",
    "    print(f\"‚úÖ [FINAL STAGE COMPLETE] Cleaned dataset saved as: {cleaned_file}\\n\")\n",
    "\n",
    "    print(\"==========================================================\")\n",
    "    print(\"üéâ Data cleaning pipeline completed successfully!\\n\")\n",
    "    return df, cleaned_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dbb17ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting data cleaning pipeline...\n",
      "\n",
      "========== STAGE 1: SCHEMA & COLUMN VALIDATION ==========\n",
      "We noticed that very few entries were provided for date of birth, gender. These columns have been removed. Segmentation will still be performed using geographic (City, State, Country) and behavioral data (e.g., orders, purchase items, total spend).\n",
      "All mandatory columns have sufficient data and are ready for cleaning.\n",
      "\n",
      "Missing Data Summary:\n",
      "customerid: 0.0% missing\n",
      "city: 44.8% missing\n",
      "state: 46.9% missing\n",
      "country: 44.7% missing\n",
      "\n",
      "========== STAGE 2: REMOVE DUPLICATE ENTRY ROW ==========\n",
      "Duplicate entries removed.\n",
      "\n",
      "========== STAGE 3: STANDARDIZATION & NORMALIZATION ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11504\\2075746767.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['customerid'] = df['customerid'].astype(str).str.strip().str.upper()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization and normalization completed.\n",
      "\n",
      "========== STAGE 4: MISSING VALUE HANDLING ==========\n",
      "Missing values handled successfully.\n",
      "\n",
      "========== STAGE 5: OUTLIER DETECTION ==========\n",
      "Outliers handled (e.g., unrealistic ages set to NaN).\n",
      "\n",
      "========== STAGE 6: DEDUPLICATION ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_11504\\2075746767.py:197: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.loc[df.groupby('customerid').apply(lambda x: x.notna().sum(axis=1).idxmax())]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate CustomerIDs deduplicated by data completeness.\n",
      "\n",
      "‚úÖ Data cleaning pipeline completed successfully!\n",
      "Cleaned dataset saved as: 2021 - 2025 Customer - Copy_cleaned.csv\n",
      "Complete data cleaning pipeline execution for customer dataset.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'CustomerID'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Y3S2\\FYP\\venv_fyp_new\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'CustomerID'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m summary_df\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Generate table\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m before_after_summary = \u001b[43mgenerate_before_after_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_customer_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcleaned_customer_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Display nicely\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m===== Before vs After Cleaning Summary =====\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mgenerate_before_after_summary\u001b[39m\u001b[34m(df_before, df_after, example_rows)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_before.columns:\n\u001b[32m     16\u001b[39m     before_missing = df_before[col].isna().sum()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     after_missing = \u001b[43mdf_after\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m.isna().sum()\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Count changed or corrected entries\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_after.columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Y3S2\\FYP\\venv_fyp_new\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Y3S2\\FYP\\venv_fyp_new\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'CustomerID'"
     ]
    }
   ],
   "source": [
    "# Keep a copy for comparison\n",
    "original_customer_df = customer_df.copy()\n",
    "cleaned_customer_df, cleaned_file_name = clean_customer_dataset(customer_df, original_customer_dataset_name)\n",
    "\n",
    "def generate_before_after_summary(df_before, df_after, example_rows=1):\n",
    "    \"\"\"\n",
    "    Generate a summary table comparing before and after cleaning:\n",
    "    - Column name\n",
    "    - Missing before\n",
    "    - Missing after\n",
    "    - Number of invalid/changed entries\n",
    "    - Example of correction (first few corrected entries)\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "    for col in df_before.columns:\n",
    "        before_missing = df_before[col].isna().sum()\n",
    "        after_missing = df_after[col].isna().sum()\n",
    "        \n",
    "        # Count changed or corrected entries\n",
    "        if col in df_after.columns:\n",
    "            invalid_count = (df_before[col] != df_after[col]).sum()\n",
    "            # Get example corrections\n",
    "            example_corrections = df_after.loc[df_before[col] != df_after[col], col].head(example_rows).tolist()\n",
    "            example_str = \", \".join([str(x) for x in example_corrections]) if example_corrections else \"-\"\n",
    "        else:\n",
    "            invalid_count = \"-\"\n",
    "            example_str = \"-\"\n",
    "        \n",
    "        summary_data.append([col, before_missing, after_missing, invalid_count, example_str])\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data, columns=[\n",
    "        \"Column\", \"Before Missing\", \"After Missing\", \"Invalid/Changed Count\", \"Example of Correction\"\n",
    "    ])\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Generate table\n",
    "before_after_summary = generate_before_after_summary(original_customer_df, cleaned_customer_df)\n",
    "\n",
    "# Display nicely\n",
    "print(\"\\n===== Before vs After Cleaning Summary =====\\n\")\n",
    "print(before_after_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fae1c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY-09 - Perlis\n",
      "MY-02 - Kedah\n",
      "MY-16 - Wilayah Persekutuan Putrajaya\n",
      "MY-13 - Sarawak\n",
      "MY-06 - Pahang\n",
      "MY-10 - Selangor\n",
      "MY-03 - Kelantan\n",
      "MY-14 - Wilayah Persekutuan Kuala Lumpur\n",
      "MY-07 - Pulau Pinang\n",
      "MY-11 - Terengganu\n",
      "MY-04 - Melaka\n",
      "MY-15 - Wilayah Persekutuan Labuan\n",
      "MY-08 - Perak\n",
      "MY-01 - Johor\n",
      "MY-12 - Sabah\n",
      "MY-05 - Negeri Sembilan\n"
     ]
    }
   ],
   "source": [
    "malaysia_states = pycountry.subdivisions.get(country_code='MY')\n",
    "\n",
    "# Print them\n",
    "for state in malaysia_states:\n",
    "    print(f\"{state.code} - {state.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP Kernel",
   "language": "python",
   "name": "fyp_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
