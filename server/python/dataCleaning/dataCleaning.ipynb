{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d01967",
   "metadata": {},
   "source": [
    "## Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30abd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install fuzzywuzzy\n",
    "# !python -m pip install --upgrade pip\n",
    "# !pip install pycountry\n",
    "# %pip install python-Levenshtein\n",
    "# %pip install pandas numpy matplotlib seaborn scikit-learn jupyter\n",
    "\n",
    "# errors='coerce' in pd.to_numeric() means:\n",
    "# ‚ÄúTry to convert everything in this column into a number.\n",
    "# If it fails (e.g. the value is text or invalid), don‚Äôt crash ‚Äî just replace it with NaN.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate kernal:\n",
    "# .\\venv_fyp\\Scripts\\activate\n",
    "import common_utils, customer_cleaning, order_cleaning\n",
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pycountry\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca31a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose file\n",
    "\n",
    "# order_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Soapan Santun - Use/2021 - 2025 Order.csv\"\n",
    "# customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Soapan Santun - Use/2021 - 2025 Customer - Copy.csv\"\n",
    "\n",
    "file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Soapan Santun - Use/2021 - 2025 Customer - Copy.csv\"\n",
    "original_file_name = \"2021 - 2025 Customer - Copy.csv\"\n",
    "dataset_type = \"customer\"\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Generate output paths\n",
    "temp_dir = os.path.dirname(file_path)\n",
    "base_name, ext = os.path.splitext(original_file_name)\n",
    "\n",
    "cleaned_path = os.path.join(temp_dir, f\"{base_name}_cleaned{ext}\")\n",
    "report_path  = os.path.join(temp_dir, f\"{base_name}_report.json\")\n",
    "\n",
    "# Run cleaning pipeline\n",
    "if dataset_type == \"customer\":\n",
    "    cleaned_df, report = customer_cleaning.clean_customer_dataset(df, cleaned_path)\n",
    "else:\n",
    "    cleaned_df, report = order_cleaning.clean_order_dataset(df, cleaned_path)\n",
    "\n",
    "# Save report\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=4, default=str)\n",
    "\n",
    "cleaned_df.head(), report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d9910",
   "metadata": {},
   "source": [
    "### Generic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================ GENERIC FUNCTIONS ============================================ #\n",
    "\n",
    "# def normalize_columns_name(df):\n",
    "#     \"\"\"Normalize column names: lowercase, strip spaces\"\"\"\n",
    "#     print(\"[LOG - STAGE 0] Running normalize_columns_name...\")\n",
    "#     df.columns = df.columns.str.strip().str.lower()\n",
    "#     print(f\"[LOG - STAGE 0] Columns after normalization: {list(df.columns)}\")\n",
    "#     return df\n",
    "\n",
    "# def check_mandatory_columns(df, dataset_type, mandatory_columns, threshold=0.9):\n",
    "#     \"\"\"\n",
    "#     Generic function to check mandatory columns for both Customer and Order datasets.\n",
    "#     - dataset_type: 'customer' or 'order'\n",
    "#     - mandatory_columns: list of required columns for that dataset\n",
    "#     - threshold: minimum acceptable fill ratio (default 0.8)\n",
    "#     \"\"\"\n",
    "#     print(f\"[LOG - STAGE 1] Running check_mandatory_columns for {dataset_type} dataset...\")\n",
    "\n",
    "#     missing_report = []\n",
    "#     warning_columns = []\n",
    "\n",
    "#     # Step 1: Check each mandatory column\n",
    "#     for col in mandatory_columns:\n",
    "#         if col in df.columns:\n",
    "#             fill_ratio = df[col].notna().mean()\n",
    "#             print(f\"[LOG - STAGE 1] Mandatory column '{col}' fill ratio: {fill_ratio:.2f}\")\n",
    "#             missing_percent = (1 - fill_ratio) * 100\n",
    "#             missing_report.append(f\"{col}: {missing_percent:.1f}% missing\")\n",
    "\n",
    "#             if fill_ratio < threshold:\n",
    "#                 warning_columns.append(col)\n",
    "#         else:\n",
    "#             print(f\"[LOG - STAGE 1] Mandatory column '{col}' not found\")\n",
    "#             missing_report.append(f\"{col}: column not found (100% missing)\")\n",
    "#             warning_columns.append(col)\n",
    "\n",
    "#     # Step 2: Generate message\n",
    "#     if warning_columns:\n",
    "#         warning_str = \", \".join(warning_columns)\n",
    "#         warning_details = [report for report in missing_report if any(col in report for col in warning_columns)]\n",
    "        \n",
    "#         # Check if location fields (city/state) are in warning\n",
    "#         location_warning = any(col in ['city', 'state'] for col in warning_columns)\n",
    "        \n",
    "#         message = (\n",
    "#             f\"‚ö†Ô∏è WARNING - Critical Data Gaps Detected:\\n\\n\"\n",
    "#             f\"The following MANDATORY field(s) have excessive missing values (>10%): {warning_str}\\n\\n\"\n",
    "#         )\n",
    "#         if warning_details:\n",
    "#             message += \"Missing Data Details:\\n\" + \"\\n\".join(f\"  ‚Ä¢ {d}\" for d in warning_details) + \"\\n\\n\"\n",
    "        \n",
    "#         message += (\n",
    "#             f\"‚ö†Ô∏è Impact: Missing mandatory data can significantly reduce the accuracy of your {dataset_type} segmentation results.\\n\\n\"\n",
    "#             f\"How Missing Values Will Be Handled:\\n\"\n",
    "#         )\n",
    "        \n",
    "#         # Add specific handling explanation based on dataset type and fields\n",
    "#         if dataset_type == \"customer\":\n",
    "#             message += (\n",
    "#                 f\"  ‚Ä¢ CustomerID: Rows without ID will be REMOVED (cannot be predicted)\\n\"\n",
    "#             )\n",
    "#             if location_warning:\n",
    "#                 message += (\n",
    "#                     f\"  ‚Ä¢ Location (City/State): Missing values will be filled using:\\n\"\n",
    "#                     f\"      1. Geocoding API (if city known, find correct state)\\n\"\n",
    "#                     f\"      2. Statistical mode (most frequent city/state in your data)\\n\"\n",
    "#                     f\"      3. If both missing: use most common city-state pair\\n\"\n",
    "#                     f\"    ‚ö†Ô∏è Note: This creates estimated data which may not reflect true customer locations\\n\"\n",
    "#                 )\n",
    "#             message += (\n",
    "#                 f\"  ‚Ä¢ Demographics (Age/Gender): Missing values will be kept as 'Unknown' for segmentation\\n\"\n",
    "#             )\n",
    "#         elif dataset_type == \"order\":\n",
    "#             message += (\n",
    "#                 f\"  ‚Ä¢ OrderID/CustomerID: Rows without IDs will be REMOVED\\n\"\n",
    "#                 f\"  ‚Ä¢ Financial data (Item Price/Total Spend): Missing values will be calculated or rows removed\\n\"\n",
    "#                 f\"  ‚Ä¢ Other fields: Filled with statistical defaults or 'Unknown'\\n\"\n",
    "#             )\n",
    "        \n",
    "#         message += (\n",
    "#             f\"\\nüî¥ STRONGLY RECOMMENDED:\\n\"\n",
    "#             f\"{'=' * 70}\\n\"\n",
    "#             f\"Please REVIEW and REUPLOAD your source data with complete information!\\n\"\n",
    "#         )\n",
    "        \n",
    "#         if location_warning:\n",
    "#             message += (\n",
    "#                 f\"\\n‚ö†Ô∏è LOCATION DATA IS CRITICAL for accurate customer segmentation!\\n\"\n",
    "#                 f\"   Filling missing locations with statistical estimates may lead to:\\n\"\n",
    "#                 f\"   ‚Ä¢ Incorrect geographic segments\\n\"\n",
    "#                 f\"   ‚Ä¢ Misleading regional analysis\\n\"\n",
    "#                 f\"   ‚Ä¢ Poor targeting accuracy\\n\"\n",
    "#                 f\"\\n   For best results, provide complete City and State information.\\n\"\n",
    "#             )\n",
    "        \n",
    "#         message += f\"{'=' * 70}\\n\"\n",
    "        \n",
    "#     else:\n",
    "#         complete_details = \"\\n\".join(f\"  ‚Ä¢ {report}\" for report in missing_report)\n",
    "#         message = (\n",
    "#             f\"‚úÖ All Mandatory Fields Validated:\\n\\n\"\n",
    "#             f\"All required {dataset_type} columns have sufficient data (‚â•90% filled) and are ready for cleaning.\\n\\n\"\n",
    "#             f\"Data Completeness:\\n{complete_details}\\n\\n\"\n",
    "#             f\"Note: We already clean missing values in these columns during the cleaning process.\"\n",
    "#         )\n",
    "\n",
    "#     return df, message\n",
    "\n",
    "# def remove_duplicate_entries(df):\n",
    "#     \"\"\"Remove duplicate rows, keeping the first occurrence\"\"\"\n",
    "#     print(\"[LOG - STAGE 2] Running remove_duplicate_entries...\")\n",
    "#     initial_len = len(df)\n",
    "#     df = df.drop_duplicates(keep='first', ignore_index=True)\n",
    "#     removed_dup = initial_len - len(df)\n",
    "#     message = None\n",
    "#     if removed_dup > 0:\n",
    "#         message = (f\"{removed_dup} duplicate records have found, we have removed it.\")\n",
    "#     print(f\"[LOG - STAGE 2] Removed {initial_len - len(df)} duplicate rows.\")\n",
    "#     return df, message\n",
    "\n",
    "# def standardize_customer_id(df):\n",
    "#     \"\"\"Standardize CustomerID format and keep null as NaN\"\"\"\n",
    "#     print(\"[LOG - STAGE 4] Running standardize_customer_id...\")\n",
    "\n",
    "#     if 'customerid' in df.columns:\n",
    "#         # Convert to string, strip spaces\n",
    "#         df.loc[:, 'customerid'] = df['customerid'].astype(str).str.strip().str.upper()\n",
    "\n",
    "#         # Convert empty string back to NaN\n",
    "#         df.loc[df['customerid'] == '', 'customerid'] = np.nan\n",
    "\n",
    "#         print(\"[LOG - STAGE 4] CustomerID column standardized (empty -> NaN)\")\n",
    "#     else:\n",
    "#         print(\"[LOG - STAGE 4] CustomerID column not found, skipping\")\n",
    "\n",
    "#     return df\n",
    "# # Might have special case of dirty data exist such as \"****\", \"1234....\", \"annbwbciwbciowb\"\n",
    "# # not sure how to handle it (Currently will say bcs we focus on small business enterprise that have use digital system, so normally customerID will not have inconsistent format issue, even the inconsistant format exist, at the end this row will not be use as when we merge we cant found that customerID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706371d9",
   "metadata": {},
   "source": [
    "## Perform Data Cleaning Pipeline - CustomerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Load dataset\n",
    "# # Replace 'customer_dataset.csv' with your actual file name or path\n",
    "# customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Soapan Santun - Use/2021 - 2025 Customer - Copy.csv\"\n",
    "# # customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Test Data/customer_dataset_balanced_5033_rows.csv\"\n",
    "# # customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Test Data/customer_dataset_light_7719_rows.csv\"\n",
    "# # customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Test Data/customer_dataset_stress_6047_rows.csv\"\n",
    "\n",
    "# original_customer_dataset_name = \"2021 - 2025 Customer - Copy.csv\"\n",
    "\n",
    "# # Read dataset\n",
    "# customer_df = pd.read_csv(customer_file_path)\n",
    "\n",
    "# # Show first few rows (original raw data)\n",
    "# customer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb18eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Load dataset\n",
    "# # Replace 'customer_dataset.csv' with your actual file name or path\n",
    "# order_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Soapan Santun - Use/2021 - 2025 Order.csv\"\n",
    "# # customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Test Data/customer_dataset_balanced_5033_rows.csv\"\n",
    "# # customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Test Data/customer_dataset_light_7719_rows.csv\"\n",
    "# # customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Test Data/customer_dataset_stress_6047_rows.csv\"\n",
    "\n",
    "# original_order_dataset_name = \"2021 - 2025 Order.csv\"\n",
    "\n",
    "# # Read dataset\n",
    "# order_df = pd.read_csv(order_file_path)\n",
    "\n",
    "# # Show first few rows (original raw data)\n",
    "# order_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508463d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Import libraries\n",
    "# # from common_utils import normalize_columns_name, check_mandatory_columns, remove_duplicate_entries, standardize_customer_id; \n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import pycountry\n",
    "# import re\n",
    "# import requests\n",
    "# import time\n",
    "# from datetime import datetime, date\n",
    "# from fuzzywuzzy import process, fuzz\n",
    "\n",
    "# # ============================================= (CUSTOMER DATASET) STAGE 0: NORMALIZE COLUMN NAMES =============================================\n",
    "# # From Generic function: normalize_columns_name\n",
    "\n",
    "# # ============================================= (CUSTOMER DATASET) STAGE 1: SCHEMA & COLUMN VALIDATION =============================================\n",
    "# # Optional columns & Mandatory columns(FROM GENERIC FUNCTION - check_mandatory_columns)\n",
    "# def customer_check_optional_columns(df, threshold=0.9):\n",
    "#     \"\"\"\n",
    "#     Check optional columns for fill percentage and drop columns that are mostly empty.\n",
    "#     Returns the modified DataFrame and a friendly message.\n",
    "#     \"\"\"\n",
    "#     print(\"[LOG - STAGE 1] Running customer_check_optional_columns...\")\n",
    "#     optional_columns = [\"date of birth\", \"gender\"]\n",
    "#     dropped_columns = []\n",
    "#     missing_report = []\n",
    "\n",
    "#     for col in optional_columns:\n",
    "#         if col in df.columns:\n",
    "#             fill_ratio = df[col].notna().mean()\n",
    "#             missing_percent = (1 - fill_ratio) * 100\n",
    "#             missing_report.append(f\"{col}: {missing_percent:.1f}% missing\")\n",
    "#             print(f\"[LOG - STAGE 1] Optional column '{col}' fill ratio: {fill_ratio:.2f}\")\n",
    "#             if fill_ratio < threshold:\n",
    "#                 dropped_columns.append(col)\n",
    "#                 df.drop(columns=[col], inplace=True)  # Drop the column immediately\n",
    "#                 # df[col].count(): This counts the number of non-missing (non-null/non-NaN) values in the current column (col).\n",
    "#                 # len(df): This gives the total number of rows in the DataFrame.\n",
    "#                 # fill_ratio: The division calculates the proportion of filled (non-missing) values in that column. A ratio of 1.0 means the column is entirely filled; a ratio of 0.1 means 90% of the values are missing.\n",
    "#                 print(f\"[LOG - STAGE 1] Dropped optional column '{col}' due to too many missing values\")\n",
    "#         else:\n",
    "#             print(f\"[LOG - STAGE 1] Optional column '{col}' not found\")\n",
    "#             missing_report.append(f\"{col}: column not found (100% missing)\")\n",
    "#             dropped_columns.append(col)\n",
    "\n",
    "#     # Generate user-friendly message\n",
    "#     if dropped_columns:\n",
    "#         dropped_str = \", \".join(dropped_columns)\n",
    "#         dropped_details = [report for report in missing_report if any(col in report for col in dropped_columns)]\n",
    "#         kept_details = [report for report in missing_report if not any(col in report for col in dropped_columns)]\n",
    "        \n",
    "#         message = (\n",
    "#             f\"Optional Columns Removed:\\n\\n\"\n",
    "#             f\"The following column(s) were removed due to insufficient data: {dropped_str}\\n\"\n",
    "#         )\n",
    "#         if dropped_details:\n",
    "#             message += \"Details:\\n\" + \"\\n\".join(f\"  ‚Ä¢ {d}\" for d in dropped_details) + \"\\n\\n\"\n",
    "        \n",
    "#         message += (\n",
    "#             \"Why? These columns had less than 90% complete data, which is too sparse for reliable analysis.\\n\\n\"\n",
    "#             \"Good News: Segmentation will still work using:\\n\"\n",
    "#             \"  ‚Ä¢ Geographic data (City, State)\\n\"\n",
    "#             \"  ‚Ä¢ Behavioral data (orders, purchase patterns, spending)\\n\"\n",
    "#         )\n",
    "        \n",
    "#         if kept_details:\n",
    "#             message += \"\\n\\n Columns Kept:\\n\" + \"\\n\".join(f\"  ‚Ä¢ {d}\" for d in kept_details)\n",
    "#     else:\n",
    "#         message = (\n",
    "#             \"All Optional Columns Retained:\\n\\n\"\n",
    "#             \"All optional columns (Date of Birth, Gender) have sufficient data (‚â•90% filled) and will be kept for analysis.\\n\\n\"\n",
    "#             \"Data Completeness:\\n\" + \"\\n\".join(f\"  ‚Ä¢ {report}\" for report in missing_report) + \"\\n\\n\"\n",
    "#             \"Note: We already clean missing values in these columns during the cleaning process.\"\n",
    "#         )\n",
    "    \n",
    "#     return df, message\n",
    "\n",
    "# # ============================================= (CUSTOMER DATASET) STAGE 2: REMOVE DUPLICATE ENTRY ROW =================================================\n",
    "# # From Generic function: remove_duplicate_entries\n",
    "\n",
    "# # ============================================= (CUSTOMER DATASET) STAGE 3: DEDUPLICATE =================================================\n",
    "# def deduplicate_customers(df):\n",
    "#     print(\"[LOG - STAGE 3] Running deduplicate_customers...\")\n",
    "#     if 'customerid' not in df.columns:\n",
    "#         print(\"[LOG - STAGE 3] 'customerid' column missing, skipping deduplication\")\n",
    "#         return df\n",
    "\n",
    "#     before_dup_id = len(df)\n",
    "#     def resolve_conflict(series):\n",
    "#         vals = series.dropna().unique()\n",
    "#         if len(vals) == 0:\n",
    "#             return pd.NA\n",
    "#         elif len(vals) == 1:\n",
    "#             return vals[0]\n",
    "#         else:\n",
    "#             return series.mode().iloc[0]\n",
    "\n",
    "#     # Vectorized groupby instead of per-group loop\n",
    "#     df = df.groupby('customerid', as_index=False).agg(resolve_conflict)\n",
    "    \n",
    "#     removed_dup_id = before_dup_id - len(df)\n",
    "#     if removed_dup_id > 0:\n",
    "#         message = (f\"{removed_dup_id} duplicate CustomerIDs removed.\")\n",
    "#     else:\n",
    "#         message = \"No duplicate CustomerIDs found.\"\n",
    "#     print(\"[LOG - STAGE 3] Deduplication complete (vectorized)\")\n",
    "#     return df, message\n",
    "\n",
    "# # ============================================= (CUSTOMER DATASET) STAGE 4: STANDARDIZATION & NORMALIZATION =============================================\n",
    "# # From Generic function: standardize_customer_id\n",
    "\n",
    "# def standardize_dob(df):\n",
    "#     \"\"\"Standardize Date of Birth column and convert to YYYY-MM-DD\"\"\"\n",
    "#     print(\"[LOG - STAGE 4] Running standardize_dob...\")\n",
    "#     # Rename only 'date of birth' to 'dob'\n",
    "#     message = None\n",
    "#     df = df.rename(columns={'date of birth': 'dob'})  \n",
    "#     if 'dob' in df.columns:\n",
    "#         print(\"[LOG - STAGE 4] DOB column found, parsing dates...\")\n",
    "#         def parse_date(x):\n",
    "#             if pd.isnull(x):\n",
    "#                 return pd.NaT\n",
    "#             for format in (\"%d/%m/%Y\", \"%m-%d-%y\", \"%Y-%m-%d\", \"%d-%b-%Y\", \"%d-%m-%Y\", \"%d %B %Y\", \"%B %d, %Y\", \"%b %d %Y\"):    \n",
    "#                 try:\n",
    "#                     return datetime.strptime(str(x), format).date() # Final format: YYYY-MM-DD | 2025-10-15\n",
    "#                 except Exception:\n",
    "#                     continue\n",
    "#             return pd.NaT  # If no valid format found\n",
    "#         df['dob'] = df['dob'].apply(parse_date)\n",
    "#         df['dob'] = pd.to_datetime(df['dob'])\n",
    "#         # Keep NaT as is - don't fill with \"Unknown\" string (causes issues with .year/.month/.day)\n",
    "#         print(\"[LOG - STAGE 4] DOB parsing complete. Invalid dates kept as NaT\")\n",
    "#         message = (\n",
    "#             \"Since your dataset includes a Date of Birth information, we derived two useful fields ‚Äî \"\n",
    "#             \"'age' and 'age_group' ‚Äî for segmentation purposes, and the original 'Date of Birth' column will be remove.\"\n",
    "#         )\n",
    "#     else:\n",
    "#         print(\"[LOG - STAGE 4] DOB column not found, skipping\")\n",
    "#     return df, message\n",
    "\n",
    "#     # %d/%m/%Y ‚Üí 12/05/2000\n",
    "#     # %m-%d-%y ‚Üí 05-12-00\n",
    "#     # %Y-%m-%d ‚Üí 2000-05-12\n",
    "#     # %d-%b-%Y ‚Üí 12-May-2000\n",
    "#     # %d-%m-%Y ‚Üí 12-5-2000\n",
    "#     # \"%d %B %Y\" ‚Üí 15 October 1998\n",
    "#     # \"%B %d, %Y\" ‚Üí October 15, 1998\n",
    "#     # \"%b %d %Y\" ‚Üí Oct 15 1998\n",
    "\n",
    "# # ===============================================================================\n",
    "\n",
    "# def derive_age_features(df):\n",
    "#     \"\"\"Derive Age from DOB\"\"\"\n",
    "#     print(\"[LOG - STAGE 4] Running derive_age_features...\")\n",
    "#     if 'dob' in df.columns:\n",
    "#         today = date.today()\n",
    "#         df['age'] = df['dob'].apply(\n",
    "#             lambda x: today.year - x.year - ((today.month, today.day) < (x.month, x.day))\n",
    "#             if pd.notnull(x) else None\n",
    "#         )\n",
    "#         df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "#         # Replace NaN with \"Unknown\" and ensure no empty strings\n",
    "#         df['age'] = df['age'].fillna(\"Unknown\")\n",
    "#         df['age'] = df['age'].replace('', 'Unknown')  # Handle empty strings\n",
    "#         print(\"[LOG - STAGE 4] Age derived from DOB, null age marked as Unknown\")\n",
    "#     else:\n",
    "#         # If DOB column doesn't exist, skip age creation\n",
    "#         print(\"[LOG - STAGE 4] DOB column not found, skipping age creation\")\n",
    "#     return df\n",
    "#     # Example: ((today.month, today.day) < (x.month, x.day))\n",
    "#     # (10,15) < (12,1) ‚Üí True (birthday in Dec is after Oct 15)\n",
    "#     # (10,15) < (10,16) ‚Üí True (birthday tomorrow)\n",
    "#     # (10,15) < (5,20) ‚Üí False (birthday already passed)\n",
    "\n",
    "#     # This function calculates each person‚Äôs age from their date of birth (dob) by subtracting their birth year from the current year and adjusting if their birthday hasn‚Äôt occurred yet this year.\n",
    "\n",
    "# # ===============================================================================\n",
    "\n",
    "# def derive_age_group(df):\n",
    "#     \"\"\"Derive Age Group based on defined buckets\"\"\"\n",
    "#     print(\"[LOG - STAGE 4] Running derive_age_group...\")\n",
    "#     if 'age' in df.columns:\n",
    "#         def categorize_age(age):\n",
    "#             if pd.isnull(age) or age == \"Unknown\" or age == '':\n",
    "#                 return 'Unknown'\n",
    "#             # Convert to numeric if string (safety check)\n",
    "#             if isinstance(age, str):\n",
    "#                 try:\n",
    "#                     age = float(age)\n",
    "#                 except:\n",
    "#                     return 'Unknown'\n",
    "#             if age < 18: return 'Below 18'\n",
    "#             elif 18 <= age <= 24: return '18-24'\n",
    "#             elif 25 <= age <= 34: return '25-34'\n",
    "#             elif 35 <= age <= 44: return '35-44'\n",
    "#             elif 45 <= age <= 54: return '45-54'\n",
    "#             elif 55 <= age <= 64: return '55-64'\n",
    "#             else: return 'Above 65'\n",
    "#         df['age_group'] = df['age'].apply(categorize_age)\n",
    "#         # Clean up any remaining empty strings or NaN\n",
    "#         df['age_group'] = df['age_group'].fillna(\"Unknown\")\n",
    "#         df['age_group'] = df['age_group'].replace('', 'Unknown')\n",
    "#         print(\"[LOG - STAGE 4] Age groups derived, null age_group marked as Unknown\")\n",
    "#     else:\n",
    "#         print(\"[LOG - STAGE 4] Age column not found, skipping age_group creation\")\n",
    "#     return df\n",
    "\n",
    "# # ===============================================================================\n",
    "\n",
    "# def drop_dob_after_age_derived(df):\n",
    "#     \"\"\"Drop DOB column after deriving age and age_group\"\"\"\n",
    "#     print(\"[LOG - STAGE 4] Running drop_dob_after_age_derived...\")\n",
    "#     if 'dob' in df.columns:\n",
    "#         df = df.drop(columns=['dob'])\n",
    "#         print(\"[LOG - STAGE 4] Dropped DOB column\")\n",
    "#     else:\n",
    "#         print(\"[LOG - STAGE 4] DOB column not found, skipping\")\n",
    "#     return df\n",
    "\n",
    "# # Column\t    Value when original DOB is null\n",
    "# # =============================================\n",
    "# # dob\t        \"Unknown\" \n",
    "# # age\t        \"Unknown\" \n",
    "# # age_group\t    \"Unknown\" \n",
    "# # =================================================================================\n",
    "\n",
    "# def standardize_gender(df):\n",
    "#     \"\"\"Clean and standardize gender values\"\"\"\n",
    "#     print(\"[LOG - STAGE 4] Running standardize_gender...\")\n",
    "#     if 'gender' in df.columns:\n",
    "#         # Normalize to lowercase for mapping\n",
    "#         gender_norm = df['gender'].astype(str).str.strip().str.lower()\n",
    "\n",
    "#         # Map common variants to canonical labels\n",
    "#         mapping = {\n",
    "#             'm': 'Male', 'male': 'Male', 'man': 'Male', 'boy': 'Male',\n",
    "#             'f': 'Female', 'female': 'Female', 'woman': 'Female', 'girl': 'Female'\n",
    "#         }\n",
    "#         gender_norm = gender_norm.replace(mapping)\n",
    "\n",
    "#         # Anything not exactly 'Male' or 'Female' becomes Unknown (e.g., other/others/na/null/empty)\n",
    "#         df['gender'] = gender_norm\n",
    "#         df.loc[~df['gender'].isin(['Male', 'Female']), 'gender'] = 'Unknown'\n",
    "#         print(\"[LOG - STAGE 4] Gender standardized (vectorized)\")\n",
    "#     else:\n",
    "#         print(\"[LOG - STAGE 4] Gender column not found, skipping\")\n",
    "#     return df\n",
    "\n",
    "# # ==================================================================================\n",
    "\n",
    "# def standardize_location(df):\n",
    "#     \"\"\"Standardize City, and State fields\"\"\"\n",
    "#     print(\"[LOG - STAGE 4] Running standardize_location...\")\n",
    "        \n",
    "#     # Helper function: detect suspicious city names\n",
    "#     def is_suspicious_city(name):\n",
    "#         if not name or name.strip() == '':\n",
    "#             return True\n",
    "#         name = str(name).strip()\n",
    "        \n",
    "#         if len(name) < 2 or len(name) > 50: # Too short or too long\n",
    "#             return True\n",
    "        \n",
    "#         if re.search(r'[^A-Za-z\\s\\'-]', name):  # Contains non-alphabetic or weird symbols   # letters, space, apostrophe, dash allowed\n",
    "#             return True\n",
    "        \n",
    "#         if re.search(r'(.)\\1{3,}', name):   # Repeated characters (e.g., \"Ccciiiittty\")\n",
    "#             return True\n",
    "#         return False\n",
    "\n",
    "#     # --- City ---\n",
    "#     if 'city' in df.columns:\n",
    "#         df['city'] = df['city'].fillna('').astype(str).str.title().str.strip()\n",
    "#         # Common city aliases (short forms, local spellings, etc.)\n",
    "#         city_alias_map = {\n",
    "#             \"Kl\": \"Kuala Lumpur\",\n",
    "#             \"PJ\": \"Petaling Jaya\",\n",
    "#         }\n",
    "#         # Apply alias replacements first\n",
    "#         df['city'] = df['city'].replace(city_alias_map)\n",
    "        \n",
    "#         suspicious_mask = df['city'].apply(lambda x: is_suspicious_city(x) or x.lower() in ['others', 'other'])\n",
    "#         suspicious_count = suspicious_mask.sum()\n",
    "#         df.loc[suspicious_mask, 'city'] = 'Unknown'\n",
    "        \n",
    "#         print(f\"[LOG - STAGE 4] Standardized 'city'. Suspicious/unknown entries set to 'Unknown': {suspicious_count}\")\n",
    "#     else:\n",
    "#         print(\"[LOG - STAGE 4] 'city' column not found, skipping city standardization\")\n",
    "    \n",
    "#     # --- State ---\n",
    "#     if 'state' in df.columns:\n",
    "#         # malaysia_states = [\"Johor\", \"Kedah\", \"Kelantan\", \"Melaka\", \"Negeri Sembilan\",\"Pahang\", \"Perak\", \"Perlis\", \"Pulau Pinang\", \"Sabah\", \"Sarawak\", \"Selangor\", \"Terengganu\", \"Kuala Lumpur\", \"Labuan\", \"Putrajaya\"]\n",
    "#         malaysia_states = [sub.name for sub in pycountry.subdivisions if sub.country_code == 'MY']\n",
    "#         alias_map = {\n",
    "#             \"Kuala Lumpur\": \"Wilayah Persekutuan Kuala Lumpur\",\n",
    "#             \"Kl\": \"Wilayah Persekutuan Kuala Lumpur\",\n",
    "#             \"Labuan\": \"Wilayah Persekutuan Labuan\",\n",
    "#             \"Putrajaya\": \"Wilayah Persekutuan Putrajaya\"\n",
    "#         }\n",
    "\n",
    "#         df['state'] = df['state'].fillna('').astype(str).str.title().str.strip()\n",
    "#         # Match only unique states once\n",
    "#         unique_states = df['state'].unique()\n",
    "#         state_map = {}\n",
    "#         for s in unique_states:\n",
    "#             s_clean = s.strip().title()\n",
    "\n",
    "#             if not s_clean or s_clean == 'Unknown':\n",
    "#                 state_map[s] = 'Unknown'\n",
    "\n",
    "#             elif s_clean in alias_map:   # Check alias first\n",
    "#                 state_map[s] = alias_map[s_clean]\n",
    "\n",
    "#             else:\n",
    "#                 match, score = process.extractOne(s_clean, malaysia_states, scorer=fuzz.token_sort_ratio)\n",
    "#                 state_map[s] = match if score >= 80 else 'Unknown'\n",
    "\n",
    "#         # Apply mapping to the dataframe\n",
    "#         df['state'] = df['state'].map(state_map)\n",
    "#         print(\"[LOG - STAGE 4] State standardized (cached fuzzy matching)\")\n",
    "#     else:\n",
    "#         print(\"[LOG - STAGE 4] 'state' column not found, skipping state standardization\")\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # ============================================= (CUSTOMER DATASET) STAGE 5: MISSING VALUE HANDLING =============================================\n",
    "# # Only handle for customerid and location fields\n",
    "# def handle_missing_values_customer(df):\n",
    "#     print(\"[LOG - STAGE 5] Running handle_missing_values...\")\n",
    "\n",
    "#     API_KEY = \"68f8ce9a38c3f632237334dyiedb96e\"\n",
    "#     GEOCODE_URL = \"https://geocode.maps.co/search\"\n",
    "#     SLEEP_TIME = 1.2\n",
    "#     cache = {}  # ‚ö° moved outside loops\n",
    "    \n",
    "#     messages = []\n",
    "#     stats = {\n",
    "#         \"customerid_removed\": 0,\n",
    "#         \"case1_api_filled\": 0,\n",
    "#         \"case1_fallback_filled\": 0,\n",
    "#         \"case2_filled\": 0,\n",
    "#         \"case3_filled\": 0\n",
    "#     }\n",
    "\n",
    "#     # --- Drop rows without ID ---\n",
    "#     if 'customerid' in df.columns:\n",
    "#         before_drop = len(df)\n",
    "#         df = df[df['customerid'].notna()].copy()\n",
    "#         rows_removed = before_drop - len(df)\n",
    "#         stats[\"customerid_removed\"] = rows_removed\n",
    "#         print(f\"[LOG - STAGE 5] Dropped {rows_removed} rows without CustomerID\")\n",
    "#         if rows_removed > 0:\n",
    "#             messages.append(f\" Removed {rows_removed} record(s) with missing CustomerID as we cannot predict valid customer identifiers.\")\n",
    "#     else:\n",
    "#         print(\"[LOG - STAGE 5] 'customerid' column missing, skipping drop\")\n",
    "\n",
    "#     # --- City & State handling ---\n",
    "#     if {'city', 'state'}.issubset(df.columns):\n",
    "#         print(\"\\n[LOG - STAGE 5] Handling missing city/state values...\")\n",
    "#         malaysia_states = [sub.name for sub in pycountry.subdivisions if sub.country_code == 'MY']\n",
    "#         cache = {}  # city -> validated state\n",
    "#         SLEEP_TIME = 1.2\n",
    "        \n",
    "#         # Case 1: missing state but city known ‚Üí fill via geocoding API\n",
    "#         print(\"\\n[LOG - STAGE 5] Case 1: Filling missing state where city is known...\")\n",
    "#         # Get rows needing state fill (city known, state unknown)\n",
    "#         mask_case1 = (df['city'] != 'Unknown') & (df['state'] == 'Unknown')\n",
    "#         cities_to_query = df.loc[mask_case1, 'city'].unique().tolist()\n",
    "\n",
    "#         print(f\"[LOG - STAGE 5] {len(cities_to_query)} unique cities need state lookup\")\n",
    "        \n",
    "#         for city in cities_to_query:\n",
    "#             if city not in cache:\n",
    "#                 # Call API\n",
    "#                 try:\n",
    "#                     resp = requests.get(GEOCODE_URL, params={\"q\": f\"{city}, Malaysia\", \"api_key\": API_KEY}, timeout=10)\n",
    "#                     if resp.status_code == 200:\n",
    "#                         data = resp.json()\n",
    "#                         if isinstance(data, list) and data:\n",
    "#                             state_name = data[0].get(\"address\", {}).get(\"state\")\n",
    "#                             if state_name and state_name in malaysia_states:\n",
    "#                                 cache[city] = state_name\n",
    "#                             else:\n",
    "#                                 cache[city] = None\n",
    "#                         else:\n",
    "#                             cache[city] = None\n",
    "#                     else:\n",
    "#                         cache[city] = None\n",
    "#                     time.sleep(SLEEP_TIME)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"[WARN - STAGE 5] Failed to get state for city '{city}': {e}\")\n",
    "#                     cache[city] = None\n",
    "\n",
    "#             # Fill values\n",
    "#             fill_state = cache.get(city)\n",
    "#             if fill_state:\n",
    "#                 filled_count = ((df['city'] == city) & (df['state'] == 'Unknown')).sum()\n",
    "#                 df.loc[(df['city'] == city) & (df['state'] == 'Unknown'), 'state'] = fill_state\n",
    "#                 stats[\"case1_api_filled\"] += filled_count\n",
    "#                 print(f\"[TRACE - STAGE 5] Filled {filled_count} ‚Üí {city} ‚Üí state='{fill_state}' (API valid)\")\n",
    "#             else:\n",
    "#                 # Fallback: use mode state & mode city for that state\n",
    "#                 # The API fails to find the city || The response doesn‚Äôt contain a valid \"state\" field || Or the returned \"state\" isn‚Äôt in the official Malaysia subdivision list.\n",
    "#                 valid_states = df[df['state'] != 'Unknown']['state']\n",
    "#                 mode_state = valid_states.mode()[0] if not valid_states.empty else 'Unknown'\n",
    "\n",
    "#                 # Compute mode city per state\n",
    "#                 mode_city_per_state = (\n",
    "#                     df[df['city'] != 'Unknown'].groupby('state')['city']\n",
    "#                     .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown')\n",
    "#                     .to_dict()\n",
    "#                 )\n",
    "#                 mode_city = mode_city_per_state.get(mode_state, 'Unknown')\n",
    "\n",
    "#                 mask_fill = (df['city'] == city) & (df['state'] == 'Unknown')\n",
    "#                 filled_count = mask_fill.sum()\n",
    "#                 df.loc[mask_fill, 'state'] = mode_state\n",
    "#                 df.loc[mask_fill, 'city'] = mode_city\n",
    "#                 stats[\"case1_fallback_filled\"] += filled_count\n",
    "#                 print(f\"[TRACE - STAGE 5] Filled {filled_count} row(s) ‚Üí city='{mode_city}', state='{mode_state}' (Fallback)\")\n",
    "\n",
    "#         if stats[\"case1_api_filled\"] > 0 or stats[\"case1_fallback_filled\"] > 0:\n",
    "#             msg = \"Location - Missing State (City Known):\\n\"\n",
    "#             if stats[\"case1_api_filled\"] > 0:\n",
    "#                 msg += f\"  ‚Ä¢ {stats['case1_api_filled']} record(s) automatically matched to correct state using map service\\n\"\n",
    "#             if stats[\"case1_fallback_filled\"] > 0:\n",
    "#                 msg += f\"  ‚Ä¢ {stats['case1_fallback_filled']} record(s) filled based on where most customers are located (city name couldn't be verified - may be misspelled or invalid)\\n\"\n",
    "#             msg += \"Why safe: We kept their original city and only filled in the missing state using reliable location data\"\n",
    "#             messages.append(msg)\n",
    "\n",
    "#         # Case 2: missing city but state known ‚Üí fill with mode city per state\n",
    "#         print(\"\\n[LOG - STAGE 5] Case 2: Filling missing city where state is known...\")\n",
    "#         mask_case2 = (df['city'] == 'Unknown') & (df['state'] != 'Unknown')\n",
    "#         if mask_case2.any():\n",
    "#             mode_city_per_state = (\n",
    "#                 df[df['city'] != 'Unknown'].groupby('state')['city']\n",
    "#                 .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown')\n",
    "#                 .to_dict()\n",
    "#             )\n",
    "#             for state, city_mode in mode_city_per_state.items():\n",
    "#                 mask_fill = mask_case2 & (df['state'] == state)\n",
    "#                 filled_count = mask_fill.sum()\n",
    "#                 if filled_count > 0:\n",
    "#                     df.loc[mask_fill, 'city'] = city_mode\n",
    "#                     stats[\"case2_filled\"] += filled_count\n",
    "#                     print(f\"[TRACE - STAGE 5] Filled {filled_count} row(s) ‚Üí missing city for state='{state}' ‚Üí city='{city_mode}'\")\n",
    "        \n",
    "#         if stats[\"case2_filled\"] > 0:\n",
    "#             msg = \"Location - Missing City (State Known):\\n\"\n",
    "#             msg += f\"  ‚Ä¢ {stats['case2_filled']} record(s) filled with representative city for their state\\n\"\n",
    "#             msg += \"Why safe: Used real cities from your actual customer base, ensuring realistic segmentation\"\n",
    "#             messages.append(msg)\n",
    "\n",
    "#         # Case 3: both missing ‚Üí fill with most frequent pair\n",
    "#         print(\"\\n[LOG - STAGE 5] Case 3: Filling missing city and state...\")\n",
    "#         mask_case3 = (df['city'] == 'Unknown') & (df['state'] == 'Unknown')\n",
    "#         if mask_case3.any():\n",
    "#             valid_pairs = df[(df['city'] != 'Unknown') & (df['state'] != 'Unknown')]\n",
    "#             if not valid_pairs.empty:\n",
    "#                 city_mode, state_mode = valid_pairs.groupby(['city', 'state']).size().idxmax()\n",
    "#                 filled_count = mask_case3.sum()\n",
    "#                 df.loc[mask_case3, ['city', 'state']] = [city_mode, state_mode]\n",
    "#                 stats[\"case3_filled\"] = filled_count\n",
    "#                 print(f\"[TRACE - STAGE 5] Filled {filled_count} row(s) ‚Üí missing city/state ‚Üí City='{city_mode}', State='{state_mode}'\")\n",
    "#             else:\n",
    "#                 print(\"[WARN - STAGE 5] No valid city/state pairs to fill missing both values\")\n",
    "        \n",
    "#         if stats[\"case3_filled\"] > 0:\n",
    "#             msg = \"Location - Both City & State Missing:\\n\"\n",
    "#             msg += f\"  ‚Ä¢ {stats['case3_filled']} record(s) filled with your primary customer location (where most customers are from)\\n\"\n",
    "#             msg += \"Why safe: Used the most common location in your customer base, minimizing impact on segmentation\"\n",
    "#             messages.append(msg)\n",
    "\n",
    "#     # Build final message\n",
    "#     if not messages:\n",
    "#         final_message = \"No missing values found in critical fields (CustomerID, City, State).\"\n",
    "#     else:\n",
    "#         final_message = \"Summary:\\n\\n\" + \"\\n\\n\".join(messages)\n",
    "\n",
    "#     return df, final_message\n",
    "\n",
    "# # ============================================= (CUSTOMER DATASET) STAGE 6: OUTLIER DETECTION =============================================\n",
    "# def customer_detect_outliers(df):\n",
    "#     \"\"\"Adaptive outlier detection (flag instead of replace).\"\"\"\n",
    "#     print(\"[LOG - STAGE 6] Running detect_outliers...\")\n",
    "#     message = None\n",
    "#     if 'age' in df.columns:\n",
    "#         # Work on a temporary numeric copy to avoid mutating the displayed 'age' values (keep 'Unknown' text)\n",
    "#         age_num = pd.to_numeric(df['age'], errors='coerce')\n",
    "#         n = len(df)\n",
    "#         print(f\"[LOG - STAGE 6] Dataset has {n} rows\")\n",
    "\n",
    "#         # Initialize flag column\n",
    "#         df['is_age_outlier'] = False\n",
    "#         message = \"\"\n",
    "\n",
    "#         if n < 500:\n",
    "#             # IQR method\n",
    "#             Q1 = age_num.quantile(0.25)\n",
    "#             Q3 = age_num.quantile(0.75)\n",
    "#             IQR = Q3 - Q1\n",
    "#             lower_bound = Q1 - 1.5 * IQR\n",
    "#             upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "#             # Flag outliers\n",
    "#             outlier_mask = (age_num < lower_bound) | (age_num > upper_bound)\n",
    "#             df.loc[outlier_mask, 'is_age_outlier'] = True\n",
    "#             outlier_count = outlier_mask.sum()\n",
    "\n",
    "#             print(f\"[LOG - STAGE 6] IQR Applied for {n} rows. Range: [{lower_bound:.1f}, {upper_bound:.1f}] \"\n",
    "#                   f\"Outliers flagged: {outlier_count}\")\n",
    "            \n",
    "#             if outlier_count > 0:\n",
    "#                 message = (\n",
    "#                     f\"Unusual Age Values Detected:\\n\\n\"\n",
    "#                     f\"We found {outlier_count} customer(s) with ages that seem unusual (either very young or very old) compared to the rest of your data.\\n\\n\"\n",
    "#                     f\"Normal age range in your data: {lower_bound:.0f} to {upper_bound:.0f} years old\\n\\n\"\n",
    "#                     f\"What we did: These customers are marked with a special flag but kept in your dataset. They might be:\\n\"\n",
    "#                     f\"   - Real customers (e.g., teenagers or senior citizens)\\n\"\n",
    "#                     f\"   - Data entry mistakes\\n\\n\"\n",
    "#                     f\"You can review them later if needed.\"\n",
    "#                 )\n",
    "#             else:\n",
    "#                 message = (\n",
    "#                     f\"Age Data Looks Good:\\n\\n\"\n",
    "#                     f\"All customer ages appear normal and consistent. No unusual values detected.\\n\\n\"\n",
    "#                     f\"Typical age range: {lower_bound:.0f} to {upper_bound:.0f} years old\"\n",
    "#                 )\n",
    "\n",
    "#         else:\n",
    "#             # Percentile method\n",
    "#             lower_bound = age_num.quantile(0.01)\n",
    "#             upper_bound = age_num.quantile(0.99)\n",
    "\n",
    "#             # Flag outliers instead of capping\n",
    "#             outlier_mask = (age_num < lower_bound) | (age_num > upper_bound)\n",
    "#             df.loc[outlier_mask, 'is_age_outlier'] = True\n",
    "#             outlier_count = outlier_mask.sum()\n",
    "\n",
    "#             print(f\"[LOG - STAGE 6] Percentile method applied for {n} rows. Range: [{lower_bound:.1f}, {upper_bound:.1f}] \"\n",
    "#                   f\"Outliers flagged: {outlier_count}\")\n",
    "            \n",
    "#             if outlier_count > 0:\n",
    "#                 message = (\n",
    "#                     f\"Unusual Age Values Detected:\\n\\n\"\n",
    "#                     f\"We found {outlier_count} customer(s) with ages that seem unusual (either very young or very old) compared to the rest of your data.\\n\\n\"\n",
    "#                     f\"Normal age range in your data: {lower_bound:.0f} to {upper_bound:.0f} years old\\n\\n\"\n",
    "#                     f\"What we did: These customers are marked with a special flag but kept in your dataset. They might be:\\n\"\n",
    "#                     f\"   - Real customers (e.g., teenagers or senior citizens)\\n\"\n",
    "#                     f\"   - Data entry mistakes\\n\\n\"\n",
    "#                     f\"You can review them later if needed.\"\n",
    "#                 )\n",
    "#             else:\n",
    "#                 message = (\n",
    "#                     f\"Age Data Looks Good:\\n\\n\"\n",
    "#                     f\"All customer ages appear normal and consistent. No unusual values detected.\\n\\n\"\n",
    "#                     f\"Typical age range: {lower_bound:.0f} to {upper_bound:.0f} years old\"\n",
    "#                 )\n",
    "\n",
    "#     else:\n",
    "#         print(\"[LOG - STAGE 6] 'age' column missing, skipping outlier detection\")\n",
    "#         message = \"Age column not found. Outlier detection skipped.\"\n",
    "\n",
    "#     return df, message\n",
    "\n",
    "# # ============================================= (CUSTOMER DATASET) DATASET CLEANING PIPELINE =============================================\n",
    "# def clean_customer_dataset(df, cleaned_output_path):\n",
    "#     \"\"\"\n",
    "#     Main cleaning pipeline for customer dataset.\n",
    "#     Executes all stages in proper order:\n",
    "#     0. Column Normalization\n",
    "#     1. Schema & Column Validation\n",
    "#     2. Duplicate Entry Removal\n",
    "#     3. Standardization & Normalization\n",
    "#     4. Missing Value Handling\n",
    "#     5. Outlier Detection\n",
    "#     6. Deduplication\n",
    "#     Finally, saves the cleaned dataset to the specified path and returns it.\n",
    "#     \"\"\"\n",
    "#     print(\"üöÄ Starting customer data cleaning pipeline...\\n\", flush=True)\n",
    "#     messages = [] \n",
    "#     report = {\"summary\": {}, \"detailed_messages\": {}}\n",
    "    \n",
    "#     # Capture initial row count before cleaning\n",
    "#     initial_row_count = len(df)\n",
    "#     report[\"summary\"][\"initial_rows\"] = initial_row_count\n",
    "    \n",
    "#     # =======================================================\n",
    "#     # STAGE 0: NORMALIZE COLUMN NAMES (FROM GENERIC FUNCTION)\n",
    "#     # =======================================================\n",
    "#     print(\"=============== [STAGE 0 START] Normalize Column Names ===============\")\n",
    "#     df = normalize_columns_name(df)\n",
    "#     print(\"‚úÖ [STAGE 0 COMPLETE] Column names normalized.\\n\")\n",
    "\n",
    "#     # =============================================\n",
    "#     # STAGE 1: SCHEMA & COLUMN VALIDATION\n",
    "#     # =============================================\n",
    "#     print(\"=============== [STAGE 1 START] Schema & Column Validation ===============\")\n",
    "#     df, optional_msg = customer_check_optional_columns(df)\n",
    "#     messages.append(optional_msg)\n",
    "#     report[\"detailed_messages\"][\"customer_check_optional_columns\"] = optional_msg\n",
    "    \n",
    "#     # (FROM GENERIC FUNCTION)\n",
    "#     customer_mandatory = [\"customerid\", \"city\", \"state\"]\n",
    "#     df, mandatory_msg = check_mandatory_columns(df,dataset_type=\"customer\", mandatory_columns=customer_mandatory)\n",
    "#     messages.append(mandatory_msg)\n",
    "#     report[\"detailed_messages\"][\"check_mandatory_columns\"] = mandatory_msg\n",
    "    \n",
    "#     print(optional_msg)\n",
    "#     print(mandatory_msg)\n",
    "#     print(\"‚úÖ [STAGE 1 COMPLETE] Schema validation done.\\n\")\n",
    "\n",
    "#     # ============================================================\n",
    "#     # STAGE 2: REMOVE DUPLICATE ENTRY ROWS (FROM GENERIC FUNCTION)\n",
    "#     # ============================================================\n",
    "#     print(\"========== [STAGE 2 START] Remove Duplicate Entry Rows ==========\")\n",
    "#     initial_rows = len(df)\n",
    "#     df, message = remove_duplicate_entries(df)\n",
    "#     messages.append(message)\n",
    "#     report[\"detailed_messages\"][\"remove_duplicate_entries\"] = message\n",
    "#     report[\"summary\"][\"duplicates_removed_rows\"] = initial_rows - len(df)\n",
    "#     print(\"‚úÖ [STAGE 2 COMPLETE] Duplicate entries removed.\\n\")\n",
    "\n",
    "#     # =============================================\n",
    "#     # STAGE 3: DEDUPLICATION\n",
    "#     # =============================================\n",
    "#     print(\"========== [STAGE 3 START] Deduplication ==========\")\n",
    "#     df, message = deduplicate_customers(df)\n",
    "#     messages.append(message)\n",
    "#     report[\"detailed_messages\"][\"deduplicate_customers\"] = message\n",
    "#     report[\"summary\"][\"rows_after_deduplication\"] = len(df)\n",
    "#     print(\"‚úÖ [STAGE 3 COMPLETE] Duplicate CustomerIDs deduplicated.\\n\")\n",
    "\n",
    "#     # =============================================\n",
    "#     # STAGE 4: STANDARDIZATION & NORMALIZATION\n",
    "#     # =============================================\n",
    "#     print(\"========== [STAGE 4 START] Standardization & Normalization ==========\")\n",
    "#     df = standardize_customer_id(df)\n",
    "#     df, dob_msg = standardize_dob(df)\n",
    "#     messages.append(dob_msg)\n",
    "#     report[\"detailed_messages\"][\"standardize_dob\"] = dob_msg\n",
    "#     df = derive_age_features(df)\n",
    "#     df = derive_age_group(df)\n",
    "#     df = drop_dob_after_age_derived(df)\n",
    "#     df = standardize_gender(df)\n",
    "#     df = standardize_location(df)\n",
    "#     print(\"‚úÖ [STAGE 4 COMPLETE] Standardization and normalization finished.\\n\")\n",
    "    \n",
    "#     # =============================================\n",
    "#     # STAGE 5: MISSING VALUE HANDLING\n",
    "#     # =============================================\n",
    "#     print(\"========== [STAGE 5 START] Missing Value Handling ==========\")\n",
    "#     df, missing_value_msg = handle_missing_values_customer(df)\n",
    "#     messages.append(missing_value_msg)\n",
    "#     report[\"detailed_messages\"][\"handle_missing_values_customer\"] = missing_value_msg\n",
    "#     print(\"‚úÖ [STAGE 5 COMPLETE] Missing values handled.\\n\")\n",
    "\n",
    "#     # =============================================\n",
    "#     # STAGE 6: OUTLIER DETECTION\n",
    "#     # =============================================\n",
    "#     print(\"========== [STAGE 6 START] Outlier Detection ==========\")\n",
    "#     df, outlier_msg = customer_detect_outliers(df)\n",
    "#     messages.append(outlier_msg)\n",
    "#     report[\"detailed_messages\"][\"customer_detect_outliers\"] = outlier_msg\n",
    "#     print(\"‚úÖ [STAGE 6 COMPLETE] Outliers handled.\\n\")\n",
    "\n",
    "#     # final profiling summary\n",
    "#     report[\"summary\"].update({\n",
    "#         \"total_rows_final\": len(df),\n",
    "#         \"total_columns_final\": len(df.columns),\n",
    "#         \"final_columns\": list(df.columns),  # Add list of remaining column names\n",
    "#     })\n",
    "#     # =============================================\n",
    "#     # SAVE CLEANED DATASET\n",
    "#     # =============================================\n",
    "#     print(\"========== [FINAL STAGE START] Save Cleaned Dataset ==========\")\n",
    "#     df.to_csv(cleaned_output_path, index=False)\n",
    "#     print(f\"‚úÖ [FINAL STAGE COMPLETE] Cleaned dataset saved at: {cleaned_output_path}\\n\")\n",
    "#     print(\"==========================================================\")\n",
    "#     print(\"üéâ Data cleaning pipeline completed successfully!\\n\")\n",
    "    \n",
    "#     return df, report # the messages can modify later if needed (now only handle neccessary part), but no return for now, we try return report first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb17ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep a copy for comparison\n",
    "# original_customer_df = customer_df.copy()\n",
    "\n",
    "# # >>>>>>>>>>>>>>>>>>>> (CUSTOMER DATASET) IMPLEMENT CLEANING PIPELINE >>>>>>>>>>>>>>>>>>>\n",
    "# cleaned_customer_df, report = clean_customer_dataset(customer_df, original_customer_dataset_name)\n",
    "\n",
    "# # AFTER CLEANING : CLEANED VERSION TO BE STORED BACK TO DATABASE\n",
    "# cleaned_customer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816c764",
   "metadata": {},
   "source": [
    "## Perform Data Cleaning Pipeline - OrderDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Load dataset\n",
    "# # Replace 'customer_dataset.csv' with your actual file name or path\n",
    "# order_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Soapan Santun - Use/2021 - 2025 Order.csv\"\n",
    "# # customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Test Data/customer_dataset_balanced_5033_rows.csv\"\n",
    "# # customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Test Data/customer_dataset_light_7719_rows.csv\"\n",
    "# # customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Test Data/customer_dataset_stress_6047_rows.csv\"\n",
    "\n",
    "# original_order_dataset_name = \"2021 - 2025 Order.csv\"\n",
    "\n",
    "# # Read dataset\n",
    "# order_df = pd.read_csv(order_file_path)\n",
    "\n",
    "# # Show first few rows (original raw data)\n",
    "# order_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1eef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Import libraries\n",
    "# # from common_utils import normalize_columns_name, check_mandatory_columns, remove_duplicate_entries, standardize_customer_id;\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from datetime import datetime, date\n",
    "# from fuzzywuzzy import process, fuzz\n",
    "\n",
    "# # ============================================= (ORDER DATASET) STAGE 0: NORMALIZE COLUMN NAMES =============================================\n",
    "# # From Generic function: normalize_columns_name\n",
    "\n",
    "# # ============================================= (ORDER DATASET) STAGE 1: SCHEMA & COLUMN VALIDATION =============================================\n",
    "# # From Generic function\n",
    "\n",
    "# # ============================================= (ORDER DATASET) STAGE 2: REMOVE DUPLICATE ENTRY ROW =================================================\n",
    "# # From Generic function: remove_duplicate_entries\n",
    "\n",
    "# # ============================================= (ORDER DATASET) STAGE 3: STANDARDIZATION & NORMALIZATION =============================================\n",
    "# # From Generic function: standardize_customer_id\n",
    "\n",
    "# def standardized_order_id(df):\n",
    "#     \"\"\"Standardize OrderID format (null is '')\"\"\"\n",
    "#     print(\"[LOG - STAGE 3] Running standardize_order_id...\")\n",
    "#     if 'orderid' in df.columns:\n",
    "#         # Fill '' with empty string before converting to string\n",
    "#         df.loc[:, 'orderid'] = df['orderid'].astype(str).str.strip().str.upper()\n",
    "        \n",
    "#         # Convert empty string back to NaN\n",
    "#         df.loc[df['orderid'] == '', 'orderid'] = np.nan\n",
    "        \n",
    "#         print(\"[LOG - STAGE 3] OrderID column standardized (empty -> NaN)\")\n",
    "#     else:\n",
    "#         print(\"[LOG - STAGE 3] OrderID column not found, skipping\")\n",
    "#     return df\n",
    "\n",
    "# def standardize_purchase_item(df):\n",
    "#     \"\"\"\"Standardize Purchase Item names (NaN preserved)\"\"\"\n",
    "#     print(\"[LOG - STAGE 3] Running standardized_purchase_item...\")\n",
    "#     if \"purchase item\" in df.columns:\n",
    "#         mask = df[\"purchase item\"].notna()\n",
    "#         df.loc[mask, \"purchase item\"] = (\n",
    "#             df.loc[mask, \"purchase item\"]\n",
    "#             .astype(str)\n",
    "#             .str.strip()\n",
    "#             .str.title()\n",
    "#         )\n",
    "#         df.loc[df[\"purchase item\"].str.strip() == \"\", \"purchase item\"] = np.nan\n",
    "#         print(\"[LOG - STAGE 3] Purchase Item standardized, NaN preserved\")\n",
    "#     else:\n",
    "#         print(\"[LOG - STAGE 3] 'purchase item' column not found, skipping\")\n",
    "#     return df\n",
    "\n",
    "# def standardize_purchase_date(df):\n",
    "#     \"\"\"Standardize Purchase Date into separate date and time columns(NaT preserved)\"\"\"\n",
    "#     print(\"[LOG - STAGE 3] Running standardize_purchase_date...\")\n",
    "#     message = None\n",
    "#     if \"purchase date\" in df.columns:\n",
    "#         # Ensure df is a deep copy (prevents SettingWithCopyWarning)\n",
    "#         df = df.copy()\n",
    "        \n",
    "#         # Clean values\n",
    "#         df.loc[df[\"purchase date\"].notna(), \"purchase date\"] = df.loc[df[\"purchase date\"].notna(), \"purchase date\"].astype(str).str.strip()\n",
    "\n",
    "#         # Convert to datetime\n",
    "#         df.loc[:, \"purchase datetime\"] = pd.to_datetime(\n",
    "#             df[\"purchase date\"], errors=\"coerce\", dayfirst=True\n",
    "#         )\n",
    "\n",
    "#         # Detect which rows have time info\n",
    "#         has_time_mask = df[\"purchase date\"].notna() & df[\"purchase date\"].str.contains(\":\", regex=False)\n",
    "        \n",
    "#         # Create standardized columns\n",
    "#         df.loc[:, \"purchase date\"] = df[\"purchase datetime\"].dt.strftime(\"%Y-%m-%d\")\n",
    "#         df.loc[:, \"purchase date\"] = df[\"purchase date\"].replace(\"NaT\", pd.NA)\n",
    "#         df.loc[:, \"purchase time\"] = None\n",
    "#         df.loc[has_time_mask, \"purchase time\"] = (\n",
    "#             df.loc[has_time_mask, \"purchase datetime\"].dt.strftime(\"%H:%M:%S\")\n",
    "#         )\n",
    "        \n",
    "#         # Drop intermediate column\n",
    "#         df.drop(columns=[\"purchase datetime\"], inplace=True, errors=\"ignore\")\n",
    "        \n",
    "#         if has_time_mask.any():\n",
    "#             message = (\n",
    "#                 \"Since your Purchase Date information have include time information, \"\n",
    "#                 \"so a separate 'purchase time' column has been derived to be used for segmentation later.\"\n",
    "#             )\n",
    "#     else:\n",
    "#         print(\"[WARN - STAGE 3] 'purchase date' column not found, skipping.\")\n",
    "#     print(\"[LOG - STAGE 3] Purchase date standardization complete, NaN preserved.\")\n",
    "#     return df, message\n",
    "\n",
    "# def standardized_item_price_and_total_spend(df):\n",
    "#     \"\"\"\n",
    "#     Standardize 'item price' and 'total spend' columns in the dataset.\n",
    "\n",
    "#     Steps performed for each column:\n",
    "#     1. Remove all non-numeric characters except digits, decimal points, and negative signs\n",
    "#     (e.g., currency symbols like RM, $, MYR are removed)\n",
    "#     2. Convert the resulting strings to numeric values; invalid or non-convertible entries are set to NaN\n",
    "#     3. Round numeric values to 2 decimal places\n",
    "\n",
    "#     This ensures the columns are clean, numeric, and ready for analysis.\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(\"[LOG - STAGE 3] Running standardized_item_price_and_total_spend...\")\n",
    "\n",
    "#     for col in [\"item price\", \"total spend\"]:\n",
    "#         if col in df.columns:\n",
    "#             # Step 1: Remove currency symbols and text\n",
    "#             df[col] = (\n",
    "#                 df[col]\n",
    "#                 .astype(str)\n",
    "#                 .str.replace(r\"[^\\d\\.\\-]\", \"\", regex=True)\n",
    "#             )\n",
    "\n",
    "#             # Step 2: Convert to numeric, coercing invalid values to NaN\n",
    "#             df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "#             # Step 3: Round to 2 decimal places\n",
    "#             df[col] = df[col].round(2)\n",
    "\n",
    "#             print(f\"[LOG - STAGE 3] {col} standardized: numeric, 2 decimal places, NaN preserved\")\n",
    "#         else:\n",
    "#             print(f\"[LOG - STAGE 3] '{col}' column not found, skipping\")\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def standardize_purchase_quantity(df):\n",
    "#     \"\"\"Standardize Purchase Quantity to integer (NaN preserved)\"\"\"\n",
    "#     print(\"[LOG - STAGE 3] Running standardize_purchase_quantity...\")\n",
    "\n",
    "#     if \"purchase quantity\" in df.columns:\n",
    "#         # Remove non-numeric characters (like pcs, x, units, etc.)\n",
    "#         df[\"purchase quantity\"] = (\n",
    "#             df[\"purchase quantity\"]\n",
    "#             .astype(str)\n",
    "#             .str.replace(r\"[^\\d\\.\\-]\", \"\", regex=True)  # keep digits only\n",
    "#         )\n",
    "\n",
    "#         # Convert to numeric (NaN for invalid)\n",
    "#         df[\"purchase quantity\"] = pd.to_numeric(df[\"purchase quantity\"], errors=\"coerce\")\n",
    "\n",
    "#         # Round any decimals (e.g. 2.5 ‚Üí 2)\n",
    "#         df[\"purchase quantity\"] = df[\"purchase quantity\"].round(0).astype(\"Int64\")\n",
    "\n",
    "#         print(\"[LOG - STAGE 3] Purchase quantity standardized to integer format, NaN preserved\")\n",
    "#     else:\n",
    "#         print(\"[LOG - STAGE 3] 'purchase quantity' column not found, skipping\")\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def standardize_transaction_method(df):\n",
    "#     \"\"\"\n",
    "#     Standardize 'transaction method' into categories:\n",
    "#     ['Cash', 'Card', 'E-Wallet', 'Online Banking', 'Auto-Debit', 'Cheque']\n",
    "#     (Unknown represents missing values)\n",
    "#     \"\"\"\n",
    "#     import re\n",
    "\n",
    "#     print(\"[LOG - STAGE 3] Running standardize_transaction_method...\")\n",
    "\n",
    "#     if \"transaction method\" not in df.columns:\n",
    "#         print(\"[LOG - STAGE 3] 'transaction method' column not found, skipping.\")\n",
    "#         return df\n",
    "    \n",
    "#     # Step1: Only process non-null values\n",
    "#     mask_notna = df[\"transaction method\"].notna()\n",
    "#     df.loc[mask_notna, \"transaction method\"] = (\n",
    "#         df.loc[mask_notna, \"transaction method\"].astype(str).str.lower().str.strip()\n",
    "#     )\n",
    "    \n",
    "#     # Step 2: Define patterns (non-capturing groups)\n",
    "#     patterns = {\n",
    "#         \"Cash\": r\"\\b(?:cash|tunai|otc|counter)\\b\",\n",
    "#         \"Card\": r\"\\b(?:card|visa|master|credit|debit|amex|credit.?debit)\\b\",\n",
    "#         \"E-Wallet\": r\"\\b(?:tng|touch\\s*n\\s*go|grab\\s*pay|grabpay|boost|shopee\\s*pay|shopeepay|spaylater|duitnow|ewallet|e-?wallet|qr|qr\\s*pay|qrcode)\\b\",\n",
    "#         \"Online Banking\": r\"\\b(?:bank|transfer|fpx|online\\s*payment|maybank2u|cimbclicks|duitnow\\s*qr|public\\s*bank)\\b\",\n",
    "#         \"Auto-Debit\": r\"\\b(?:auto.?debit|standing|recurring|subscription|auto\\s*pay)\\b\",\n",
    "#         \"Cheque\": r\"\\b(?:cheque|cek|check)\\b\",\n",
    "#     }\n",
    "\n",
    "#     # Step 4: Apply vectorized regex matching\n",
    "#     for category, pattern in patterns.items():\n",
    "#         mask = df[\"transaction method\"].str.contains(pattern, flags=re.IGNORECASE, na=False, regex=True)\n",
    "#         df.loc[mask, \"transaction method\"] = category\n",
    "\n",
    "#     mask_unmatched = mask_notna & ~df[\"transaction method\"].isin(patterns.keys())\n",
    "#     df.loc[mask_unmatched, \"transaction method\"] = pd.NA\n",
    "\n",
    "#     print(\"[LOG - STAGE 3] Transaction method standardized successfully, NaN preserved.\")\n",
    "#     return df\n",
    "\n",
    "# # ============================================= (ORDER DATASET) STAGE 4: MISSING VALUE HANDLING =============================================\n",
    "# def handle_missing_values_order(df):\n",
    "#     \"\"\"\n",
    "#     Strategy (for SME context):\n",
    "#     - Drop rows if (orderid, customerid, purchase date) are missing\n",
    "#     - Drop rows if (purchase time) is missing (when time column exists)\n",
    "#     - Drop rows if both (item price and total spend) are missing\n",
    "#     - Drop rows if (item price) is missing (critical for calculations)\n",
    "#     - Fill or calculate non-critical missing fields logically:\n",
    "#         - purchase quantity: calculate from (total spend / item price) if possible, otherwise fill with 1\n",
    "#         - total spend: calculate from (item price √ó quantity) if possible, otherwise drop row\n",
    "#         - transaction method: fill with \"Unknown\"\n",
    "    \n",
    "#     Note: purchase item is NOT filled because product name is important for business analysis\n",
    "#     \"\"\"\n",
    "\n",
    "#     initial_count = len(df)\n",
    "#     messages = []\n",
    "#     stats = {\n",
    "#         \"critical_ids_removed\": 0,\n",
    "#         \"purchase_time_removed\": 0,\n",
    "#         \"no_financial_removed\": 0,\n",
    "#         \"item_price_removed\": 0,\n",
    "#         \"total_spend_removed\": 0,\n",
    "#         \"qty_calculated\": 0,\n",
    "#         \"total_spend_calculated\": 0,\n",
    "#         \"transaction_method_filled\": 0\n",
    "#     }\n",
    "\n",
    "#     # Drop rows missing critical identifiers\n",
    "#     critical_cols = [\"orderid\", \"customerid\", \"purchase date\"]\n",
    "#     existing_critical = [c for c in critical_cols if c in df.columns]\n",
    "#     before_critical = len(df)\n",
    "#     df = df.dropna(subset=existing_critical)\n",
    "#     stats[\"critical_ids_removed\"] = before_critical - len(df)\n",
    "#     print(f\"[LOG - STAGE 4] Dropped rows with missing critical identifiers: {stats['critical_ids_removed']}\")\n",
    "    \n",
    "#     if \"purchase time\" in df.columns:\n",
    "#         # Drop rows where purchase time is null\n",
    "#         before_drop = len(df)\n",
    "#         df = df.dropna(subset=[\"purchase time\"])\n",
    "#         stats[\"purchase_time_removed\"] = before_drop - len(df)\n",
    "#         print(f\"[LOG - STAGE 4] Dropped {stats['purchase_time_removed']} rows with missing purchase time\")\n",
    "\n",
    "#     # Drop rows missing both financial info\n",
    "#     before_financial = len(df)\n",
    "#     df = df.dropna(subset=[\"item price\", \"total spend\"], how=\"all\")\n",
    "#     stats[\"no_financial_removed\"] = before_financial - len(df)\n",
    "#     print(f\"[LOG - STAGE 4] Dropped {stats['no_financial_removed']} rows with no financial info\")\n",
    "\n",
    "#     for col in [\"item price\", \"total spend\"]:\n",
    "#         if col in df.columns:\n",
    "#             df[col] = df[col].astype(str).str.replace(r\"[^\\d\\.\\-]\", \"\", regex=True)\n",
    "#             df[col] = pd.to_numeric(df[col], errors=\"coerce\").round(2)\n",
    "            \n",
    "#             # Remove zero or negative values that are logically impossible\n",
    "#             df.loc[df[col] <= 0, col] = np.nan\n",
    "#             zero_count = df[col].isna().sum()\n",
    "#             if zero_count > 0:\n",
    "#                 print(f\"[WARN - STAGE 4] {zero_count} rows in '{col}' are zero or invalid and set to NaN\")\n",
    "\n",
    "#     # Handle purchase quantity: calculate if possible\n",
    "#     if \"purchase quantity\" in df.columns:\n",
    "#         mask_missing_qty = df['purchase quantity'].isna() & df['total spend'].notna() & df['item price'].notna()\n",
    "#         calculated_count = mask_missing_qty.sum()\n",
    "#         df.loc[mask_missing_qty, 'purchase quantity'] = (df.loc[mask_missing_qty, 'total spend'] / df.loc[mask_missing_qty, 'item price']).round(0).astype('Int64')\n",
    "#         stats[\"qty_calculated\"] = calculated_count\n",
    "#         # Remaining missing ‚Üí 1 as fallback\n",
    "#         df['purchase quantity'] = df['purchase quantity'].fillna(1)\n",
    "\n",
    "#     # Handle item price - but very rare happened so I decided to simply drop that row\n",
    "#     if \"item price\" in df.columns:\n",
    "#         before_drop = len(df)\n",
    "#         df = df.dropna(subset=[\"item price\"])\n",
    "#         stats[\"item_price_removed\"] = before_drop - len(df)\n",
    "#         print(f\"[LOG - STAGE 4] Dropped {stats['item_price_removed']} rows with missing 'item price' (critical for calculations)\")\n",
    "\n",
    "#     # Handle total spend\n",
    "#     if {\"item price\", \"purchase quantity\", \"total spend\"}.issubset(df.columns):\n",
    "#         mask_missing_total = df[\"total spend\"].isna() & df[\"item price\"].notna() & df[\"purchase quantity\"].notna()\n",
    "#         calculated_total_count = mask_missing_total.sum()\n",
    "#         df.loc[mask_missing_total, \"total spend\"] = (\n",
    "#             df.loc[mask_missing_total, \"item price\"] * df.loc[mask_missing_total, \"purchase quantity\"]\n",
    "#         ).round(2)\n",
    "#         stats[\"total_spend_calculated\"] = calculated_total_count\n",
    "#         # Any remaining missing ‚Üí drop (very rare)\n",
    "#         before_drop = len(df)\n",
    "#         df = df.dropna(subset=[\"total spend\"])\n",
    "#         stats[\"total_spend_removed\"] = before_drop - len(df)\n",
    "#         print(f\"[LOG - STAGE 4] Dropped {stats['total_spend_removed']} rows with missing 'total spend' after calculation\")\n",
    "\n",
    "#     # Transaction method ‚Üí Unknown\n",
    "#     if \"transaction method\" in df.columns:\n",
    "#         before_fill = df[\"transaction method\"].isna().sum()\n",
    "#         df[\"transaction method\"] = df[\"transaction method\"].replace([\"\", \"NaN\", None], np.nan)\n",
    "#         df[\"transaction method\"] = df[\"transaction method\"].fillna(\"Unknown\")\n",
    "#         stats[\"transaction_method_filled\"] = before_fill\n",
    "\n",
    "#     # Build message\n",
    "#     dropped_total = initial_count - len(df)\n",
    "    \n",
    "#     if dropped_total > 0:\n",
    "#         messages.append(f\"{dropped_total} order(s) removed due to missing critical information:\")\n",
    "#         if stats[\"critical_ids_removed\"] > 0:\n",
    "#             messages.append(f\"  - {stats['critical_ids_removed']} order(s) without OrderID, CustomerID, or Purchase Date\")\n",
    "#         if stats[\"purchase_time_removed\"] > 0:\n",
    "#             messages.append(f\"  - {stats['purchase_time_removed']} order(s) without Purchase Time\")\n",
    "#         if stats[\"no_financial_removed\"] > 0:\n",
    "#             messages.append(f\"  - {stats['no_financial_removed']} order(s) without any price or total spend information\")\n",
    "#         if stats[\"item_price_removed\"] > 0:\n",
    "#             messages.append(f\"  - {stats['item_price_removed']} order(s) without Item Price (needed for calculations)\")\n",
    "#         if stats[\"total_spend_removed\"] > 0:\n",
    "#             messages.append(f\"  - {stats['total_spend_removed']} order(s) without Total Spend even after calculation\")\n",
    "    \n",
    "#     calculation_msgs = []\n",
    "#     if stats[\"qty_calculated\"] > 0:\n",
    "#         calculation_msgs.append(f\"  - Calculated Purchase Quantity for {stats['qty_calculated']} order(s) using Total Spend / Item Price\")\n",
    "#     if stats[\"total_spend_calculated\"] > 0:\n",
    "#         calculation_msgs.append(f\"  - Calculated Total Spend for {stats['total_spend_calculated']} order(s) using Item Price x Quantity\")\n",
    "#     if stats[\"transaction_method_filled\"] > 0:\n",
    "#         calculation_msgs.append(f\"  - Filled {stats['transaction_method_filled']} missing Transaction Method(s) with 'Unknown'\")\n",
    "    \n",
    "#     if calculation_msgs:\n",
    "#         messages.append(\"\\nData Filled/Calculated:\")\n",
    "#         messages.extend(calculation_msgs)\n",
    "    \n",
    "#     if not messages:\n",
    "#         final_message = \"All order records have complete information. No missing values found.\"\n",
    "#     else:\n",
    "#         final_message = \"Missing Value Handling Summary:\\n\\n\" + \"\\n\".join(messages)\n",
    "    \n",
    "#     # Final summary\n",
    "#     print(f\"[LOG - STAGE 4] Dropped total {dropped_total} rows ({dropped_total/initial_count:.2%}) due to missing critical data\")\n",
    "#     print(f\"[LOG - STAGE 4] Dataset now has {len(df)} rows after missing value handling\")\n",
    "\n",
    "#     return df, final_message\n",
    "\n",
    "# # ============================================= (ORDER DATASET) STAGE 5: OUTLIER DETECTION =============================================\n",
    "# def order_detect_outliers(df):\n",
    "#     \"\"\"\n",
    "#     Stage 5: Detect and flag outliers for order dataset (NO capping/replacement).\n",
    "#     - Apply IQR method if dataset < 500 rows.\n",
    "#     - Apply percentile method (1st‚Äì99th) if dataset >= 500 rows.\n",
    "#     - Columns: purchase quantity, total spend.\n",
    "#     - Outliers are FLAGGED only, original values preserved.\n",
    "#     \"\"\"\n",
    "#     print(f\"[LOG - STAGE 5] Dataset has {len(df)} rows\")\n",
    "\n",
    "#     numeric_cols = [\"purchase quantity\", \"total spend\"]\n",
    "#     df = df.copy()\n",
    "    \n",
    "#     messages = []\n",
    "#     outlier_info = []\n",
    "#     dataset_size = len(df)\n",
    "    \n",
    "#     # Initialize flag columns\n",
    "#     df['is_quantity_outlier'] = False\n",
    "#     df['is_spend_outlier'] = False\n",
    "    \n",
    "#     for col in numeric_cols:\n",
    "#         if col not in df.columns:\n",
    "#             continue\n",
    "\n",
    "#         if df[col].dropna().empty:\n",
    "#             print(f\"[WARN - STAGE 5] {col} is empty or missing, skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         if len(df) < 500:\n",
    "#             # IQR Method\n",
    "#             Q1 = df[col].quantile(0.25)\n",
    "#             Q3 = df[col].quantile(0.75)\n",
    "#             IQR = Q3 - Q1\n",
    "#             lower = Q1 - 1.5 * IQR\n",
    "#             upper = Q3 + 1.5 * IQR\n",
    "#             method = \"IQR method\"\n",
    "#         else:\n",
    "#             # Percentile Method\n",
    "#             lower = df[col].quantile(0.01)\n",
    "#             upper = df[col].quantile(0.99)\n",
    "#             method = \"Percentile method\"\n",
    "\n",
    "#         # Flag outliers (do NOT modify original values)\n",
    "#         outlier_mask = (df[col] < lower) | (df[col] > upper)\n",
    "#         outliers_count = outlier_mask.sum()\n",
    "        \n",
    "#         # Set appropriate flag column\n",
    "#         if col == \"purchase quantity\":\n",
    "#             df.loc[outlier_mask, 'is_quantity_outlier'] = True\n",
    "#         elif col == \"total spend\":\n",
    "#             df.loc[outlier_mask, 'is_spend_outlier'] = True\n",
    "        \n",
    "#         print(f\"[LOG - STAGE 5] {method} applied on '{col}'. Range: [{lower:.2f}, {upper:.2f}]. Outliers flagged: {outliers_count}\")\n",
    "        \n",
    "#         if outliers_count > 0:\n",
    "#             col_display = col.replace('_', ' ').title()\n",
    "#             outlier_info.append(f\"  - {col_display}: {outliers_count} unusual value(s) detected (may indicate high-value customers)\")\n",
    "\n",
    "#     # Build message\n",
    "#     if outlier_info:\n",
    "#         messages.append(\"Unusual Values Detected:\")\n",
    "#         messages.append(\"\\nWe found some extremely high or low values in your order data.\")\n",
    "#         messages.append(\"\\nWhat we did: Flagged these values for your attention. Original values are preserved.\")\n",
    "#         messages.append(\"\\nDetails:\")\n",
    "#         messages.extend(outlier_info)\n",
    "#         messages.append(\"\\nNote: These outliers might represent VIP customers or bulk buyers. They will be considered during segmentation to help identify high-value customer groups.\")\n",
    "#     else:\n",
    "#         messages.append(\"Order Values Look Good:\")\n",
    "#         messages.append(\"\\nAll order values (quantities and amounts) appear normal and consistent. No unusual values detected.\")\n",
    "\n",
    "#     final_message = \"\\n\".join(messages)\n",
    "    \n",
    "#     print(\"‚úÖ [STAGE 5 COMPLETE] Outliers flagged successfully (values preserved).\")\n",
    "#     return df, final_message\n",
    "\n",
    "# # ============================================= (ORDER DATASET) DATASET CLEANING PIPELINE =============================================\n",
    "# def clean_order_dataset(df, cleaned_output_path):\n",
    "#     print(\"üöÄ Starting order data cleaning pipeline...\\n\")\n",
    "#     messages = [] \n",
    "#     report = {\"summary\": {}, \"detailed_messages\": {}}\n",
    "    \n",
    "#     # Capture initial row count before cleaning\n",
    "#     initial_row_count = len(df)\n",
    "#     report[\"summary\"][\"initial_rows\"] = initial_row_count\n",
    "    \n",
    "#     # =======================================================\n",
    "#     # STAGE 0: NORMALIZE COLUMN NAMES (FROM GENERIC FUNCTION)\n",
    "#     # =======================================================\n",
    "#     print(\"========== [STAGE 0 START] Normalize Column Names ==========\")\n",
    "#     df = normalize_columns_name(df)\n",
    "#     print(\"‚úÖ [STAGE 0 COMPLETE] Column names normalized.\\n\")\n",
    "    \n",
    "#     # =============================================\n",
    "#     # STAGE 1: SCHEMA & COLUMN VALIDATION\n",
    "#     # =============================================\n",
    "#     print(\"========== [STAGE 1 START] Schema & Column Validation ==========\")\n",
    "#     order_mandatory = [\"orderid\", \"customerid\", \"purchase item\", \"purchase date\", \"item price\", \"purchase quantity\", \"total spend\", \"transaction method\"]\n",
    "#     df, mandatory_msg = check_mandatory_columns(df, dataset_type=\"order\", mandatory_columns=order_mandatory)\n",
    "#     messages.append(mandatory_msg)\n",
    "#     report[\"detailed_messages\"][\"check_mandatory_columns\"] = mandatory_msg\n",
    "#     print(mandatory_msg)\n",
    "#     print(\"‚úÖ [STAGE 1 COMPLETE] Schema validation done.\\n\")\n",
    "    \n",
    "#     # ============================================================\n",
    "#     # STAGE 2: REMOVE DUPLICATE ENTRY ROWS (FROM GENERIC FUNCTION)\n",
    "#     # ============================================================\n",
    "#     print(\"========== [STAGE 2 START] Remove Duplicate Entry Rows ==========\")\n",
    "#     initial_rows = len(df)\n",
    "#     df, message = remove_duplicate_entries(df)\n",
    "#     messages.append(message)\n",
    "#     report[\"detailed_messages\"][\"remove_duplicate_entries\"] = message\n",
    "#     report[\"summary\"][\"duplicates_removed_rows\"] = initial_rows - len(df)\n",
    "#     print(\"‚úÖ [STAGE 2 COMPLETE] Duplicate entries removed.\\n\")\n",
    "    \n",
    "#     # =============================================\n",
    "#     # STAGE 3: STANDARDIZATION & NORMALIZATION\n",
    "#     # =============================================\n",
    "#     print(\"========== [STAGE 3 START] Standardization & Normalization ==========\")\n",
    "#     df = standardized_order_id(df)\n",
    "#     df = standardize_customer_id(df)\n",
    "#     df = standardize_purchase_item(df)\n",
    "#     df, standardize_purchaseDateMessage = standardize_purchase_date(df)\n",
    "#     messages.append(standardize_purchaseDateMessage)\n",
    "#     report[\"detailed_messages\"][\"standardize_purchase_date\"] = standardize_purchaseDateMessage\n",
    "#     df = standardized_item_price_and_total_spend(df)\n",
    "#     df = standardize_purchase_quantity(df)\n",
    "#     df = standardize_transaction_method(df)\n",
    "#     print(\"‚úÖ [STAGE 3 COMPLETE] Standardization and normalization finished.\\n\")\n",
    "\n",
    "#     # ===============================================\n",
    "#     # STAGE 4: MISSING VALUE HANDLING\n",
    "#     # ===============================================\n",
    "#     print(\"========== [STAGE 4 START] Missing Value Handling ==========\")\n",
    "#     df, missing_value_msg = handle_missing_values_order(df)\n",
    "#     messages.append(missing_value_msg)\n",
    "#     report[\"detailed_messages\"][\"handle_missing_values_order\"] = missing_value_msg\n",
    "#     print(\"‚úÖ [STAGE 4 COMPLETE] Missing values handled.\\n\")\n",
    "    \n",
    "#     # =============================================\n",
    "#     # STAGE 5: OUTLIER DETECTION\n",
    "#     # =============================================\n",
    "#     print(\"========== [STAGE 5 START] Outlier Detection ==========\")\n",
    "#     df, outlier_msg = order_detect_outliers(df)\n",
    "#     messages.append(outlier_msg)\n",
    "#     report[\"detailed_messages\"][\"order_detect_outliers\"] = outlier_msg\n",
    "#     print(\"‚úÖ [STAGE 5 COMPLETE] Outliers handled.\\n\")\n",
    "    \n",
    "#     # final profiling summary\n",
    "#     report[\"summary\"].update({\n",
    "#         \"total_rows_final\": len(df),\n",
    "#         \"total_columns_final\": len(df.columns),\n",
    "#         \"final_columns\": list(df.columns),  # Add list of remaining column names\n",
    "#     })\n",
    "#     # =============================================\n",
    "#     # SAVE CLEANED DATASET\n",
    "#     # =============================================\n",
    "#     print(\"========== [FINAL STAGE START] Save Cleaned Dataset ==========\")\n",
    "#     # base_name, ext = os.path.splitext(original_order_dataset_name)\n",
    "#     df.to_csv(cleaned_output_path, index=False)\n",
    "#     print(f\"‚úÖ [FINAL STAGE COMPLETE] Cleaned dataset saved at: {cleaned_output_path}\\n\")\n",
    "\n",
    "#     print(\"==========================================================\")\n",
    "#     print(\"üéâ Data cleaning pipeline completed successfully!\\n\")\n",
    "#     return df, report\n",
    "#     # later add on return clean file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a6b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# # Define absolute paths\n",
    "# script_path = \"C:/Y3S2/FYP/server/python/cleaningPipeline/cleaning_main.py\"\n",
    "# temp_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Soapan Santun - Use/2021 - 2025 Customer - Copy.csv\"\n",
    "# original_file_name = \"2021 - 2025 Customer - Copy.csv\"\n",
    "# dataset_type = \"customer\"\n",
    "\n",
    "# # Construct the command with absolute paths\n",
    "# command = [\n",
    "#     \"python\", \n",
    "#     script_path,\n",
    "#     \"--type\", dataset_type,\n",
    "#     \"--temp_file_path_with_filename\", temp_file_path,\n",
    "#     \"--original_file_name\", original_file_name\n",
    "# ]\n",
    "\n",
    "# # Run the cleaning pipeline\n",
    "# result = subprocess.run(\n",
    "#     command, \n",
    "#     capture_output=True, \n",
    "#     text=True,\n",
    "#     cwd=os.path.dirname(script_path)  # Set working directory to script's folder\n",
    "# )\n",
    "\n",
    "# # Check results\n",
    "# if result.returncode == 0:\n",
    "#     print(\"‚úÖ Cleaning succeeded!\")\n",
    "#     print(result.stdout)\n",
    "# else:\n",
    "#     print(\"‚ùå Cleaning failed!\")\n",
    "#     print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1e7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP Kernel",
   "language": "python",
   "name": "fyp_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
