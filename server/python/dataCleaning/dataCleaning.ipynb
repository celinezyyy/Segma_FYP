{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d01967",
   "metadata": {},
   "source": [
    "## Dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30abd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install fuzzywuzzy\n",
    "# !python -m pip install --upgrade pip\n",
    "# !pip install pycountry\n",
    "# %pip install python-Levenshtein\n",
    "# %pip install pandas numpy matplotlib seaborn scikit-learn jupyter\n",
    "\n",
    "# errors='coerce' in pd.to_numeric() means:\n",
    "# ‚ÄúTry to convert everything in this column into a number.\n",
    "# If it fails (e.g. the value is text or invalid), don‚Äôt crash ‚Äî just replace it with NaN.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ec00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate kernal:\n",
    "# .\\venv_fyp\\Scripts\\activate\n",
    "\n",
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pycountry\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe554a9",
   "metadata": {},
   "source": [
    "### Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd905e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d9910",
   "metadata": {},
   "source": [
    "### Generic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a999bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================ GENERIC FUNCTIONS ============================================ #\n",
    "\n",
    "def normalize_columns_name(df):\n",
    "    \"\"\"Normalize column names: lowercase, strip spaces\"\"\"\n",
    "    print(\"[LOG] Running normalize_columns_name...\")\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    print(f\"[LOG] Columns after normalization: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def check_mandatory_columns(df, dataset_type, mandatory_columns, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Generic function to check mandatory columns for both Customer and Order datasets.\n",
    "    - dataset_type: 'customer' or 'order'\n",
    "    - mandatory_columns: list of required columns for that dataset\n",
    "    - threshold: minimum acceptable fill ratio (default 0.8)\n",
    "    \"\"\"\n",
    "    print(f\"[LOG] Running check_mandatory_columns for {dataset_type} dataset...\")\n",
    "\n",
    "    missing_report = []\n",
    "    warning_columns = []\n",
    "\n",
    "    # Step 1: Check each mandatory column\n",
    "    for col in mandatory_columns:\n",
    "        if col in df.columns:\n",
    "            fill_ratio = df[col].notna().mean()\n",
    "            print(f\"[LOG] Mandatory column '{col}' fill ratio: {fill_ratio:.2f}\")\n",
    "            missing_percent = (1 - fill_ratio) * 100\n",
    "            missing_report.append(f\"{col}: {missing_percent:.1f}% missing\")\n",
    "\n",
    "            if fill_ratio < threshold:\n",
    "                warning_columns.append(col)\n",
    "        else:\n",
    "            print(f\"[LOG] Mandatory column '{col}' not found\")\n",
    "            missing_report.append(f\"{col}: column not found (100% missing)\")\n",
    "            warning_columns.append(col)\n",
    "\n",
    "    # Step 2: Generate message\n",
    "    if warning_columns:\n",
    "        warning_str = \", \".join(warning_columns)\n",
    "        message = (\n",
    "            f\"Some key fields in the {dataset_type} dataset have a high number of missing values: {warning_str}. \"\n",
    "            \"The system will continue cleaning and handle missing values automatically, \"\n",
    "            \"but we STRONGLY encourage you to reupload your source data, ensure it was as complete as possible for accurate segmentation result later.\\n\\n\"\n",
    "            \"Missing Data Summary:\\n\" + \"\\n\".join(missing_report)\n",
    "        )\n",
    "    else:\n",
    "        message = (\n",
    "            f\"All mandatory columns in the {dataset_type} dataset have sufficient data and are ready for cleaning.\\n\\n\"\n",
    "            \"Missing Data Summary:\\n\" + \"\\n\".join(missing_report)\n",
    "        )\n",
    "\n",
    "    return df, message\n",
    "\n",
    "def remove_duplicate_entries(df):\n",
    "    \"\"\"Remove duplicate rows, keeping the first occurrence\"\"\"\n",
    "    print(\"[LOG] Running remove_duplicate_entries...\")\n",
    "    initial_len = len(df)\n",
    "    df = df.drop_duplicates(keep='first', ignore_index=True)\n",
    "    print(f\"[LOG] Removed {initial_len - len(df)} duplicate rows.\")\n",
    "    return df\n",
    "\n",
    "def standardize_customer_id(df):\n",
    "    \"\"\"Standardize CustomerID format (null value is '')\"\"\"\n",
    "    print(\"[LOG] Running standardize_customer_id...\")\n",
    "    if 'customerid' in df.columns:\n",
    "        # Fill NaN with empty string before converting to string\n",
    "        df.loc[:, 'customerid'] = df['customerid'].fillna('').astype(str).str.strip().str.upper()\n",
    "        print(\"[LOG] CustomerID column standardized\")\n",
    "    else:\n",
    "        print(\"[LOG] CustomerID column not found, skipping\")\n",
    "    return df\n",
    "    # Might have special case of dirty data exist such as \"****\", \"1234....\", \"annbwbciwbciowb\"\n",
    "    # not sure how to handle it (Currently will say bcs we focus on small business enterprise that have use digital system, so normally customerID will not have inconsistent format issue, even the inconsistant format exist, at the end this row will not be use as when we merge we cant found that customerID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706371d9",
   "metadata": {},
   "source": [
    "## Perform Data Cleaning Pipeline - CustomerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4769ad2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Date of Birth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUST0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sungai Besi</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUST0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUST0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mutiara Damansara</td>\n",
       "      <td>Selangor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUST0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUST0003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shah Alam</td>\n",
       "      <td>Selangor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CustomerID  Date of Birth  Gender               City         State\n",
       "0   CUST0001            NaN     NaN        Sungai Besi  Kuala Lumpur\n",
       "1   CUST0001            NaN     NaN                NaN           NaN\n",
       "2   CUST0002            NaN     NaN  Mutiara Damansara      Selangor\n",
       "3   CUST0002            NaN     NaN                NaN           NaN\n",
       "4   CUST0003            NaN     NaN          Shah Alam      Selangor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load dataset\n",
    "# Replace 'customer_dataset.csv' with your actual file name or path\n",
    "customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Soapan Santun - Use/2021 - 2025 Customer - Copy.csv\"\n",
    "# customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Test Data/customer_dataset_balanced_5033_rows.csv\"\n",
    "# customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Test Data/customer_dataset_light_7719_rows.csv\"\n",
    "# customer_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Test Data/customer_dataset_stress_6047_rows.csv\"\n",
    "\n",
    "original_customer_dataset_name = \"2021 - 2025 Customer - Copy.csv\"\n",
    "\n",
    "# Read dataset\n",
    "customer_df = pd.read_csv(customer_file_path)\n",
    "\n",
    "# Show first few rows (original raw data)\n",
    "customer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "508463d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================= (CUSTOMER DATASET) STAGE 0: NORMALIZE COLUMN NAMES =============================================\n",
    "# From Generic function: normalize_columns_name\n",
    "\n",
    "# ============================================= (CUSTOMER DATASET) STAGE 1: SCHEMA & COLUMN VALIDATION =============================================\n",
    "# Optional columns & Mandatory columns(FROM GENERIC FUNCTION - check_mandatory_columns)\n",
    "def customer_check_optional_columns(df, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Check optional columns for fill percentage and drop columns that are mostly empty.\n",
    "    Returns the modified DataFrame and a friendly message.\n",
    "    \"\"\"\n",
    "    print(\"[LOG] Running customer_check_optional_columns...\")\n",
    "    optional_columns = [\"date of birth\", \"gender\"]\n",
    "    dropped_columns = []\n",
    "    missing_report = []\n",
    "\n",
    "    for col in optional_columns:\n",
    "        if col in df.columns:\n",
    "            fill_ratio = df[col].notna().mean()\n",
    "            missing_percent = (1 - fill_ratio) * 100\n",
    "            missing_report.append(f\"{col}: {missing_percent:.1f}% missing\")\n",
    "            print(f\"[LOG] Optional column '{col}' fill ratio: {fill_ratio:.2f}\")\n",
    "            if fill_ratio < threshold:\n",
    "                dropped_columns.append(col)\n",
    "                df.drop(columns=[col], inplace=True)  # Drop the column immediately\n",
    "                # df[col].count(): This counts the number of non-missing (non-null/non-NaN) values in the current column (col).\n",
    "                # len(df): This gives the total number of rows in the DataFrame.\n",
    "                # fill_ratio: The division calculates the proportion of filled (non-missing) values in that column. A ratio of 1.0 means the column is entirely filled; a ratio of 0.1 means 90% of the values are missing.\n",
    "                print(f\"[LOG] Dropped optional column '{col}' due to too many missing values\")\n",
    "        else:\n",
    "            print(f\"[LOG] Optional column '{col}' not found\")\n",
    "            missing_report.append(f\"{col}: column not found (100% missing)\")\n",
    "            dropped_columns.append(col)\n",
    "\n",
    "    # Generate user-friendly message\n",
    "    if dropped_columns:\n",
    "        dropped_str = \", \".join(dropped_columns)\n",
    "        message = (\n",
    "            f\"We noticed that very few entries were provided for {dropped_str}. \"\n",
    "            \"These columns have been removed. \"\n",
    "            \"Segmentation will still be performed using geographic (City, State) \"\n",
    "            \"and behavioral data (e.g., orders, purchase items, total spend).\\n\\n\"\n",
    "            \"Missing Data Summary:\\n\" + \"\\n\".join(missing_report)\n",
    "        )\n",
    "    else:\n",
    "        message = (\n",
    "            \"All optional columns have enough data and are kept for analysis.\\n\\n\"\n",
    "            \"Missing Data Summary:\\n\" + \"\\n\".join(missing_report)\n",
    "        )\n",
    "    \n",
    "    return df, message\n",
    "\n",
    "# ============================================= (CUSTOMER DATASET) STAGE 2: REMOVE DUPLICATE ENTRY ROW =================================================\n",
    "# From Generic function: remove_duplicate_entries\n",
    "\n",
    "# ============================================= (CUSTOMER DATASET) STAGE 3: DEDUPLICATE =================================================\n",
    "def deduplicate_customers(df):\n",
    "    print(\"[LOG] Running deduplicate_customers...\")\n",
    "    if 'customerid' not in df.columns:\n",
    "        print(\"[LOG] 'customerid' column missing, skipping deduplication\")\n",
    "        return df\n",
    "\n",
    "    def resolve_conflict(series):\n",
    "        vals = series.dropna().unique()\n",
    "        if len(vals) == 0:\n",
    "            return pd.NA\n",
    "        elif len(vals) == 1:\n",
    "            return vals[0]\n",
    "        else:\n",
    "            return series.mode().iloc[0]\n",
    "\n",
    "    # ‚ö° Vectorized groupby instead of per-group loop\n",
    "    df = df.groupby('customerid', as_index=False).agg(resolve_conflict)\n",
    "    print(\"[LOG] Deduplication complete (vectorized)\")\n",
    "    return df\n",
    "\n",
    "# ============================================= (CUSTOMER DATASET) STAGE 4: STANDARDIZATION & NORMALIZATION =============================================\n",
    "# From Generic function: standardize_customer_id\n",
    "\n",
    "def standardize_dob(df):\n",
    "    \"\"\"Standardize Date of Birth column and convert to YYYY-MM-DD\"\"\"\n",
    "    print(\"[LOG] Running standardize_dob...\")\n",
    "    # Rename only 'date of birth' to 'dob'\n",
    "    df = df.rename(columns={'date of birth': 'dob'})  \n",
    "    if 'dob' in df.columns:\n",
    "        print(\"[LOG] DOB column found, parsing dates...\")\n",
    "        def parse_date(x):\n",
    "            if pd.isnull(x):\n",
    "                return pd.NaT\n",
    "            for format in (\"%d/%m/%Y\", \"%m-%d-%y\", \"%Y-%m-%d\", \"%d-%b-%Y\", \"%d-%m-%Y\"):    \n",
    "                try:\n",
    "                    return datetime.strptime(str(x), format).date() # Final format: YYYY-MM-DD | 2025-10-15\n",
    "                except Exception:\n",
    "                    continue\n",
    "            return pd.NaT  # If no valid format found\n",
    "        df['dob'] = df['dob'].apply(parse_date)\n",
    "        df['dob'] = pd.to_datetime(df['dob'])\n",
    "        print(\"[LOG] DOB parsing complete. Invalid dates marked as NaT\")\n",
    "    else:\n",
    "        print(\"[LOG] DOB column not found, skipping\")\n",
    "    return df\n",
    "\n",
    "    # %d/%m/%Y ‚Üí 12/05/2000\n",
    "    # %m-%d-%y ‚Üí 05-12-00\n",
    "    # %Y-%m-%d ‚Üí 2000-05-12\n",
    "    # %d-%b-%Y ‚Üí 12-May-2000\n",
    "    # %d-%m-%Y ‚Üí 12-5-2000\n",
    "\n",
    "# ===============================================================================\n",
    "\n",
    "def derive_age_features(df):\n",
    "    \"\"\"Derive Age from DOB\"\"\"\n",
    "    print(\"[LOG] Running derive_age_features...\")\n",
    "    if 'dob' in df.columns:\n",
    "        today = date.today()\n",
    "        df['age'] = df['dob'].apply(\n",
    "            lambda x: today.year - x.year - ((today.month, today.day) < (x.month, x.day))\n",
    "            if pd.notnull(x) else None\n",
    "        )\n",
    "        df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "        print(\"[LOG] Age derived from DOB\")\n",
    "    else:\n",
    "        print(\"[LOG] DOB column not found, skipping\")\n",
    "    return df\n",
    "    # Example: ((today.month, today.day) < (x.month, x.day))\n",
    "    # (10,15) < (12,1) ‚Üí True (birthday in Dec is after Oct 15)\n",
    "    # (10,15) < (10,16) ‚Üí True (birthday tomorrow)\n",
    "    # (10,15) < (5,20) ‚Üí False (birthday already passed)\n",
    "\n",
    "    # This function calculates each person‚Äôs age from their date of birth (dob) by subtracting their birth year from the current year and adjusting if their birthday hasn‚Äôt occurred yet this year.\n",
    "\n",
    "# ===============================================================================\n",
    "\n",
    "def derive_age_group(df):\n",
    "    \"\"\"Derive Age Group based on defined buckets\"\"\"\n",
    "    print(\"[LOG] Running derive_age_group...\")\n",
    "    if 'age' in df.columns:\n",
    "        def categorize_age(age):\n",
    "            if pd.isnull(age):\n",
    "                return 'Unknown'\n",
    "            if age < 18: return 'Below 18'\n",
    "            elif 18 <= age <= 24: return '18-24'\n",
    "            elif 25 <= age <= 34: return '25-34'\n",
    "            elif 35 <= age <= 44: return '35-44'\n",
    "            elif 45 <= age <= 54: return '45-54'\n",
    "            elif 55 <= age <= 64: return '55-64'\n",
    "            else: return 'Above 65'\n",
    "        df['age_group'] = df['age'].apply(categorize_age)\n",
    "        print(\"[LOG] Age groups derived\")\n",
    "    else:\n",
    "        print(\"[LOG] Age column not found, skipping\")\n",
    "    return df\n",
    "\n",
    "# ===============================================================================\n",
    "\n",
    "def drop_dob_after_age_derived(df):\n",
    "    \"\"\"Drop DOB column after deriving age and age_group\"\"\"\n",
    "    print(\"[LOG] Running drop_dob_after_age_derived...\")\n",
    "    if 'dob' in df.columns:\n",
    "        df = df.drop(columns=['dob'])\n",
    "        print(\"[LOG] Dropped DOB column\")\n",
    "    else:\n",
    "        print(\"[LOG] DOB column not found, skipping\")\n",
    "    return df\n",
    "\n",
    "# =================================================================================\n",
    "\n",
    "def standardize_gender(df):\n",
    "    \"\"\"Clean and standardize gender values\"\"\"\n",
    "    print(\"[LOG] Running standardize_gender...\")\n",
    "    if 'gender' in df.columns:\n",
    "        df['gender'] = (\n",
    "            df['gender']\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.lower()\n",
    "            .replace({\n",
    "                'm': 'Male', 'male': 'Male', 'man': 'Male', 'boy': 'Male',\n",
    "                'f': 'Female', 'female': 'Female', 'woman': 'Female', 'girl': 'Female'\n",
    "            })\n",
    "        )\n",
    "        df.loc[~df['gender'].isin(['Male', 'Female']), 'gender'] = 'Unknown'\n",
    "        print(\"[LOG] Gender standardized (vectorized)\")\n",
    "    else:\n",
    "        print(\"[LOG] Gender column not found, skipping\")\n",
    "    return df\n",
    "\n",
    "# ==================================================================================\n",
    "\n",
    "def standardize_location(df):\n",
    "    \"\"\"Standardize City, and State fields\"\"\"\n",
    "    print(\"[LOG] Running standardize_location...\")\n",
    "        \n",
    "    # Helper function: detect suspicious city names\n",
    "    def is_suspicious_city(name):\n",
    "        if not name or name.strip() == '':\n",
    "            return True\n",
    "        name = str(name).strip()\n",
    "        \n",
    "        if len(name) < 2 or len(name) > 50: # Too short or too long\n",
    "            return True\n",
    "        \n",
    "        if re.search(r'[^A-Za-z\\s\\'-]', name):  # Contains non-alphabetic or weird symbols   # letters, space, apostrophe, dash allowed\n",
    "            return True\n",
    "        \n",
    "        if re.search(r'(.)\\1{3,}', name):   # Repeated characters (e.g., \"Ccciiiittty\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # --- City ---\n",
    "    if 'city' in df.columns:\n",
    "        df['city'] = df['city'].fillna('').astype(str).str.title().str.strip()\n",
    "        # Common city aliases (short forms, local spellings, etc.)\n",
    "        city_alias_map = {\n",
    "            \"Kl\": \"Kuala Lumpur\",\n",
    "            \"PJ\": \"Petaling Jaya\",\n",
    "        }\n",
    "        # Apply alias replacements first\n",
    "        df['city'] = df['city'].replace(city_alias_map)\n",
    "        \n",
    "        suspicious_mask = df['city'].apply(lambda x: is_suspicious_city(x) or x.lower() in ['others', 'other'])\n",
    "        suspicious_count = suspicious_mask.sum()\n",
    "        df.loc[suspicious_mask, 'city'] = 'Unknown'\n",
    "        \n",
    "        print(f\"[LOG] Standardized 'city'. Suspicious/unknown entries set to 'Unknown': {suspicious_count}\")\n",
    "    else:\n",
    "        print(\"[LOG] 'city' column not found, skipping city standardization\")\n",
    "    \n",
    "    # --- State ---\n",
    "    if 'state' in df.columns:\n",
    "        # malaysia_states = [\"Johor\", \"Kedah\", \"Kelantan\", \"Melaka\", \"Negeri Sembilan\",\"Pahang\", \"Perak\", \"Perlis\", \"Pulau Pinang\", \"Sabah\", \"Sarawak\", \"Selangor\", \"Terengganu\", \"Kuala Lumpur\", \"Labuan\", \"Putrajaya\"]\n",
    "        malaysia_states = [sub.name for sub in pycountry.subdivisions if sub.country_code == 'MY']\n",
    "        alias_map = {\n",
    "            \"Kuala Lumpur\": \"Wilayah Persekutuan Kuala Lumpur\",\n",
    "            \"Kl\": \"Wilayah Persekutuan Kuala Lumpur\",\n",
    "            \"Labuan\": \"Wilayah Persekutuan Labuan\",\n",
    "            \"Putrajaya\": \"Wilayah Persekutuan Putrajaya\"\n",
    "        }\n",
    "\n",
    "        df['state'] = df['state'].fillna('').astype(str).str.title().str.strip()\n",
    "        # ‚ö° Match only unique states once\n",
    "        unique_states = df['state'].unique()\n",
    "        state_map = {}\n",
    "        for s in unique_states:\n",
    "            s_clean = s.strip().title()\n",
    "\n",
    "            if not s_clean or s_clean == 'Unknown':\n",
    "                state_map[s] = 'Unknown'\n",
    "\n",
    "            elif s_clean in alias_map:   # Check alias first\n",
    "                state_map[s] = alias_map[s_clean]\n",
    "\n",
    "            else:\n",
    "                match, score = process.extractOne(s_clean, malaysia_states, scorer=fuzz.token_sort_ratio)\n",
    "                state_map[s] = match if score >= 80 else 'Unknown'\n",
    "\n",
    "        # Apply mapping to the dataframe\n",
    "        df['state'] = df['state'].map(state_map)\n",
    "        print(\"[LOG] State standardized (cached fuzzy matching)\")\n",
    "    else:\n",
    "        print(\"[LOG] 'state' column not found, skipping state standardization\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================= (CUSTOMER DATASET) STAGE 5: MISSING VALUE HANDLING =============================================\n",
    "def handle_missing_values_customer(df):\n",
    "    print(\"[LOG] Running handle_missing_values...\")\n",
    "\n",
    "    API_KEY = \"68f8ce9a38c3f632237334dyiedb96e\"\n",
    "    GEOCODE_URL = \"https://geocode.maps.co/search\"\n",
    "    SLEEP_TIME = 1.2\n",
    "    cache = {}  # ‚ö° moved outside loops\n",
    "\n",
    "    # --- Drop rows without ID ---\n",
    "    if 'customerid' in df.columns:\n",
    "        before_drop = len(df)\n",
    "        df = df[df['customerid'].notna()].copy()\n",
    "        print(f\"[LOG] Dropped {before_drop - len(df)} rows without CustomerID\")\n",
    "    else:\n",
    "        print(\"[LOG] 'customerid' column missing, skipping drop\")\n",
    "\n",
    "    # ----- Age -----\n",
    "    if 'age' in df.columns:\n",
    "        missing_ratio = df['age'].isna().mean()\n",
    "        print(f\"[LOG] Age missing ratio: {missing_ratio:.2%}\")\n",
    "        if missing_ratio > 0:\n",
    "            if 'gender' in df.columns and df['gender'].nunique() > 1:\n",
    "                # Group by gender if available\n",
    "                df['age'] = df.groupby('gender')['age'].transform(\n",
    "                    lambda x: x.fillna(x.median())\n",
    "                )\n",
    "                print(\"[LOG] Applied gender-based median imputation for missing age\")\n",
    "                \n",
    "                # Always fill any remaining missing values with overall median \n",
    "                df['age'] = df['age'].fillna(df['age'].median())\n",
    "                \n",
    "                # Update derived column if needed\n",
    "                df = derive_age_group(df)\n",
    "        else:\n",
    "            print(\"[LOG] No missing values for age\")\n",
    "    else:\n",
    "        print(\"[LOG] 'age' column missing, skipping age imputation\")\n",
    "\n",
    "    # --- Gender mode imputation ---\n",
    "    if 'gender' in df.columns:\n",
    "        mode_series = df.loc[df['gender'].isin(['Male', 'Female']), 'gender'].mode()\n",
    "\n",
    "        if not mode_series.empty:\n",
    "            mode = mode_series[0]\n",
    "            unknown_mask = df['gender'] == 'Unknown'\n",
    "            count = unknown_mask.sum()\n",
    "            if count > 0:\n",
    "                df.loc[unknown_mask, 'gender'] = mode\n",
    "                print(f\"[LOG] Replaced {count} 'Unknown' gender values with mode: {mode}\")\n",
    "        else:\n",
    "            print(\"[WARN] No valid gender mode found (only 'Unknown' present) ‚Äî skipping imputation.\")\n",
    "    else:\n",
    "        print(\"[LOG] 'gender' column missing, skipping gender imputation\")\n",
    "\n",
    "   \n",
    "    # --- City & State handling ---\n",
    "    if {'city', 'state'}.issubset(df.columns):\n",
    "        print(\"\\nüîç Handling missing city/state values...\")\n",
    "        malaysia_states = [sub.name for sub in pycountry.subdivisions if sub.country_code == 'MY']\n",
    "        cache = {}  # city -> validated state\n",
    "        SLEEP_TIME = 1.2\n",
    "        \n",
    "        # Case 1: missing state but city known ‚Üí fill via geocoding API\n",
    "        print(\"\\n[LOG] Case 1: Filling missing state where city is known...\")\n",
    "        # Get rows needing state fill (city known, state unknown)\n",
    "        mask_case1 = (df['city'] != 'Unknown') & (df['state'] == 'Unknown')\n",
    "        cities_to_query = df.loc[mask_case1, 'city'].unique().tolist()\n",
    "\n",
    "        print(f\"[LOG] {len(cities_to_query)} unique cities need state lookup\")\n",
    "        \n",
    "        for city in cities_to_query:\n",
    "            if city not in cache:\n",
    "                # Call API\n",
    "                try:\n",
    "                    resp = requests.get(GEOCODE_URL, params={\"q\": f\"{city}, Malaysia\", \"api_key\": API_KEY}, timeout=10)\n",
    "                    if resp.status_code == 200:\n",
    "                        data = resp.json()\n",
    "                        if isinstance(data, list) and data:\n",
    "                            state_name = data[0].get(\"address\", {}).get(\"state\")\n",
    "                            if state_name and state_name in malaysia_states:\n",
    "                                cache[city] = state_name\n",
    "                            else:\n",
    "                                cache[city] = None\n",
    "                        else:\n",
    "                            cache[city] = None\n",
    "                    else:\n",
    "                        cache[city] = None\n",
    "                    time.sleep(SLEEP_TIME)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Failed to get state for city '{city}': {e}\")\n",
    "                    cache[city] = None\n",
    "\n",
    "            # Fill values\n",
    "            fill_state = cache.get(city)\n",
    "            if fill_state:\n",
    "                df.loc[(df['city'] == city) & (df['state'] == 'Unknown'), 'state'] = fill_state\n",
    "                print(f\"[TRACE] Filled {city} ‚Üí state='{fill_state}' (API valid)\")\n",
    "            else:\n",
    "                # Fallback: use mode state & mode city for that state\n",
    "                # The API fails to find the city || The response doesn‚Äôt contain a valid \"state\" field || Or the returned \"state\" isn‚Äôt in the official Malaysia subdivision list.\n",
    "                valid_states = df[df['state'] != 'Unknown']['state']\n",
    "                mode_state = valid_states.mode()[0] if not valid_states.empty else 'Unknown'\n",
    "\n",
    "                # Compute mode city per state\n",
    "                mode_city_per_state = (\n",
    "                    df[df['city'] != 'Unknown'].groupby('state')['city']\n",
    "                    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown')\n",
    "                    .to_dict()\n",
    "                )\n",
    "                mode_city = mode_city_per_state.get(mode_state, 'Unknown')\n",
    "\n",
    "                mask_fill = (df['city'] == city) & (df['state'] == 'Unknown')\n",
    "                df.loc[mask_fill, 'state'] = mode_state\n",
    "                df.loc[mask_fill, 'city'] = mode_city\n",
    "                print(f\"[TRACE] Filled {mask_fill.sum()} row(s) ‚Üí city='{mode_city}', state='{mode_state}' (Fallback)\")\n",
    "\n",
    "        # Case 2: missing city but state known ‚Üí fill with mode city per state\n",
    "        print(\"\\n[LOG] Case 2: Filling missing city where state is known...\")\n",
    "        mask_case2 = (df['city'] == 'Unknown') & (df['state'] != 'Unknown')\n",
    "        if mask_case2.any():\n",
    "            mode_city_per_state = (\n",
    "                df[df['city'] != 'Unknown'].groupby('state')['city']\n",
    "                .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'Unknown')\n",
    "                .to_dict()\n",
    "            )\n",
    "            for state, city_mode in mode_city_per_state.items():\n",
    "                mask_fill = mask_case2 & (df['state'] == state)\n",
    "                df.loc[mask_fill, 'city'] = city_mode\n",
    "                print(f\"[TRACE] Filled {mask_fill.sum()} row(s) ‚Üí missing city for state='{state}' ‚Üí city='{city_mode}'\")\n",
    "\n",
    "        # Case 3: both missing ‚Üí fill with most frequent pair\n",
    "        print(\"\\n[LOG] Case 3: Filling missing city and state...\")\n",
    "        mask_case3 = (df['city'] == 'Unknown') & (df['state'] == 'Unknown')\n",
    "        if mask_case3.any():\n",
    "            valid_pairs = df[(df['city'] != 'Unknown') & (df['state'] != 'Unknown')]\n",
    "            if not valid_pairs.empty:\n",
    "                city_mode, state_mode = valid_pairs.groupby(['city', 'state']).size().idxmax()\n",
    "                df.loc[mask_case3, ['city', 'state']] = [city_mode, state_mode]\n",
    "                print(f\"[TRACE] Filled {mask_case3.sum()} row(s) ‚Üí missing city/state ‚Üí City='{city_mode}', State='{state_mode}'\")\n",
    "            else:\n",
    "                print(\"[WARN] No valid city/state pairs to fill missing both values\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================= (CUSTOMER DATASET) STAGE 6: OUTLIER DETECTION =============================================\n",
    "def customer_detect_outliers(df):\n",
    "    \"\"\"Adaptive outlier handling based on dataset size.\"\"\"\n",
    "    print(\"[LOG] Running detect_outliers...\")\n",
    "    if 'age' in df.columns:\n",
    "        df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "        # df['age_original'] = df['age']  # ‚úÖ Keep a copy of original age values (for comparison or re-deriving age_group)\n",
    "\n",
    "        n = len(df)\n",
    "        print(f\"[LOG] Dataset has {n} rows\")\n",
    "        if n < 500:\n",
    "            # IQR method\n",
    "            Q1 = df['age'].quantile(0.25)\n",
    "            Q3 = df['age'].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = df[(df['age'] <  Q1) | (df['age'] > Q3)].shape[0]\n",
    "            df.loc[(df['age'] < lower_bound) | (df['age'] > upper_bound), 'age'] = np.nan\n",
    "            print(f\"[LOG] IQR Applied for {n} rows. Range: [{lower_bound:.1f}, {upper_bound:.1f}] Outliers set to NaN: {outliers}\")\n",
    "        else:\n",
    "            # Percentile capping\n",
    "            lower_bound = df['age'].quantile(0.01)\n",
    "            upper_bound = df['age'].quantile(0.99)\n",
    "            df['age'] = df['age'].clip(lower=lower_bound, upper=upper_bound)\n",
    "            print(f\"[LOG] Percentile capping  applied for {n} rows. Capped to [{lower_bound:.1f}, {upper_bound:.1f}]\")\n",
    "    else:\n",
    "        print(\"[LOG] 'age' column missing, skipping outlier detection\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8bb09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================= (CUSTOMER DATASET) DATASET CLEANING PIPELINE =============================================\n",
    "def clean_customer_dataset(df, original_customer_dataset_name):\n",
    "    \"\"\"\n",
    "    Main cleaning pipeline for customer dataset.\n",
    "    Executes all stages in proper order:\n",
    "    0. Column Normalization\n",
    "    1. Schema & Column Validation\n",
    "    2. Duplicate Entry Removal\n",
    "    3. Standardization & Normalization\n",
    "    4. Missing Value Handling\n",
    "    5. Outlier Detection\n",
    "    6. Deduplication\n",
    "    Finally, saves the cleaned dataset and returns it.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting customer data cleaning pipeline...\\n\")\n",
    "\n",
    "    # =======================================================\n",
    "    # STAGE 0: NORMALIZE COLUMN NAMES (FROM GENERIC FUNCTION)\n",
    "    # =======================================================\n",
    "    print(\"========== [STAGE 0 START] Normalize Column Names ==========\")\n",
    "    df = normalize_columns_name(df)\n",
    "    print(\"‚úÖ [STAGE 0 COMPLETE] Column names normalized.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 1: SCHEMA & COLUMN VALIDATION\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 1 START] Schema & Column Validation ==========\")\n",
    "    df, optional_msg = customer_check_optional_columns(df)\n",
    "    \n",
    "    # (FROM GENERIC FUNCTION)\n",
    "    customer_mandatory = [\"customerid\", \"city\", \"state\"]\n",
    "    df, mandatory_msg = check_mandatory_columns(df,dataset_type=\"customer\", mandatory_columns=customer_mandatory)\n",
    "\n",
    "    print(optional_msg)\n",
    "    print(mandatory_msg)\n",
    "    print(\"‚úÖ [STAGE 1 COMPLETE] Schema validation done.\\n\")\n",
    "\n",
    "    # ============================================================\n",
    "    # STAGE 2: REMOVE DUPLICATE ENTRY ROWS (FROM GENERIC FUNCTION)\n",
    "    # ============================================================\n",
    "    print(\"========== [STAGE 2 START] Remove Duplicate Entry Rows ==========\")\n",
    "    df = remove_duplicate_entries(df)\n",
    "    print(\"‚úÖ [STAGE 2 COMPLETE] Duplicate entries removed.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 3: DEDUPLICATION\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 3 START] Deduplication ==========\")\n",
    "    df = deduplicate_customers(df)\n",
    "    print(\"‚úÖ [STAGE 3 COMPLETE] Duplicate CustomerIDs deduplicated.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # STAGE 4: STANDARDIZATION & NORMALIZATION\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 4 START] Standardization & Normalization ==========\")\n",
    "    df = standardize_customer_id(df)\n",
    "    df = standardize_dob(df)\n",
    "    df = derive_age_features(df)\n",
    "    df = derive_age_group(df)\n",
    "    df = drop_dob_after_age_derived(df)\n",
    "    df = standardize_gender(df)\n",
    "    df = standardize_location(df)\n",
    "    print(\"‚úÖ [STAGE 4 COMPLETE] Standardization and normalization finished.\\n\")\n",
    "    \n",
    "    print(\"========== [Standardized Customer Dataset START] Save Standardized Dataset ==========\")\n",
    "    base_name, ext = os.path.splitext(original_customer_dataset_name)\n",
    "    standardized_file = f\"{base_name}_standardized{ext}\"\n",
    "    df.to_csv(standardized_file, index=False)\n",
    "    print(f\"‚úÖ [STANDARDIZED STAGE COMPLETE] Standardized Customer dataset saved as: {standardized_file}\\n\")\n",
    "    \n",
    "    # =============================================\n",
    "    # STAGE 5: MISSING VALUE HANDLING\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 5 START] Missing Value Handling ==========\")\n",
    "    df = handle_missing_values_customer(df)\n",
    "    print(\"‚úÖ [STAGE 5 COMPLETE] Missing values handled.\\n\")\n",
    "\n",
    "    # # =============================================\n",
    "    # # STAGE 6: OUTLIER DETECTION\n",
    "    # # =============================================\n",
    "    print(\"========== [STAGE 6 START] Outlier Detection ==========\")\n",
    "    df = customer_detect_outliers(df)   # make sure detect_outliers returns df\n",
    "    print(\"‚úÖ [STAGE 6 COMPLETE] Outliers handled.\\n\")\n",
    "\n",
    "    # =============================================\n",
    "    # SAVE CLEANED DATASET\n",
    "    # =============================================\n",
    "    print(\"========== [FINAL STAGE START] Save Cleaned Dataset ==========\")\n",
    "    # base_name, ext = os.path.splitext(original_customer_dataset_name)\n",
    "    cleaned_file = f\"{base_name}_cleaned{ext}\"\n",
    "    df.to_csv(cleaned_file, index=False)\n",
    "    print(f\"‚úÖ [FINAL STAGE COMPLETE] Cleaned dataset saved as: {cleaned_file}\\n\")\n",
    "\n",
    "    print(\"==========================================================\")\n",
    "    print(\"üéâ Data cleaning pipeline completed successfully!\\n\")\n",
    "    return df, cleaned_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dbb17ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting customer data cleaning pipeline...\n",
      "\n",
      "========== [STAGE 0 START] Normalize Column Names ==========\n",
      "[LOG] Running normalize_columns_name...\n",
      "[LOG] Columns after normalization: ['customerid', 'date of birth', 'gender', 'city', 'state']\n",
      "‚úÖ [STAGE 0 COMPLETE] Column names normalized.\n",
      "\n",
      "========== [STAGE 1 START] Schema & Column Validation ==========\n",
      "[LOG] Running customer_check_optional_columns...\n",
      "[LOG] Optional column 'date of birth' fill ratio: 0.00\n",
      "[LOG] Dropped optional column 'date of birth' due to too many missing values\n",
      "[LOG] Optional column 'gender' fill ratio: 0.00\n",
      "[LOG] Dropped optional column 'gender' due to too many missing values\n",
      "[LOG] Running check_mandatory_columns for customer dataset...\n",
      "[LOG] Mandatory column 'customerid' fill ratio: 1.00\n",
      "[LOG] Mandatory column 'city' fill ratio: 0.55\n",
      "[LOG] Mandatory column 'state' fill ratio: 0.53\n",
      "We noticed that very few entries were provided for date of birth, gender. These columns have been removed. Segmentation will still be performed using geographic (City, State) and behavioral data (e.g., orders, purchase items, total spend).\n",
      "\n",
      "Missing Data Summary:\n",
      "date of birth: 100.0% missing\n",
      "gender: 100.0% missing\n",
      "Some key fields in the customer dataset have a high number of missing values: city, state. The system will continue cleaning and handle missing values automatically, but we STRONGLY encourage you to reupload your source data, ensure it was as complete as possible for accurate segmentation result later.\n",
      "\n",
      "Missing Data Summary:\n",
      "customerid: 0.0% missing\n",
      "city: 44.8% missing\n",
      "state: 46.9% missing\n",
      "‚úÖ [STAGE 1 COMPLETE] Schema validation done.\n",
      "\n",
      "========== [STAGE 2 START] Remove Duplicate Entry Rows ==========\n",
      "[LOG] Running remove_duplicate_entries...\n",
      "[LOG] Removed 879 duplicate rows.\n",
      "‚úÖ [STAGE 2 COMPLETE] Duplicate entries removed.\n",
      "\n",
      "========== [STAGE 3 START] Deduplication ==========\n",
      "[LOG] Running deduplicate_customers...\n",
      "[LOG] Deduplication complete (vectorized)\n",
      "‚úÖ [STAGE 3 COMPLETE] Duplicate CustomerIDs deduplicated.\n",
      "\n",
      "========== [STAGE 4 START] Standardization & Normalization ==========\n",
      "[LOG] Running standardize_customer_id...\n",
      "[LOG] CustomerID column standardized\n",
      "[LOG] Running standardize_dob...\n",
      "[LOG] DOB column not found, skipping\n",
      "[LOG] Running derive_age_features...\n",
      "[LOG] DOB column not found, skipping\n",
      "[LOG] Running derive_age_group...\n",
      "[LOG] Age column not found, skipping\n",
      "[LOG] Running drop_dob_after_age_derived...\n",
      "[LOG] DOB column not found, skipping\n",
      "[LOG] Running standardize_gender...\n",
      "[LOG] Gender column not found, skipping\n",
      "[LOG] Running standardize_location...\n",
      "[LOG] Standardized 'city'. Suspicious/unknown entries set to 'Unknown': 293\n",
      "[LOG] State standardized (cached fuzzy matching)\n",
      "‚úÖ [STAGE 4 COMPLETE] Standardization and normalization finished.\n",
      "\n",
      "========== [Standardized Customer Dataset START] Save Standardized Dataset ==========\n",
      "‚úÖ [STANDARDIZED STAGE COMPLETE] Standardized Customer dataset saved as: 2021 - 2025 Customer - Copy_standardized.csv\n",
      "\n",
      "========== [STAGE 5 START] Missing Value Handling ==========\n",
      "[LOG] Running handle_missing_values...\n",
      "[LOG] Dropped 0 rows without CustomerID\n",
      "[LOG] 'age' column missing, skipping age imputation\n",
      "[LOG] 'gender' column missing, skipping gender imputation\n",
      "\n",
      "üîç Handling missing city/state values...\n",
      "\n",
      "[LOG] Case 1: Filling missing state where city is known...\n",
      "[LOG] 35 unique cities need state lookup\n",
      "[TRACE] Filled Jelutong ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Tanjong Tokong ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Nibong Tebal ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Bayan Lepas ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Tanjung Bungah ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Sungai Bakap ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Balik Pulau ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Timur Laut ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Bukit Mertajam ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Gelugor ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Bukit Minyak ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Permatang Pauh ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Georgetown ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Seberang Perai Utara ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Singapore ‚Üí state='Johor' (API valid)\n",
      "[TRACE] Filled Barat Daya ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Seberang Perai Tengah ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Pulau Pinang ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Simpang Ampat ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Seberang Perai Selatan ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled 1 row(s) ‚Üí city='Petaling Jaya', state='Selangor' (Fallback)\n",
      "[TRACE] Filled Butterworth ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Southwest Penang Island District ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled North Seberang Perai District ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled 1 row(s) ‚Üí city='Petaling Jaya', state='Selangor' (Fallback)\n",
      "[TRACE] Filled Alor Gajah ‚Üí state='Melaka' (API valid)\n",
      "[TRACE] Filled Durian Tunggal ‚Üí state='Melaka' (API valid)\n",
      "[TRACE] Filled Kubang Semang ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Merlimau ‚Üí state='Melaka' (API valid)\n",
      "[TRACE] Filled 1 row(s) ‚Üí city='Petaling Jaya', state='Selangor' (Fallback)\n",
      "[TRACE] Filled Tasek Gelugor ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Selangor ‚Üí state='Selangor' (API valid)\n",
      "[TRACE] Filled Penang ‚Üí state='Pulau Pinang' (API valid)\n",
      "[TRACE] Filled Kedah ‚Üí state='Kedah' (API valid)\n",
      "[TRACE] Filled Perak ‚Üí state='Perak' (API valid)\n",
      "\n",
      "[LOG] Case 2: Filling missing city where state is known...\n",
      "[TRACE] Filled 12 row(s) ‚Üí missing city for state='Johor' ‚Üí city='Johor Bahru'\n",
      "[TRACE] Filled 2 row(s) ‚Üí missing city for state='Kedah' ‚Üí city='Kuala Muda'\n",
      "[TRACE] Filled 2 row(s) ‚Üí missing city for state='Kelantan' ‚Üí city='Kota Bharu'\n",
      "[TRACE] Filled 9 row(s) ‚Üí missing city for state='Melaka' ‚Üí city='Melaka Tengah'\n",
      "[TRACE] Filled 6 row(s) ‚Üí missing city for state='Negeri Sembilan' ‚Üí city='Seremban'\n",
      "[TRACE] Filled 0 row(s) ‚Üí missing city for state='Pahang' ‚Üí city='Kuantan'\n",
      "[TRACE] Filled 4 row(s) ‚Üí missing city for state='Perak' ‚Üí city='Kinta'\n",
      "[TRACE] Filled 1 row(s) ‚Üí missing city for state='Perlis' ‚Üí city='Kuala Perlis'\n",
      "[TRACE] Filled 0 row(s) ‚Üí missing city for state='Pulau Pinang' ‚Üí city='Timur Laut'\n",
      "[TRACE] Filled 10 row(s) ‚Üí missing city for state='Sabah' ‚Üí city='Kota Kinabalu'\n",
      "[TRACE] Filled 4 row(s) ‚Üí missing city for state='Sarawak' ‚Üí city='Kuching'\n",
      "[TRACE] Filled 79 row(s) ‚Üí missing city for state='Selangor' ‚Üí city='Petaling Jaya'\n",
      "[TRACE] Filled 1 row(s) ‚Üí missing city for state='Terengganu' ‚Üí city='Kuala Terengganu'\n",
      "[TRACE] Filled 69 row(s) ‚Üí missing city for state='Wilayah Persekutuan Kuala Lumpur' ‚Üí city='Kuala Lumpur'\n",
      "[TRACE] Filled 0 row(s) ‚Üí missing city for state='Wilayah Persekutuan Labuan' ‚Üí city='Labuan'\n",
      "[TRACE] Filled 1 row(s) ‚Üí missing city for state='Wilayah Persekutuan Putrajaya' ‚Üí city='Putrajaya'\n",
      "\n",
      "[LOG] Case 3: Filling missing city and state...\n",
      "[TRACE] Filled 93 row(s) ‚Üí missing city/state ‚Üí City='Kuala Lumpur', State='Wilayah Persekutuan Kuala Lumpur'\n",
      "‚úÖ [STAGE 5 COMPLETE] Missing values handled.\n",
      "\n",
      "========== [STAGE 6 START] Outlier Detection ==========\n",
      "[LOG] Running detect_outliers...\n",
      "[LOG] 'age' column missing, skipping outlier detection\n",
      "‚úÖ [STAGE 6 COMPLETE] Outliers handled.\n",
      "\n",
      "========== [FINAL STAGE START] Save Cleaned Dataset ==========\n",
      "‚úÖ [FINAL STAGE COMPLETE] Cleaned dataset saved as: 2021 - 2025 Customer - Copy_cleaned.csv\n",
      "\n",
      "==========================================================\n",
      "üéâ Data cleaning pipeline completed successfully!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerid</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CUST0001</td>\n",
       "      <td>Sungai Besi</td>\n",
       "      <td>Wilayah Persekutuan Kuala Lumpur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUST0002</td>\n",
       "      <td>Mutiara Damansara</td>\n",
       "      <td>Selangor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CUST0003</td>\n",
       "      <td>Shah Alam</td>\n",
       "      <td>Selangor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CUST0004</td>\n",
       "      <td>Muar</td>\n",
       "      <td>Johor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUST0005</td>\n",
       "      <td>Petaling Jaya</td>\n",
       "      <td>Selangor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customerid               city                             state\n",
       "0   CUST0001        Sungai Besi  Wilayah Persekutuan Kuala Lumpur\n",
       "1   CUST0002  Mutiara Damansara                          Selangor\n",
       "2   CUST0003          Shah Alam                          Selangor\n",
       "3   CUST0004               Muar                             Johor\n",
       "4   CUST0005      Petaling Jaya                          Selangor"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep a copy for comparison\n",
    "original_customer_df = customer_df.copy()\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>> (CUSTOMER DATASET) IMPLEMENT CLEANING PIPELINE >>>>>>>>>>>>>>>>>>>\n",
    "cleaned_customer_df, cleaned_file_name = clean_customer_dataset(customer_df, original_customer_dataset_name)\n",
    "\n",
    "# AFTER CLEANING : CLEANED VERSION TO BE STORED BACK TO DATABASE\n",
    "cleaned_customer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cfcb11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================= (CUSTOMER DATASET) DATASET CLEANING REPORT =============================================\n",
    "def generate_cleaning_report(original_df, cleaned_df, filename_prefix=\"cleaning_summary\"):\n",
    "    report = {}\n",
    "\n",
    "    # --- Missing Values ---\n",
    "    report['Missing Values (Before)'] = original_df.isna().sum()\n",
    "    report['Missing Values (After)'] = cleaned_df.isna().sum()\n",
    "\n",
    "    # --- Duplicates ---\n",
    "    duplicates_before = len(original_df) - len(original_df.drop_duplicates())\n",
    "    duplicates_after = len(cleaned_df) - len(cleaned_df.drop_duplicates())\n",
    "    report['Duplicates (Before)'] = [duplicates_before] * len(original_df.columns)\n",
    "    report['Duplicates (After)'] = [duplicates_after] * len(original_df.columns)\n",
    "\n",
    "    # --- Combine into Summary DataFrame ---\n",
    "    summary = pd.DataFrame(report, index=original_df.columns)\n",
    "    summary.index.name = \"Column Name\"\n",
    "\n",
    "    # --- Save to CSV (with timestamp) ---\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{filename_prefix}_{timestamp}.csv\"\n",
    "    summary.to_csv(filename, index=True)\n",
    "\n",
    "    # --- Print Summary (compact view) ---\n",
    "    print(\"\\nüìä === DATA CLEANING SUMMARY REPORT ===\")\n",
    "    print(summary.to_string())\n",
    "    print(f\"\\n‚úÖ Report saved to: {filename}\")\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a288255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> (CUSTOMER DATASET) DATASET CLEANING PIPELINE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# summary = generate_cleaning_report(original_customer_df, cleaned_customer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816c764",
   "metadata": {},
   "source": [
    "## Perform Data Cleaning Pipeline - OrderDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d86825b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Purchase Item</th>\n",
       "      <th>Purchase Date</th>\n",
       "      <th>Item Price</th>\n",
       "      <th>Purchase Quantity</th>\n",
       "      <th>Total Spend</th>\n",
       "      <th>Transaction Method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>211103JN8114CU</td>\n",
       "      <td>CUST0001</td>\n",
       "      <td>Aloe Vera Homemade Soap | Acne-Prone Skin | Ba...</td>\n",
       "      <td>03/11/2021 20:21</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RM42.03</td>\n",
       "      <td>ShopeePay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211103JN8114CU</td>\n",
       "      <td>CUST0001</td>\n",
       "      <td>Oats &amp; Calendula Dandruff Shampoo Bar | Sulfat...</td>\n",
       "      <td>03/11/2021 20:21</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shopee Coin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>211104MMPQH994</td>\n",
       "      <td>CUST0002</td>\n",
       "      <td>Calendula Homemade Soap | Suitable For Eczema-...</td>\n",
       "      <td>04/11/2021 15:17</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>$70</td>\n",
       "      <td>ShopeePay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>211104MMPQH994</td>\n",
       "      <td>CUST0002</td>\n",
       "      <td>Green Tea &amp; Mint Hair Loss Shampoo | Sulfate-F...</td>\n",
       "      <td>04/11/2021 15:17</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>211104MSJ8BGNC</td>\n",
       "      <td>CUST0003</td>\n",
       "      <td>Green Tea &amp; Mint Hair Loss Shampoo | Sulfate-F...</td>\n",
       "      <td>04/11/2021 16:44</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "      <td>ShopeePay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          OrderID CustomerID  \\\n",
       "0  211103JN8114CU   CUST0001   \n",
       "1  211103JN8114CU   CUST0001   \n",
       "2  211104MMPQH994   CUST0002   \n",
       "3  211104MMPQH994   CUST0002   \n",
       "4  211104MSJ8BGNC   CUST0003   \n",
       "\n",
       "                                       Purchase Item     Purchase Date  \\\n",
       "0  Aloe Vera Homemade Soap | Acne-Prone Skin | Ba...  03/11/2021 20:21   \n",
       "1  Oats & Calendula Dandruff Shampoo Bar | Sulfat...  03/11/2021 20:21   \n",
       "2  Calendula Homemade Soap | Suitable For Eczema-...  04/11/2021 15:17   \n",
       "3  Green Tea & Mint Hair Loss Shampoo | Sulfate-F...  04/11/2021 15:17   \n",
       "4  Green Tea & Mint Hair Loss Shampoo | Sulfate-F...  04/11/2021 16:44   \n",
       "\n",
       "   Item Price  Purchase Quantity Total Spend Transaction Method  \n",
       "0        18.0                1.0     RM42.03          ShopeePay  \n",
       "1        23.0                1.0         NaN        Shopee Coin  \n",
       "2        23.0                2.0         $70          ShopeePay  \n",
       "3        24.0                1.0         NaN                NaN  \n",
       "4        24.0                1.0          26          ShopeePay  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load dataset\n",
    "# Replace 'customer_dataset.csv' with your actual file name or path\n",
    "order_file_path = \"C:/Users/user/OneDrive/Desktop/Onedrive_YuyanDipsy/OneDrive/UM Y4S1/WIA3002 FYP 1 & 2/FYP2/Data/Soapan Santun - Use/2021 - 2025 Order.csv\"\n",
    "original_order_dataset_name = \"2021 - 2025 Order.csv\"\n",
    "\n",
    "# Read dataset\n",
    "order_df = pd.read_csv(order_file_path)\n",
    "\n",
    "# Show first few rows (original raw data)\n",
    "order_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd1eef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================= (ORDER DATASET) STAGE 0: NORMALIZE COLUMN NAMES =============================================\n",
    "# From Generic function: normalize_columns_name\n",
    "\n",
    "# ============================================= (ORDER DATASET) STAGE 1: SCHEMA & COLUMN VALIDATION =============================================\n",
    "# From Generic function\n",
    "\n",
    "# ============================================= (ORDER DATASET) STAGE 2: REMOVE DUPLICATE ENTRY ROW =================================================\n",
    "# From Generic function: remove_duplicate_entries\n",
    "\n",
    "# ============================================= (ORDER DATASET) STAGE 3: STANDARDIZATION & NORMALIZATION =============================================\n",
    "# From Generic function: standardize_customer_id\n",
    "\n",
    "def standardized_order_id(df):\n",
    "    \"\"\"Standardize OrderID format (null is '')\"\"\"\n",
    "    print(\"[LOG] Running standardize_order_id...\")\n",
    "    if 'orderid' in df.columns:\n",
    "        # Fill NaN with empty string before converting to string\n",
    "        df.loc[:, 'orderid'] = df['orderid'].fillna('').astype(str).str.strip().str.upper()\n",
    "        print(\"[LOG] OrderID column standardized\")\n",
    "    else:\n",
    "        print(\"[LOG] OrderID column not found, skipping\")\n",
    "    return df\n",
    "\n",
    "def standardize_purchase_item(df):\n",
    "    \"\"\"\"Standardize Purchase Item names (NaN preserved)\"\"\"\n",
    "    print(\"[LOG] Running standardized_purchase_item...\")\n",
    "    if \"purchase item\" in df.columns:\n",
    "        df.loc[:, \"purchase item\"] = (\n",
    "            df[\"purchase item\"]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.title()\n",
    "        )\n",
    "        print(\"[LOG] Purchase item standardized\")\n",
    "    return df\n",
    "\n",
    "def standardize_purchase_date(df):\n",
    "    \"\"\"Standardize Purchase Date into separate date and time columns(NaT preserved)\"\"\"\n",
    "    print(\"[LOG] Running standardize_purchase_date...\")\n",
    "\n",
    "    if \"purchase date\" in df.columns:\n",
    "        # Ensure df is a deep copy (prevents SettingWithCopyWarning)\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Clean values\n",
    "        df.loc[:, \"purchase date\"] = df[\"purchase date\"].astype(str).str.strip()\n",
    "\n",
    "        # Convert to datetime\n",
    "        df.loc[:, \"purchase datetime\"] = pd.to_datetime(\n",
    "            df[\"purchase date\"], errors=\"coerce\", dayfirst=True\n",
    "        )\n",
    "\n",
    "        # Detect which rows have time info\n",
    "        has_time_mask = df[\"purchase date\"].str.contains(\":\", regex=False)\n",
    "\n",
    "        # Create standardized columns\n",
    "        df.loc[:, \"purchase date\"] = df[\"purchase datetime\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        df.loc[:, \"purchase time\"] = None\n",
    "        df.loc[has_time_mask, \"purchase time\"] = (\n",
    "            df.loc[has_time_mask, \"purchase datetime\"].dt.strftime(\"%H:%M:%S\")\n",
    "        )\n",
    "\n",
    "        # Drop intermediate column\n",
    "        df.drop(columns=[\"purchase datetime\"], inplace=True, errors=\"ignore\")\n",
    "    else:\n",
    "        print(\"[WARN] 'purchase date' column not found, skipping.\")\n",
    "    print(\"[LOG] Purchase date standardization complete.\")\n",
    "    return df\n",
    "\n",
    "def standardized_item_price_and_total_spend(df):\n",
    "    \"\"\"\n",
    "    Standardize 'item price' and 'total spend' columns:\n",
    "    - Remove currency symbols/text (RM, $, MYR, etc.)\n",
    "    - Convert to numeric (invalid entries ‚Üí NaN)\n",
    "    - Round to 2 decimal places\n",
    "    \"\"\"\n",
    "    print(\"[LOG] Running standardized_item_price_and_total_spend...\")\n",
    "\n",
    "    for col in [\"item price\", \"total spend\"]:\n",
    "        if col in df.columns:\n",
    "            # Step 1: Remove currency symbols and text\n",
    "            df[col] = (\n",
    "                df[col]\n",
    "                .astype(str)\n",
    "                .str.replace(r\"[^\\d\\.\\-]\", \"\", regex=True)\n",
    "            )\n",
    "\n",
    "            # Step 2: Convert to numeric, coercing invalid values to NaN\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "            # Step 3: Round to 2 decimal places\n",
    "            df[col] = df[col].round(2)\n",
    "\n",
    "            print(f\"[LOG] {col} standardized: numeric, 2 decimal places\")\n",
    "        else:\n",
    "            print(f\"[LOG] '{col}' column not found, skipping\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def standardize_purchase_quantity(df):\n",
    "    \"\"\"Standardize Purchase Quantity to integer (NaN preserved)\"\"\"\n",
    "    print(\"[LOG] Running standardize_purchase_quantity...\")\n",
    "\n",
    "    if \"purchase quantity\" in df.columns:\n",
    "        # Remove non-numeric characters (like pcs, x, units, etc.)\n",
    "        df[\"purchase quantity\"] = (\n",
    "            df[\"purchase quantity\"]\n",
    "            .astype(str)\n",
    "            .str.replace(r\"[^\\d\\.\\-]\", \"\", regex=True)  # keep digits only\n",
    "        )\n",
    "\n",
    "        # Convert to numeric (NaN for invalid)\n",
    "        df[\"purchase quantity\"] = pd.to_numeric(df[\"purchase quantity\"], errors=\"coerce\")\n",
    "\n",
    "        # Round any decimals (e.g. 2.5 ‚Üí 2)\n",
    "        df[\"purchase quantity\"] = df[\"purchase quantity\"].round(0).astype(\"Int64\")\n",
    "\n",
    "        print(\"[LOG] Purchase quantity standardized to integer format\")\n",
    "    else:\n",
    "        print(\"[LOG] 'purchase quantity' column not found, skipping\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def standardize_transaction_method(df):\n",
    "    \"\"\"\n",
    "    Standardize 'transaction method' into categories:\n",
    "    ['Cash', 'Card', 'E-Wallet', 'Online Banking', 'Auto-Debit', 'Cheque']\n",
    "    (Unknown represents missing values)\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    print(\"[LOG] Running standardize_transaction_method...\")\n",
    "\n",
    "    if \"transaction method\" not in df.columns:\n",
    "        print(\"[LOG] 'transaction method' column not found, skipping.\")\n",
    "        return df\n",
    "\n",
    "    # Step 1: Normalize text\n",
    "    df[\"transaction method\"] = (\n",
    "        df[\"transaction method\"]\n",
    "        .astype(str)\n",
    "        .str.lower()\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # Step 2: Define patterns (non-capturing groups)\n",
    "    patterns = {\n",
    "        \"Cash\": r\"\\b(?:cash|tunai|otc|counter)\\b\",\n",
    "        \"Card\": r\"\\b(?:card|visa|master|credit|debit|amex|credit.?debit)\\b\",\n",
    "        \"E-Wallet\": r\"\\b(?:tng|touch\\s*n\\s*go|grab\\s*pay|grabpay|boost|shopee\\s*pay|shopeepay|spaylater|duitnow|ewallet|e-?wallet|qr|qr\\s*pay|qrcode)\\b\",\n",
    "        \"Online Banking\": r\"\\b(?:bank|transfer|fpx|online\\s*payment|maybank2u|cimbclicks|duitnow\\s*qr|public\\s*bank)\\b\",\n",
    "        \"Auto-Debit\": r\"\\b(?:auto.?debit|standing|recurring|subscription|auto\\s*pay)\\b\",\n",
    "        \"Cheque\": r\"\\b(?:cheque|cek|check)\\b\",\n",
    "    }\n",
    "\n",
    "    # Step 4: Apply vectorized regex matching\n",
    "    for category, pattern in patterns.items():\n",
    "        mask = df[\"transaction method\"].str.contains(pattern, flags=re.IGNORECASE, na=False, regex=True)\n",
    "        df.loc[mask, \"transaction method\"] = category\n",
    "    \n",
    "    # Step 5: Replace values that didn‚Äôt match anything\n",
    "    valid_categories = list(patterns.keys())\n",
    "    df.loc[~df[\"transaction method\"].isin(valid_categories), \"transaction method\"] = \"Unknown\"\n",
    "\n",
    "    print(\"[LOG] Transaction method standardized successfully.\")\n",
    "    return df\n",
    "\n",
    "# ============================================= (ORDER DATASET) STAGE 4: MISSING VALUE HANDLING =============================================\n",
    "def handle_missing_values_order(df):\n",
    "    \"\"\"\n",
    "    Strategy (for SME context):\n",
    "    - Drop rows if critical identifiers are missing (orderid, customerid, purchase date)\n",
    "    - Drop rows if both item price and total spend are missing\n",
    "    - Fill or infer non-critical missing fields logically:\n",
    "        - purchase item: \"Unknown Item\"\n",
    "        - purchase quantity: 1\n",
    "        - item price: median price\n",
    "        - total spend: item_price * quantity (if available)\n",
    "        - transaction method: \"Unknown\"\n",
    "        - purchase time: \"Unknown\"\n",
    "    \"\"\"\n",
    "\n",
    "    initial_count = len(df)\n",
    "\n",
    "    # Drop rows missing critical identifiers\n",
    "    critical_cols = [\"orderid\", \"customerid\", \"purchase date\"]\n",
    "    existing_critical = [c for c in critical_cols if c in df.columns]\n",
    "    df = df.dropna(subset=existing_critical)\n",
    "    print(f\"[LOG] Dropped rows with missing critical identifiers: {initial_count - len(df)}\")\n",
    "\n",
    "    # Drop rows missing both financial info\n",
    "    before_financial = len(df)\n",
    "    df = df.dropna(subset=[\"item price\", \"total spend\"], how=\"all\")\n",
    "    print(f\"[LOG] Dropped {before_financial - len(df)} rows with no financial info\")\n",
    "\n",
    "    # Fill non-critical fields\n",
    "    if \"purchase item\" in df.columns:\n",
    "        df[\"purchase item\"] = df[\"purchase item\"].fillna(\"Unknown Item\")\n",
    "\n",
    "    if \"purchase quantity\" in df.columns:\n",
    "        df[\"purchase quantity\"] = df[\"purchase quantity\"].fillna(1)\n",
    "\n",
    "    # Handle item price (replace with median)\n",
    "    if \"item price\" in df.columns:\n",
    "        median_price = df[\"item price\"].median(skipna=True)\n",
    "        df[\"item price\"] = df[\"item price\"].fillna(median_price)\n",
    "\n",
    "    # Handle total spend (calculate or fallback)\n",
    "    if {\"item price\", \"purchase quantity\", \"total spend\"}.issubset(df.columns):\n",
    "        mask_missing_total = df[\"total spend\"].isna()\n",
    "        df.loc[mask_missing_total, \"total spend\"] = (\n",
    "            df.loc[mask_missing_total, \"item price\"] * df.loc[mask_missing_total, \"purchase quantity\"]\n",
    "        )\n",
    "        # Fill any still missing values with median\n",
    "        median_total = df[\"total spend\"].median(skipna=True)\n",
    "        df[\"total spend\"] = df[\"total spend\"].fillna(median_total)\n",
    "\n",
    "    # Transaction method ‚Üí Unknown\n",
    "    if \"transaction method\" in df.columns:\n",
    "        df[\"transaction method\"] = df[\"transaction method\"].replace([\"\", \"NaN\", None], np.nan)\n",
    "        df[\"transaction method\"] = df[\"transaction method\"].fillna(\"Unknown\")\n",
    "\n",
    "    # Purchase time ‚Üí Unknown\n",
    "    if \"purchase time\" in df.columns:\n",
    "        df[\"purchase time\"] = df[\"purchase time\"].fillna(\"Unknown\")\n",
    "\n",
    "    # Final summary\n",
    "    dropped_total = initial_count - len(df)\n",
    "    print(f\"[LOG] Dropped total {dropped_total} rows ({dropped_total/initial_count:.2%}) due to missing critical data\")\n",
    "    print(f\"[LOG] Dataset now has {len(df)} rows after missing value handling\")\n",
    "    print(\"========== [STAGE 4 COMPLETE] ==========\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================= (ORDER DATASET) STAGE 5: OUTLIER DETECTION =============================================\n",
    "def order_detect_outliers(df):\n",
    "    \"\"\"\n",
    "    Stage 5: Handle outliers for order dataset.\n",
    "    - Apply IQR method if dataset < 500 rows.\n",
    "    - Apply percentile capping (1st‚Äì99th) if dataset >= 500 rows.\n",
    "    - Columns: item price, purchase quantity, total spend.\n",
    "    \"\"\"\n",
    "    print(f\"[LOG] Dataset has {len(df)} rows\")\n",
    "\n",
    "    numeric_cols = [\"purchase quantity\", \"total spend\"]\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        if df[col].dropna().empty:\n",
    "            print(f\"[WARN] {col} is empty or missing, skipping.\")\n",
    "            continue\n",
    "\n",
    "        if len(df) < 500:\n",
    "            # IQR Method\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - 1.5 * IQR\n",
    "            upper = Q3 + 1.5 * IQR\n",
    "            method = \"IQR\"\n",
    "        else:\n",
    "            # Percentile Capping\n",
    "            lower = df[col].quantile(0.01)\n",
    "            upper = df[col].quantile(0.99)\n",
    "            method = \"Percentile Capping\"\n",
    "\n",
    "        # Apply capping\n",
    "        df[col] = df[col].clip(lower, upper)\n",
    "        print(f\"[LOG] {method} applied on '{col}', capped between [{lower:.2f}, {upper:.2f}]\")\n",
    "\n",
    "    print(\"‚úÖ [STAGE 5 COMPLETE] Outliers handled successfully.\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82eea848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================= (ORDER DATASET) DATASET CLEANING PIPELINE =============================================\n",
    "def clean_order_dataset(df, original_order_dataset_name):\n",
    "    print(\"üöÄ Starting order data cleaning pipeline...\\n\")\n",
    "    \n",
    "    # =======================================================\n",
    "    # STAGE 0: NORMALIZE COLUMN NAMES (FROM GENERIC FUNCTION)\n",
    "    # =======================================================\n",
    "    print(\"========== [STAGE 0 START] Normalize Column Names ==========\")\n",
    "    df = normalize_columns_name(df)\n",
    "    print(\"‚úÖ [STAGE 0 COMPLETE] Column names normalized.\\n\")\n",
    "    \n",
    "    # =============================================\n",
    "    # STAGE 1: SCHEMA & COLUMN VALIDATION\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 1 START] Schema & Column Validation ==========\")\n",
    "    order_mandatory = [\"orderid\", \"customerid\", \"purchase item\", \"purchase date\", \"item price\", \"purchase quantity\", \"total spend\", \"transaction method\"]\n",
    "    df, message = check_mandatory_columns(df, dataset_type=\"order\", mandatory_columns=order_mandatory)\n",
    "    print(message)\n",
    "    print(\"‚úÖ [STAGE 1 COMPLETE] Schema validation done.\\n\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # STAGE 2: REMOVE DUPLICATE ENTRY ROWS (FROM GENERIC FUNCTION)\n",
    "    # ============================================================\n",
    "    print(\"========== [STAGE 2 START] Remove Duplicate Entry Rows ==========\")\n",
    "    df = remove_duplicate_entries(df)\n",
    "    print(\"‚úÖ [STAGE 2 COMPLETE] Duplicate entries removed.\\n\")\n",
    "    \n",
    "    # =============================================\n",
    "    # STAGE 3: STANDARDIZATION & NORMALIZATION\n",
    "    # =============================================\n",
    "    print(\"========== [STAGE 3 START] Standardization & Normalization ==========\")\n",
    "    df = standardized_order_id(df)\n",
    "    df = standardize_customer_id(df)\n",
    "    df = standardize_purchase_item(df)\n",
    "    df = standardize_purchase_date(df)\n",
    "    df = standardized_item_price_and_total_spend(df)\n",
    "    df = standardize_purchase_quantity(df)\n",
    "    df = standardize_transaction_method(df)\n",
    "    print(\"‚úÖ [STAGE 3 COMPLETE] Standardization and normalization finished.\\n\")\n",
    "    \n",
    "    print(\"========== [Standardized Order Dataset START] Save Standardized Dataset ==========\")\n",
    "    base_name, ext = os.path.splitext(original_order_dataset_name)\n",
    "    cleaned_file = f\"{base_name}_standardized{ext}\"\n",
    "    df.to_csv(cleaned_file, index=False, float_format=\"%.2f\")\n",
    "    print(f\"‚úÖ [STANDARDIZED STAGE COMPLETE] Standardized Order dataset saved as: {cleaned_file}\\n\")\n",
    "    \n",
    "    # ===============================================\n",
    "    # STAGE 4: MISSING VALUE HANDLING\n",
    "    # ===============================================\n",
    "    print(\"========== [STAGE 4 START] Missing Value Handling ==========\")\n",
    "    df = handle_missing_values_order(df)\n",
    "    print(\"‚úÖ [STAGE 4 COMPLETE] Missing values handled.\\n\")\n",
    "    \n",
    "    # # =============================================\n",
    "    # # STAGE 6: OUTLIER DETECTION\n",
    "    # # =============================================\n",
    "    print(\"========== [STAGE 5 START] Outlier Detection ==========\")\n",
    "    df = order_detect_outliers(df)   # make sure detect_outliers returns df\n",
    "    print(\"‚úÖ [STAGE 5 COMPLETE] Outliers handled.\\n\")\n",
    "    \n",
    "    # =============================================\n",
    "    # SAVE CLEANED DATASET\n",
    "    # =============================================\n",
    "    print(\"========== [FINAL STAGE START] Save Cleaned Dataset ==========\")\n",
    "    # base_name, ext = os.path.splitext(original_order_dataset_name)\n",
    "    cleaned_file = f\"{base_name}_cleaned{ext}\"\n",
    "    df.to_csv(cleaned_file, index=False)\n",
    "    print(f\"‚úÖ [FINAL STAGE COMPLETE] Cleaned dataset saved as: {cleaned_file}\\n\")\n",
    "\n",
    "    print(\"==========================================================\")\n",
    "    print(\"üéâ Data cleaning pipeline completed successfully!\\n\")\n",
    "    return df, cleaned_file\n",
    "    # later add on return clean file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e60a6b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting order data cleaning pipeline...\n",
      "\n",
      "========== [STAGE 0 START] Normalize Column Names ==========\n",
      "[LOG] Running normalize_columns_name...\n",
      "[LOG] Columns after normalization: ['orderid', 'customerid', 'purchase item', 'purchase date', 'item price', 'purchase quantity', 'total spend', 'transaction method']\n",
      "‚úÖ [STAGE 0 COMPLETE] Column names normalized.\n",
      "\n",
      "========== [STAGE 1 START] Schema & Column Validation ==========\n",
      "[LOG] Running check_mandatory_columns for order dataset...\n",
      "[LOG] Mandatory column 'orderid' fill ratio: 1.00\n",
      "[LOG] Mandatory column 'customerid' fill ratio: 1.00\n",
      "[LOG] Mandatory column 'purchase item' fill ratio: 0.92\n",
      "[LOG] Mandatory column 'purchase date' fill ratio: 1.00\n",
      "[LOG] Mandatory column 'item price' fill ratio: 0.92\n",
      "[LOG] Mandatory column 'purchase quantity' fill ratio: 0.92\n",
      "[LOG] Mandatory column 'total spend' fill ratio: 0.55\n",
      "[LOG] Mandatory column 'transaction method' fill ratio: 0.67\n",
      "Some key fields in the order dataset have a high number of missing values: total spend, transaction method. The system will continue cleaning and handle missing values automatically, but we STRONGLY encourage you to reupload your source data, ensure it was as complete as possible for accurate segmentation result later.\n",
      "\n",
      "Missing Data Summary:\n",
      "orderid: 0.0% missing\n",
      "customerid: 0.0% missing\n",
      "purchase item: 7.7% missing\n",
      "purchase date: 0.0% missing\n",
      "item price: 7.6% missing\n",
      "purchase quantity: 7.7% missing\n",
      "total spend: 44.6% missing\n",
      "transaction method: 33.0% missing\n",
      "‚úÖ [STAGE 1 COMPLETE] Schema validation done.\n",
      "\n",
      "========== [STAGE 2 START] Remove Duplicate Entry Rows ==========\n",
      "[LOG] Running remove_duplicate_entries...\n",
      "[LOG] Removed 40 duplicate rows.\n",
      "‚úÖ [STAGE 2 COMPLETE] Duplicate entries removed.\n",
      "\n",
      "========== [STAGE 3 START] Standardization & Normalization ==========\n",
      "[LOG] Running standardize_order_id...\n",
      "[LOG] OrderID column standardized\n",
      "[LOG] Running standardize_customer_id...\n",
      "[LOG] CustomerID column standardized\n",
      "[LOG] Running standardized_purchase_item...\n",
      "[LOG] Purchase item standardized\n",
      "[LOG] Running standardize_purchase_date...\n",
      "[LOG] Purchase date standardization complete.\n",
      "[LOG] Running standardized_item_price_and_total_spend...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_28092\\3762731870.py:49: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.loc[:, \"purchase datetime\"] = pd.to_datetime(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] item price standardized: numeric, 2 decimal places\n",
      "[LOG] total spend standardized: numeric, 2 decimal places\n",
      "[LOG] Running standardize_purchase_quantity...\n",
      "[LOG] Purchase quantity standardized to integer format\n",
      "[LOG] Running standardize_transaction_method...\n",
      "[LOG] Transaction method standardized successfully.\n",
      "‚úÖ [STAGE 3 COMPLETE] Standardization and normalization finished.\n",
      "\n",
      "========== [Standardized Order Dataset START] Save Standardized Dataset ==========\n",
      "‚úÖ [STANDARDIZED STAGE COMPLETE] Standardized Order dataset saved as: 2021 - 2025 Order_standardized.csv\n",
      "\n",
      "========== [STAGE 4 START] Missing Value Handling ==========\n",
      "[LOG] Dropped rows with missing critical identifiers: 0\n",
      "[LOG] Dropped 407 rows with no financial info\n",
      "[LOG] Dropped total 407 rows (7.10%) due to missing critical data\n",
      "[LOG] Dataset now has 5328 rows after missing value handling\n",
      "========== [STAGE 4 COMPLETE] ==========\n",
      "‚úÖ [STAGE 4 COMPLETE] Missing values handled.\n",
      "\n",
      "========== [STAGE 5 START] Outlier Detection ==========\n",
      "[LOG] Dataset has 5328 rows\n",
      "[LOG] Percentile Capping applied on 'purchase quantity', capped between [1.00, 6.00]\n",
      "[LOG] Percentile Capping applied on 'total spend', capped between [11.90, 243.31]\n",
      "‚úÖ [STAGE 5 COMPLETE] Outliers handled successfully.\n",
      "‚úÖ [STAGE 5 COMPLETE] Outliers handled.\n",
      "\n",
      "========== [FINAL STAGE START] Save Cleaned Dataset ==========\n",
      "‚úÖ [FINAL STAGE COMPLETE] Cleaned dataset saved as: 2021 - 2025 Order_cleaned.csv\n",
      "\n",
      "==========================================================\n",
      "üéâ Data cleaning pipeline completed successfully!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orderid</th>\n",
       "      <th>customerid</th>\n",
       "      <th>purchase item</th>\n",
       "      <th>purchase date</th>\n",
       "      <th>item price</th>\n",
       "      <th>purchase quantity</th>\n",
       "      <th>total spend</th>\n",
       "      <th>transaction method</th>\n",
       "      <th>purchase time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>211103JN8114CU</td>\n",
       "      <td>CUST0001</td>\n",
       "      <td>Aloe Vera Homemade Soap | Acne-Prone Skin | Ba...</td>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>42.030</td>\n",
       "      <td>E-Wallet</td>\n",
       "      <td>20:21:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211103JN8114CU</td>\n",
       "      <td>CUST0001</td>\n",
       "      <td>Oats &amp; Calendula Dandruff Shampoo Bar | Sulfat...</td>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.000</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>20:21:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>211104MMPQH994</td>\n",
       "      <td>CUST0002</td>\n",
       "      <td>Calendula Homemade Soap | Suitable For Eczema-...</td>\n",
       "      <td>2021-11-04</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2</td>\n",
       "      <td>70.000</td>\n",
       "      <td>E-Wallet</td>\n",
       "      <td>15:17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>211104MMPQH994</td>\n",
       "      <td>CUST0002</td>\n",
       "      <td>Green Tea &amp; Mint Hair Loss Shampoo | Sulfate-F...</td>\n",
       "      <td>2021-11-04</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.000</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>15:17:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>211104MSJ8BGNC</td>\n",
       "      <td>CUST0003</td>\n",
       "      <td>Green Tea &amp; Mint Hair Loss Shampoo | Sulfate-F...</td>\n",
       "      <td>2021-11-04</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000</td>\n",
       "      <td>E-Wallet</td>\n",
       "      <td>16:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>211104NAHE4EFA</td>\n",
       "      <td>CUST0004</td>\n",
       "      <td>Green Tea &amp; Mint Hair Loss Shampoo | Sulfate-F...</td>\n",
       "      <td>2021-11-04</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2</td>\n",
       "      <td>243.307</td>\n",
       "      <td>Card</td>\n",
       "      <td>21:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>211104NAHE4EFA</td>\n",
       "      <td>CUST0004</td>\n",
       "      <td>Green Tea &amp; Mint Hair Loss Shampoo | Sulfate-F...</td>\n",
       "      <td>2021-11-04</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2</td>\n",
       "      <td>48.000</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>21:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>211104NAHE4EFA</td>\n",
       "      <td>CUST0004</td>\n",
       "      <td>Oats &amp; Calendula Dandruff Shampoo Bar | Sulfat...</td>\n",
       "      <td>2021-11-04</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2</td>\n",
       "      <td>46.000</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>21:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>211105PGD0R3EG</td>\n",
       "      <td>CUST0005</td>\n",
       "      <td>Green Tea &amp; Mint Hair Loss Shampoo | Sulfate-F...</td>\n",
       "      <td>2021-11-05</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000</td>\n",
       "      <td>Card</td>\n",
       "      <td>09:05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>211105QAU8FV5B</td>\n",
       "      <td>CUST0006</td>\n",
       "      <td>Green Tea &amp; Mint Hair Loss Shampoo | Sulfate-F...</td>\n",
       "      <td>2021-11-05</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>45.000</td>\n",
       "      <td>E-Wallet</td>\n",
       "      <td>16:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>211105QAU8FV5B</td>\n",
       "      <td>CUST0006</td>\n",
       "      <td>Oats &amp; Calendula Dandruff Shampoo Bar | Sulfat...</td>\n",
       "      <td>2021-11-05</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.000</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>16:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>211106SB6XRNG0</td>\n",
       "      <td>CUST0007</td>\n",
       "      <td>Green Tea &amp; Mint Hair Loss Shampoo | Sulfate-F...</td>\n",
       "      <td>2021-11-06</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28.500</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>12:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>211106SVJ6JGC8</td>\n",
       "      <td>CUST0008</td>\n",
       "      <td>Eco-Friendly Dishwashing Soap | Homemade Soap ...</td>\n",
       "      <td>2021-11-06</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1</td>\n",
       "      <td>49.700</td>\n",
       "      <td>E-Wallet</td>\n",
       "      <td>17:21:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>211106SVJ6JGC8</td>\n",
       "      <td>CUST0008</td>\n",
       "      <td>Herbal Face Homemade Soap | Skin Care | Suitab...</td>\n",
       "      <td>2021-11-06</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>17:21:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>211106SVJ6JGC8</td>\n",
       "      <td>CUST0008</td>\n",
       "      <td>Oats &amp; Calendula Dandruff Shampoo Bar | Sulfat...</td>\n",
       "      <td>2021-11-06</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.000</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>17:21:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>211107UU4PXY39</td>\n",
       "      <td>CUST0009</td>\n",
       "      <td>Eco-Friendly Dishwashing Soap | Homemade Soap ...</td>\n",
       "      <td>2021-11-07</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5</td>\n",
       "      <td>18.500</td>\n",
       "      <td>E-Wallet</td>\n",
       "      <td>12:01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>211107VGMY0NJQ</td>\n",
       "      <td>CUST0010</td>\n",
       "      <td>Eco-Friendly Dishwashing Soap | Homemade Soap ...</td>\n",
       "      <td>2021-11-07</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2</td>\n",
       "      <td>149.400</td>\n",
       "      <td>E-Wallet</td>\n",
       "      <td>18:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>211107VGMY0NJQ</td>\n",
       "      <td>CUST0010</td>\n",
       "      <td>Calendula Homemade Soap | Suitable For Eczema-...</td>\n",
       "      <td>2021-11-07</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.000</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>18:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>211107VGMY0NJQ</td>\n",
       "      <td>CUST0010</td>\n",
       "      <td>Aloe Vera Homemade Soap | Acne-Prone Skin | Ba...</td>\n",
       "      <td>2021-11-07</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.000</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>18:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>211107VGMY0NJQ</td>\n",
       "      <td>CUST0010</td>\n",
       "      <td>Green Tea &amp; Mint Hair Loss Shampoo | Sulfate-F...</td>\n",
       "      <td>2021-11-07</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2</td>\n",
       "      <td>48.000</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>18:25:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           orderid customerid  \\\n",
       "0   211103JN8114CU   CUST0001   \n",
       "1   211103JN8114CU   CUST0001   \n",
       "2   211104MMPQH994   CUST0002   \n",
       "3   211104MMPQH994   CUST0002   \n",
       "4   211104MSJ8BGNC   CUST0003   \n",
       "5   211104NAHE4EFA   CUST0004   \n",
       "6   211104NAHE4EFA   CUST0004   \n",
       "7   211104NAHE4EFA   CUST0004   \n",
       "8   211105PGD0R3EG   CUST0005   \n",
       "10  211105QAU8FV5B   CUST0006   \n",
       "11  211105QAU8FV5B   CUST0006   \n",
       "12  211106SB6XRNG0   CUST0007   \n",
       "13  211106SVJ6JGC8   CUST0008   \n",
       "14  211106SVJ6JGC8   CUST0008   \n",
       "15  211106SVJ6JGC8   CUST0008   \n",
       "16  211107UU4PXY39   CUST0009   \n",
       "18  211107VGMY0NJQ   CUST0010   \n",
       "19  211107VGMY0NJQ   CUST0010   \n",
       "20  211107VGMY0NJQ   CUST0010   \n",
       "21  211107VGMY0NJQ   CUST0010   \n",
       "\n",
       "                                        purchase item purchase date  \\\n",
       "0   Aloe Vera Homemade Soap | Acne-Prone Skin | Ba...    2021-11-03   \n",
       "1   Oats & Calendula Dandruff Shampoo Bar | Sulfat...    2021-11-03   \n",
       "2   Calendula Homemade Soap | Suitable For Eczema-...    2021-11-04   \n",
       "3   Green Tea & Mint Hair Loss Shampoo | Sulfate-F...    2021-11-04   \n",
       "4   Green Tea & Mint Hair Loss Shampoo | Sulfate-F...    2021-11-04   \n",
       "5   Green Tea & Mint Hair Loss Shampoo | Sulfate-F...    2021-11-04   \n",
       "6   Green Tea & Mint Hair Loss Shampoo | Sulfate-F...    2021-11-04   \n",
       "7   Oats & Calendula Dandruff Shampoo Bar | Sulfat...    2021-11-04   \n",
       "8   Green Tea & Mint Hair Loss Shampoo | Sulfate-F...    2021-11-05   \n",
       "10  Green Tea & Mint Hair Loss Shampoo | Sulfate-F...    2021-11-05   \n",
       "11  Oats & Calendula Dandruff Shampoo Bar | Sulfat...    2021-11-05   \n",
       "12  Green Tea & Mint Hair Loss Shampoo | Sulfate-F...    2021-11-06   \n",
       "13  Eco-Friendly Dishwashing Soap | Homemade Soap ...    2021-11-06   \n",
       "14  Herbal Face Homemade Soap | Skin Care | Suitab...    2021-11-06   \n",
       "15  Oats & Calendula Dandruff Shampoo Bar | Sulfat...    2021-11-06   \n",
       "16  Eco-Friendly Dishwashing Soap | Homemade Soap ...    2021-11-07   \n",
       "18  Eco-Friendly Dishwashing Soap | Homemade Soap ...    2021-11-07   \n",
       "19  Calendula Homemade Soap | Suitable For Eczema-...    2021-11-07   \n",
       "20  Aloe Vera Homemade Soap | Acne-Prone Skin | Ba...    2021-11-07   \n",
       "21  Green Tea & Mint Hair Loss Shampoo | Sulfate-F...    2021-11-07   \n",
       "\n",
       "    item price  purchase quantity  total spend transaction method  \\\n",
       "0         18.0                  1       42.030           E-Wallet   \n",
       "1         23.0                  1       23.000            Unknown   \n",
       "2         23.0                  2       70.000           E-Wallet   \n",
       "3         24.0                  1       24.000            Unknown   \n",
       "4         24.0                  1       26.000           E-Wallet   \n",
       "5         24.0                  2      243.307               Card   \n",
       "6         24.0                  2       48.000            Unknown   \n",
       "7         23.0                  2       46.000            Unknown   \n",
       "8         24.0                  1       25.000               Card   \n",
       "10        24.0                  1       45.000           E-Wallet   \n",
       "11        23.0                  1       23.000            Unknown   \n",
       "12        24.0                  1       28.500            Unknown   \n",
       "13         3.7                  1       49.700           E-Wallet   \n",
       "14        25.0                  1       25.000            Unknown   \n",
       "15        23.0                  1       23.000            Unknown   \n",
       "16         3.7                  5       18.500           E-Wallet   \n",
       "18         3.7                  2      149.400           E-Wallet   \n",
       "19        23.0                  1       23.000            Unknown   \n",
       "20        18.0                  1       18.000            Unknown   \n",
       "21        24.0                  2       48.000            Unknown   \n",
       "\n",
       "   purchase time  \n",
       "0       20:21:00  \n",
       "1       20:21:00  \n",
       "2       15:17:00  \n",
       "3       15:17:00  \n",
       "4       16:44:00  \n",
       "5       21:47:00  \n",
       "6       21:47:00  \n",
       "7       21:47:00  \n",
       "8       09:05:00  \n",
       "10      16:58:00  \n",
       "11      16:58:00  \n",
       "12      12:10:00  \n",
       "13      17:21:00  \n",
       "14      17:21:00  \n",
       "15      17:21:00  \n",
       "16      12:01:00  \n",
       "18      18:25:00  \n",
       "19      18:25:00  \n",
       "20      18:25:00  \n",
       "21      18:25:00  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep a copy for comparison\n",
    "original_order_df = order_df.copy()\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>> (ORDER DATASET) IMPLEMENT CLEANING PIPELINE >>>>>>>>>>>>>>>>>>>\n",
    "# cleaned_order_df, cleaned_file_name = clean_order_dataset(order_df, original_order_dataset_name)\n",
    "cleaned_order_df, cleaned_file_name = clean_order_dataset(order_df, original_order_dataset_name)\n",
    "\n",
    "# AFTER CLEANING : CLEANED VERSION TO BE STORED BACK TO DATABASE\n",
    "cleaned_order_df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP Kernel",
   "language": "python",
   "name": "fyp_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
